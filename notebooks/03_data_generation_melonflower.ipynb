{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba493af",
   "metadata": {},
   "source": [
    "# Data Exploration: MelonFlower Dataset Analysis\n",
    "\n",
    "**CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection - MelonFlower Focus**\n",
    "\n",
    "**Authors:** Satvik Praveen, Yoonsung Jung  \n",
    "**Institution:** Texas A&M University  \n",
    "**Course:** Computer Vision and Deep Learning  \n",
    "**Date:** April 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive data exploration and analysis of the MelonFlower Dataset for agricultural flower detection. We focus on analyzing flower characteristics, bloom stages, color variations, temporal patterns, and pollination states to optimize CBAM-STN-TPS-YOLO training for flower detection tasks in agricultural settings.\n",
    "\n",
    "## Key Objectives\n",
    "1. Load and analyze MelonFlower dataset structure and composition\n",
    "2. Examine flower distribution patterns and bloom stage variations\n",
    "3. Analyze flower characteristics including color, size, and morphology\n",
    "4. Explore temporal flowering patterns and seasonal variations\n",
    "5. Assess pollination state detection and flower health indicators\n",
    "6. Evaluate dataset quality and identify flower-specific challenges\n",
    "7. Generate comprehensive visualizations and flower-focused summary reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287954ef",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06715540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced setup and imports for CBAM-STN-TPS-YOLO MelonFlower analysis\n",
    "Compatible with comprehensive framework standards\n",
    "\"\"\"\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch ecosystem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import color, feature, measure, morphology\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import entropy, pearsonr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Configuration management\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Configuration class with documented parameters\"\"\"\n",
    "    # Memory management\n",
    "    max_batch_size: int = 50\n",
    "    memory_cleanup_interval: int = 100\n",
    "    \n",
    "    # Flower detection thresholds\n",
    "    tiny_flower_threshold: float = 0.005  # Based on agricultural studies\n",
    "    small_flower_threshold: float = 0.02\n",
    "    medium_flower_threshold: float = 0.08\n",
    "    large_flower_threshold: float = 0.2\n",
    "    \n",
    "    # Color analysis\n",
    "    background_similarity_severe: float = 0.15  # High similarity threshold\n",
    "    background_similarity_moderate: float = 0.30\n",
    "    \n",
    "    # Health assessment weights\n",
    "    color_health_weight: float = 0.3\n",
    "    texture_health_weight: float = 0.3\n",
    "    size_health_weight: float = 0.2\n",
    "    uniformity_health_weight: float = 0.2\n",
    "\n",
    "# Memory management utilities\n",
    "class MemoryManager:\n",
    "    \"\"\"Enhanced memory management for large-scale analysis\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_memory():\n",
    "        \"\"\"Clear GPU memory and cache with monitoring\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_memory_usage():\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1e9  # GB\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_memory_threshold(threshold_gb=8.0):\n",
    "        \"\"\"Check if memory usage exceeds threshold\"\"\"\n",
    "        current = MemoryManager.get_memory_usage()\n",
    "        return current > threshold_gb\n",
    "\n",
    "# Enhanced error handling\n",
    "class AnalysisError(Exception):\n",
    "    \"\"\"Custom exception for analysis operations\"\"\"\n",
    "    pass\n",
    "\n",
    "def safe_operation(operation_name: str, operation_func, *args, **kwargs):\n",
    "    \"\"\"Execute operations with specific error handling\"\"\"\n",
    "    logger.info(f\"Starting {operation_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Check memory before operation\n",
    "        if MemoryManager.check_memory_threshold():\n",
    "            logger.warning(f\"High memory usage before {operation_name}, clearing cache\")\n",
    "            MemoryManager.clear_memory()\n",
    "        \n",
    "        result = operation_func(*args, **kwargs)\n",
    "        logger.info(f\"Completed {operation_name} successfully\")\n",
    "        return result\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found during {operation_name}: {e}\")\n",
    "        raise AnalysisError(f\"Required files missing for {operation_name}\")\n",
    "    \n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"GPU memory error during {operation_name}: {e}\")\n",
    "        MemoryManager.clear_memory()\n",
    "        raise AnalysisError(f\"Insufficient GPU memory for {operation_name}\")\n",
    "    \n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Value error during {operation_name}: {e}\")\n",
    "        raise AnalysisError(f\"Invalid parameters for {operation_name}: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during {operation_name}: {e}\")\n",
    "        import traceback\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "# Batch processing utilities\n",
    "def process_in_batches(items, batch_size=None, operation_func=None, **kwargs):\n",
    "    \"\"\"Process items in batches with memory management\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = AnalysisConfig().max_batch_size\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_result = operation_func(batch, **kwargs)\n",
    "            results.extend(batch_result if isinstance(batch_result, list) else [batch_result])\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Batch {i//batch_size} failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Memory cleanup every few batches\n",
    "        if (i // batch_size) % 5 == 0:\n",
    "            MemoryManager.clear_memory()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Project imports with enhanced fallback\n",
    "try:\n",
    "    # Add project root to path\n",
    "    project_root = Path(__file__).parent.parent if '__file__' in globals() else Path('.').parent\n",
    "    sys.path.append(str(project_root))\n",
    "    \n",
    "    # Core model components\n",
    "    from src.models import create_model, CBAM_STN_TPS_YOLO\n",
    "    from src.data import create_agricultural_dataloader, get_multi_spectral_transforms\n",
    "    from src.utils.visualization import Visualizer, plot_training_curves, visualize_predictions\n",
    "    from src.utils.evaluation import ModelEvaluator, calculate_model_complexity\n",
    "    from src.utils.config_validator import load_and_validate_config\n",
    "    \n",
    "    logger.info(\"Project imports successful\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    logger.warning(f\"Project import warning: {e}\")\n",
    "    logger.info(\"Creating optimized dummy implementations\")\n",
    "    \n",
    "    class DummyMelonFlowerDataset:\n",
    "        def __init__(self, data_dir, split='train'):\n",
    "            self.class_names = ['flower', 'bud', 'mature_flower', 'withered_flower']\n",
    "            self.split = split\n",
    "            self.data_dir = data_dir\n",
    "            self._length = {'train': 1250, 'val': 320, 'test': 180}.get(split, 100)\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self._length\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Use seed for reproducible dummy data\n",
    "            np.random.seed(idx)  # Ensure consistent dummy data\n",
    "            \n",
    "            # Generate realistic flower data\n",
    "            image = torch.randn(3, 640, 640)\n",
    "            \n",
    "            # Simulate flowers with agricultural characteristics\n",
    "            num_flowers = np.random.choice([1, 2, 3, 4, 5, 6], p=[0.3, 0.25, 0.2, 0.15, 0.08, 0.02])\n",
    "            targets = []\n",
    "            \n",
    "            for _ in range(num_flowers):\n",
    "                cls = np.random.choice([0, 1, 2, 3], p=[0.4, 0.3, 0.25, 0.05])\n",
    "                x = np.random.uniform(0.1, 0.9)\n",
    "                y = np.random.uniform(0.1, 0.9)\n",
    "                \n",
    "                # Size varies by flower stage with realistic distributions\n",
    "                config = AnalysisConfig()\n",
    "                if cls == 0:  # flower\n",
    "                    w, h = np.random.uniform(0.08, 0.20, 2)\n",
    "                elif cls == 1:  # bud\n",
    "                    w, h = np.random.uniform(0.03, 0.08, 2)\n",
    "                elif cls == 2:  # mature flower\n",
    "                    w, h = np.random.uniform(0.10, 0.25, 2)\n",
    "                else:  # withered\n",
    "                    w, h = np.random.uniform(0.05, 0.15, 2)\n",
    "                \n",
    "                targets.append([cls, x, y, w, h])\n",
    "            \n",
    "            targets = torch.tensor(targets, dtype=torch.float32) if targets else torch.zeros(0, 5)\n",
    "            path = f\"dummy_melonflower_{self.split}_{idx:04d}.jpg\"\n",
    "            \n",
    "            return image, targets, path\n",
    "    \n",
    "    class Visualizer:\n",
    "        def __init__(self, class_names=None):\n",
    "            self.class_names = class_names or ['flower', 'bud', 'mature_flower', 'withered_flower']\n",
    "        \n",
    "        def plot_training_curves(self, *args, **kwargs):\n",
    "            logger.info(\"Plotting training curves with dummy implementation\")\n",
    "\n",
    "# Setup enhanced logging\n",
    "def setup_logging():\n",
    "    \"\"\"Setup comprehensive logging system\"\"\"\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    \n",
    "    # Create logs directory\n",
    "    log_dir = Path('../logs')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=log_format,\n",
    "        handlers=[\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "            logging.FileHandler(log_dir / 'melonflower_analysis.log'),\n",
    "            logging.FileHandler(log_dir / f'melonflower_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Set specific logger levels\n",
    "    logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "    logging.getLogger('PIL').setLevel(logging.WARNING)\n",
    "    \n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# Enhanced plotting configuration\n",
    "def setup_plotting():\n",
    "    \"\"\"Setup optimized plotting configuration\"\"\"\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Memory-efficient plot settings\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (10, 6),  # Smaller default size\n",
    "        'figure.dpi': 100,\n",
    "        'savefig.dpi': 300,\n",
    "        'font.size': 11,\n",
    "        'axes.titlesize': 13,\n",
    "        'axes.labelsize': 11,\n",
    "        'xtick.labelsize': 9,\n",
    "        'ytick.labelsize': 9,\n",
    "        'legend.fontsize': 9,\n",
    "        'figure.max_open_warning': 20\n",
    "    })\n",
    "    \n",
    "    # Custom flower color palette\n",
    "    flower_colors = ['#FF69B4', '#FFB6C1', '#FFA07A', '#98FB98', '#87CEEB', '#DDA0DD']\n",
    "    sns.set_palette(flower_colors)\n",
    "\n",
    "setup_plotting()\n",
    "\n",
    "# Device configuration with validation\n",
    "def setup_device():\n",
    "    \"\"\"Setup optimal device configuration with validation\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        gpu_name = torch.cuda.get_device_name()\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        logger.info(f\"CUDA available: {gpu_name}\")\n",
    "        logger.info(f\"GPU Memory: {total_memory:.1f} GB\")\n",
    "        \n",
    "        # Validate memory availability\n",
    "        if total_memory < 4.0:\n",
    "            logger.warning(\"Low GPU memory detected. Consider reducing batch sizes.\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        logger.info(\"MPS (Apple Silicon) available\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        logger.warning(\"Using CPU - analysis will be slower\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "# Enhanced seed setting\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducible results\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    logger.info(f\"Random seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Enhanced directory structure\n",
    "def setup_directories():\n",
    "    \"\"\"Setup organized directory structure with validation\"\"\"\n",
    "    base_dir = Path('../results/notebooks/melonflower_exploration')\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    subdirs = [\n",
    "        'visualizations', 'statistics', 'color_analysis', \n",
    "        'morphology', 'health_assessment', 'challenges', \n",
    "        'samples', 'temporal_analysis', 'cache', 'logs'\n",
    "    ]\n",
    "    \n",
    "    created_dirs = {}\n",
    "    for subdir in subdirs:\n",
    "        dir_path = base_dir / subdir\n",
    "        dir_path.mkdir(exist_ok=True)\n",
    "        created_dirs[subdir] = dir_path\n",
    "    \n",
    "    logger.info(f\"Results directory structure created at: {base_dir}\")\n",
    "    return base_dir, created_dirs\n",
    "\n",
    "notebook_results_dir, result_dirs = setup_directories()\n",
    "\n",
    "# Configure warnings with specificity\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='.*Trying to infer the `batch_size`.*')\n",
    "\n",
    "# Enhanced pandas configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Initialize global configuration\n",
    "config = AnalysisConfig()\n",
    "\n",
    "logger.info(\"Enhanced environment setup complete!\")\n",
    "logger.info(f\"Configuration loaded with batch size: {config.max_batch_size}\")\n",
    "logger.info(f\"Results directory: {notebook_results_dir}\")\n",
    "logger.info(\"Ready for comprehensive MelonFlower dataset analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17207aa",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced dataset loading with comprehensive flower-specific analysis\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "class DatasetValidator:\n",
    "    \"\"\"Validates dataset integrity and structure\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_dataset_structure(data_dir: Path, expected_splits: List[str]) -> Dict[str, bool]:\n",
    "        \"\"\"Validate dataset directory structure\"\"\"\n",
    "        validation_results = {}\n",
    "        \n",
    "        for split in expected_splits:\n",
    "            split_dir = data_dir / split\n",
    "            images_dir = split_dir / 'images'\n",
    "            labels_dir = split_dir / 'labels'\n",
    "            \n",
    "            validation_results[split] = {\n",
    "                'split_exists': split_dir.exists(),\n",
    "                'images_exist': images_dir.exists() if split_dir.exists() else False,\n",
    "                'labels_exist': labels_dir.exists() if split_dir.exists() else False,\n",
    "                'has_files': len(list(images_dir.glob('*'))) > 0 if images_dir.exists() else False\n",
    "            }\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_sample_data(dataset, num_samples: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Validate sample data from dataset\"\"\"\n",
    "        validation_results = {\n",
    "            'sample_count': 0,\n",
    "            'valid_samples': 0,\n",
    "            'image_shapes': [],\n",
    "            'target_formats': [],\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        max_samples = min(num_samples, len(dataset))\n",
    "        indices = np.random.choice(len(dataset), max_samples, replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            try:\n",
    "                image, targets, path = dataset[idx]\n",
    "                validation_results['sample_count'] += 1\n",
    "                \n",
    "                # Validate image format\n",
    "                if isinstance(image, torch.Tensor):\n",
    "                    validation_results['image_shapes'].append(list(image.shape))\n",
    "                    validation_results['valid_samples'] += 1\n",
    "                else:\n",
    "                    validation_results['errors'].append(f\"Invalid image format at index {idx}\")\n",
    "                \n",
    "                # Validate targets format\n",
    "                if isinstance(targets, torch.Tensor):\n",
    "                    validation_results['target_formats'].append(list(targets.shape))\n",
    "                else:\n",
    "                    validation_results['errors'].append(f\"Invalid targets format at index {idx}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                validation_results['errors'].append(f\"Error at index {idx}: {str(e)}\")\n",
    "        \n",
    "        return validation_results\n",
    "\n",
    "class MelonFlowerDatasetLoader:\n",
    "    \"\"\"Enhanced dataset loader with validation and error recovery\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "        self.validator = DatasetValidator()\n",
    "        \n",
    "    def load_single_dataset(self, data_dir: str, split: str, melonflower_config: Dict) -> Tuple[Any, Dict]:\n",
    "        \"\"\"Load a single dataset split with validation\"\"\"\n",
    "        dataset_info = {\n",
    "            'split': split,\n",
    "            'status': 'loading',\n",
    "            'validation_results': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            data_path = Path(data_dir)\n",
    "            \n",
    "            # Validate structure first\n",
    "            if data_path.exists():\n",
    "                validation = self.validator.validate_dataset_structure(\n",
    "                    data_path, [split]\n",
    "                )\n",
    "                dataset_info['validation_results'] = validation[split]\n",
    "                \n",
    "                if validation[split]['has_files']:\n",
    "                    # Try to load real dataset\n",
    "                    try:\n",
    "                        from src.data.dataset import MelonFlowerDataset\n",
    "                        dataset = MelonFlowerDataset(data_dir, split=split)\n",
    "                        dataset_info['status'] = 'real_dataset'\n",
    "                        \n",
    "                        # Validate sample data\n",
    "                        sample_validation = self.validator.validate_sample_data(dataset)\n",
    "                        dataset_info['sample_validation'] = sample_validation\n",
    "                        \n",
    "                        if sample_validation['valid_samples'] == 0:\n",
    "                            raise ValueError(\"No valid samples found in dataset\")\n",
    "                            \n",
    "                        logger.info(f\"Real MelonFlower {split}: {len(dataset)} images\")\n",
    "                        \n",
    "                    except ImportError:\n",
    "                        logger.warning(f\"MelonFlowerDataset class not found, using dummy data\")\n",
    "                        dataset = DummyMelonFlowerDataset(data_dir, split=split)\n",
    "                        dataset_info['status'] = 'dummy_dataset'\n",
    "                else:\n",
    "                    logger.warning(f\"No files found in {data_path}, using dummy data\")\n",
    "                    dataset = DummyMelonFlowerDataset(data_dir, split=split)\n",
    "                    dataset_info['status'] = 'dummy_dataset'\n",
    "            else:\n",
    "                logger.info(f\"Data directory {data_path} not found, using dummy data\")\n",
    "                dataset = DummyMelonFlowerDataset(data_dir, split=split)\n",
    "                dataset_info['status'] = 'dummy_dataset'\n",
    "                \n",
    "            # Collect dataset statistics\n",
    "            dataset_info.update({\n",
    "                'size': len(dataset),\n",
    "                'classes': getattr(dataset, 'class_names', melonflower_config['expected_classes']),\n",
    "                'num_classes': len(getattr(dataset, 'class_names', melonflower_config['expected_classes'])),\n",
    "                'domain': 'agricultural_flower_detection',\n",
    "                'target_size': melonflower_config['target_size'],\n",
    "                'color_channels': melonflower_config['color_channels']\n",
    "            })\n",
    "            \n",
    "            return dataset, dataset_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {split} dataset: {e}\")\n",
    "            # Final fallback\n",
    "            dataset = DummyMelonFlowerDataset(data_dir, split=split)\n",
    "            dataset_info.update({\n",
    "                'status': 'fallback_dummy',\n",
    "                'error': str(e),\n",
    "                'size': len(dataset),\n",
    "                'classes': melonflower_config['expected_classes'],\n",
    "                'num_classes': len(melonflower_config['expected_classes']),\n",
    "                'domain': 'agricultural_flower_detection',\n",
    "                'target_size': melonflower_config['target_size'],\n",
    "                'color_channels': melonflower_config['color_channels']\n",
    "            })\n",
    "            \n",
    "            return dataset, dataset_info\n",
    "\n",
    "def load_and_validate_melonflower_datasets() -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Load and validate MelonFlower datasets with comprehensive error handling\"\"\"\n",
    "    \n",
    "    # Enhanced configuration with validation\n",
    "    melonflower_config = {\n",
    "        'data_dir': '../data/MelonFlower',\n",
    "        'splits': ['train', 'val', 'test'],\n",
    "        'expected_classes': ['flower', 'bud', 'mature_flower', 'withered_flower'],\n",
    "        'target_size': (640, 640),\n",
    "        'color_channels': 3,\n",
    "        'annotation_format': 'YOLO',\n",
    "        'expected_min_samples': {\n",
    "            'train': 100,\n",
    "            'val': 20,\n",
    "            'test': 10\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    datasets = {}\n",
    "    dataset_stats = {}\n",
    "    loader = MelonFlowerDatasetLoader(config)\n",
    "    \n",
    "    logger.info(\"Loading MelonFlower datasets with validation\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Load datasets with parallel processing for efficiency\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                loader.load_single_dataset, \n",
    "                melonflower_config['data_dir'], \n",
    "                split, \n",
    "                melonflower_config\n",
    "            ): split for split in melonflower_config['splits']\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            split = futures[future]\n",
    "            try:\n",
    "                dataset, dataset_info = future.result()\n",
    "                datasets[f'MelonFlower_{split}'] = dataset\n",
    "                dataset_stats[f'MelonFlower_{split}'] = dataset_info\n",
    "                \n",
    "                # Validate minimum sample requirements\n",
    "                min_samples = melonflower_config['expected_min_samples'].get(split, 10)\n",
    "                if dataset_info['size'] < min_samples:\n",
    "                    logger.warning(f\"Dataset {split} has {dataset_info['size']} samples, expected minimum {min_samples}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process {split} dataset: {e}\")\n",
    "    \n",
    "    # Log final statistics\n",
    "    total_real = sum(1 for stats in dataset_stats.values() if stats['status'] == 'real_dataset')\n",
    "    total_dummy = len(dataset_stats) - total_real\n",
    "    \n",
    "    logger.info(f\"Dataset loading complete: {total_real} real, {total_dummy} dummy datasets\")\n",
    "    \n",
    "    return datasets, dataset_stats, melonflower_config\n",
    "\n",
    "def analyze_dataset_composition(datasets: Dict, dataset_stats: Dict) -> Dict:\n",
    "    \"\"\"Analyze overall dataset composition and characteristics with validation\"\"\"\n",
    "    \n",
    "    logger.info(\"Analyzing dataset composition\")\n",
    "    \n",
    "    if not datasets or not dataset_stats:\n",
    "        logger.error(\"No datasets available for composition analysis\")\n",
    "        return {}\n",
    "    \n",
    "    composition_analysis = {\n",
    "        'total_datasets': len(datasets),\n",
    "        'total_images': sum(stats['size'] for stats in dataset_stats.values()),\n",
    "        'split_distribution': {},\n",
    "        'class_distribution': {},\n",
    "        'domain_characteristics': {},\n",
    "        'quality_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze split distribution\n",
    "    for dataset_name, stats in dataset_stats.items():\n",
    "        split = stats['split']\n",
    "        composition_analysis['split_distribution'][split] = (\n",
    "            composition_analysis['split_distribution'].get(split, 0) + stats['size']\n",
    "        )\n",
    "    \n",
    "    # Analyze class distribution with validation\n",
    "    all_classes = set()\n",
    "    class_consistency = True\n",
    "    expected_classes = None\n",
    "    \n",
    "    for stats in dataset_stats.values():\n",
    "        current_classes = set(stats['classes'])\n",
    "        all_classes.update(current_classes)\n",
    "        \n",
    "        if expected_classes is None:\n",
    "            expected_classes = current_classes\n",
    "        elif expected_classes != current_classes:\n",
    "            class_consistency = False\n",
    "            logger.warning(\"Inconsistent class definitions across datasets\")\n",
    "    \n",
    "    composition_analysis['class_distribution'] = {\n",
    "        'unique_classes': sorted(list(all_classes)),\n",
    "        'num_unique_classes': len(all_classes),\n",
    "        'class_consistency': class_consistency\n",
    "    }\n",
    "    \n",
    "    # Enhanced domain characteristics\n",
    "    composition_analysis['domain_characteristics'] = {\n",
    "        'primary_domain': 'agricultural_flower_detection',\n",
    "        'object_types': ['flowers', 'buds', 'mature_flowers', 'withered_flowers'],\n",
    "        'detection_challenges': [\n",
    "            'color_similarity_background',\n",
    "            'bloom_stage_variations',\n",
    "            'size_scale_differences',\n",
    "            'temporal_appearance_changes',\n",
    "            'environmental_lighting_effects',\n",
    "            'pollination_state_identification'\n",
    "        ],\n",
    "        'agricultural_context': 'crop_monitoring_pollination_assessment',\n",
    "        'seasonal_factors': ['spring_emergence', 'summer_peak', 'autumn_senescence']\n",
    "    }\n",
    "    \n",
    "    # Quality metrics\n",
    "    composition_analysis['quality_metrics'] = {\n",
    "        'real_datasets': sum(1 for stats in dataset_stats.values() if stats.get('status') == 'real_dataset'),\n",
    "        'dummy_datasets': sum(1 for stats in dataset_stats.values() if 'dummy' in stats.get('status', '')),\n",
    "        'total_validation_errors': sum(len(stats.get('sample_validation', {}).get('errors', [])) for stats in dataset_stats.values()),\n",
    "        'average_dataset_size': np.mean([stats['size'] for stats in dataset_stats.values()]) if dataset_stats else 0\n",
    "    }\n",
    "    \n",
    "    return composition_analysis\n",
    "\n",
    "def create_dataset_overview_visualization(datasets: Dict, dataset_stats: Dict, composition_analysis: Dict):\n",
    "    \"\"\"Create comprehensive dataset overview visualization with error handling\"\"\"\n",
    "    \n",
    "    if not datasets or not dataset_stats or not composition_analysis:\n",
    "        logger.error(\"Insufficient data for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Use memory-efficient figure creation\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Dataset size distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        dataset_names = list(dataset_stats.keys())\n",
    "        dataset_sizes = [stats['size'] for stats in dataset_stats.values()]\n",
    "        colors = sns.color_palette(\"husl\", len(dataset_names))\n",
    "        \n",
    "        bars = ax1.bar(range(len(dataset_names)), dataset_sizes, color=colors, alpha=0.8)\n",
    "        ax1.set_title('Dataset Size Distribution', fontweight='bold', fontsize=12)\n",
    "        ax1.set_ylabel('Number of Images')\n",
    "        ax1.set_xticks(range(len(dataset_names)))\n",
    "        ax1.set_xticklabels([name.replace('MelonFlower_', '') for name in dataset_names])\n",
    "        \n",
    "        # Add value labels efficiently\n",
    "        for bar, size in zip(bars, dataset_sizes):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + max(dataset_sizes)*0.01,\n",
    "                    f'{size}', ha='center', va='bottom', fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Split distribution pie chart\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        split_dist = composition_analysis['split_distribution']\n",
    "        split_labels = list(split_dist.keys())\n",
    "        split_values = list(split_dist.values())\n",
    "        \n",
    "        wedges, texts, autotexts = ax2.pie(split_values, labels=split_labels, autopct='%1.1f%%', \n",
    "                                          startangle=90, colors=sns.color_palette(\"Set2\", len(split_labels)))\n",
    "        ax2.set_title('Data Split Distribution', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # 3. Class distribution\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        class_info = composition_analysis['class_distribution']\n",
    "        class_names = class_info['unique_classes']\n",
    "        class_counts = [1] * len(class_names)  # Equal representation for visualization\n",
    "        \n",
    "        ax3.bar(range(len(class_names)), class_counts, \n",
    "               color=sns.color_palette(\"viridis\", len(class_names)), alpha=0.8)\n",
    "        ax3.set_title('Flower Class Types', fontweight='bold', fontsize=12)\n",
    "        ax3.set_ylabel('Relative Representation')\n",
    "        ax3.set_xticks(range(len(class_names)))\n",
    "        ax3.set_xticklabels([name.replace('_', '\\n') for name in class_names], fontsize=9)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Detection challenges\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        challenges = composition_analysis['domain_characteristics']['detection_challenges']\n",
    "        challenge_weights = [3, 2, 3, 2, 1, 2]  # Relative importance weights\n",
    "        \n",
    "        y_pos = range(len(challenges))\n",
    "        bars = ax4.barh(y_pos, challenge_weights, \n",
    "                       color=sns.color_palette(\"Reds\", len(challenges)), alpha=0.8)\n",
    "        ax4.set_title('Detection Challenge Severity', fontweight='bold', fontsize=12)\n",
    "        ax4.set_xlabel('Relative Difficulty')\n",
    "        ax4.set_yticks(y_pos)\n",
    "        ax4.set_yticklabels([c.replace('_', ' ').title() for c in challenges], fontsize=9)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Quality metrics\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "        quality_metrics = composition_analysis['quality_metrics']\n",
    "        quality_labels = ['Real\\nDatasets', 'Dummy\\nDatasets', 'Validation\\nErrors']\n",
    "        quality_values = [\n",
    "            quality_metrics['real_datasets'],\n",
    "            quality_metrics['dummy_datasets'],\n",
    "            quality_metrics['total_validation_errors']\n",
    "        ]\n",
    "        \n",
    "        colors_quality = ['green', 'orange', 'red']\n",
    "        bars = ax5.bar(range(len(quality_labels)), quality_values, \n",
    "                      color=colors_quality, alpha=0.8)\n",
    "        ax5.set_title('Dataset Quality Metrics', fontweight='bold', fontsize=12)\n",
    "        ax5.set_ylabel('Count')\n",
    "        ax5.set_xticks(range(len(quality_labels)))\n",
    "        ax5.set_xticklabels(quality_labels, fontsize=9)\n",
    "        \n",
    "        for bar, value in zip(bars, quality_values):\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height + max(quality_values)*0.01,\n",
    "                    f'{value}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 6. Seasonal factors\n",
    "        ax6 = fig.add_subplot(gs[1, 2])\n",
    "        seasonal_factors = composition_analysis['domain_characteristics']['seasonal_factors']\n",
    "        seasonal_weights = [3, 5, 2]  # Spring, Summer peak, Autumn\n",
    "        \n",
    "        ax6.bar(range(len(seasonal_factors)), seasonal_weights,\n",
    "               color=['lightgreen', 'gold', 'orange'], alpha=0.8)\n",
    "        ax6.set_title('Seasonal Factor Importance', fontweight='bold', fontsize=12)\n",
    "        ax6.set_ylabel('Relative Importance')\n",
    "        ax6.set_xticks(range(len(seasonal_factors)))\n",
    "        ax6.set_xticklabels([f.replace('_', '\\n').title() for f in seasonal_factors], fontsize=9)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(notebook_results_dir / 'visualizations' / 'dataset_overview.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Clear figure to save memory\n",
    "        plt.close(fig)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create visualization: {e}\")\n",
    "        plt.close('all')  # Ensure cleanup on error\n",
    "\n",
    "# Execute dataset loading and analysis with enhanced error handling\n",
    "def execute_dataset_loading():\n",
    "    \"\"\"Execute the complete dataset loading pipeline\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        datasets, dataset_stats, melonflower_config = safe_operation(\n",
    "            \"Loading MelonFlower datasets\", \n",
    "            load_and_validate_melonflower_datasets\n",
    "        )\n",
    "        \n",
    "        if not datasets or not dataset_stats:\n",
    "            raise AnalysisError(\"Failed to load any datasets\")\n",
    "        \n",
    "        # Analyze composition\n",
    "        composition_analysis = safe_operation(\n",
    "            \"Analyzing dataset composition\",\n",
    "            analyze_dataset_composition,\n",
    "            datasets, dataset_stats\n",
    "        )\n",
    "        \n",
    "        if not composition_analysis:\n",
    "            raise AnalysisError(\"Failed to analyze dataset composition\")\n",
    "        \n",
    "        # Display results\n",
    "        logger.info(\"MelonFlower Dataset Overview:\")\n",
    "        logger.info(f\"Total datasets: {composition_analysis['total_datasets']}\")\n",
    "        logger.info(f\"Total images: {composition_analysis['total_images']}\")\n",
    "        logger.info(f\"Domain: {composition_analysis['domain_characteristics']['primary_domain']}\")\n",
    "        logger.info(f\"Unique classes: {composition_analysis['class_distribution']['num_unique_classes']}\")\n",
    "        \n",
    "        # Log split distribution\n",
    "        for split, count in composition_analysis['split_distribution'].items():\n",
    "            percentage = (count / composition_analysis['total_images']) * 100\n",
    "            logger.info(f\"{split.capitalize()}: {count} images ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Create visualization\n",
    "        safe_operation(\n",
    "            \"Creating dataset overview visualization\",\n",
    "            create_dataset_overview_visualization,\n",
    "            datasets, dataset_stats, composition_analysis\n",
    "        )\n",
    "        \n",
    "        # Save statistics with error handling\n",
    "        try:\n",
    "            output_path = notebook_results_dir / 'statistics' / 'dataset_overview.json'\n",
    "            serializable_analysis = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_datasets': composition_analysis['total_datasets'],\n",
    "                'total_images': composition_analysis['total_images'],\n",
    "                'split_distribution': composition_analysis['split_distribution'],\n",
    "                'class_distribution': composition_analysis['class_distribution'],\n",
    "                'domain_characteristics': composition_analysis['domain_characteristics'],\n",
    "                'quality_metrics': composition_analysis['quality_metrics'],\n",
    "                'dataset_details': dataset_stats\n",
    "            }\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(serializable_analysis, f, indent=2, default=str)\n",
    "            \n",
    "            logger.info(f\"Dataset overview saved to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save dataset overview: {e}\")\n",
    "        \n",
    "        return datasets, dataset_stats, composition_analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Dataset loading pipeline failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Execute the pipeline\n",
    "datasets, dataset_stats, composition_analysis = execute_dataset_loading()\n",
    "\n",
    "if datasets is None:\n",
    "    logger.error(\"Dataset loading failed completely. Please check data paths and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd23f3e",
   "metadata": {},
   "source": [
    "## 3. Flower Distribution and Bloom Stage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced flower distribution and bloom stage analysis with comprehensive metrics\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "@dataclass\n",
    "class FlowerAnalysisThresholds:\n",
    "    \"\"\"Centralized thresholds for flower analysis with scientific basis\"\"\"\n",
    "    # Size thresholds based on agricultural literature\n",
    "    tiny_flower_area: float = 0.003      # Very small buds\n",
    "    small_flower_area: float = 0.015     # Small flowers  \n",
    "    medium_flower_area: float = 0.05     # Medium flowers\n",
    "    large_flower_area: float = 0.15      # Large flowers\n",
    "    \n",
    "    # Spatial analysis thresholds\n",
    "    edge_preference_threshold: float = 0.2\n",
    "    center_preference_threshold: float = 0.3\n",
    "    clustering_distance_threshold: float = 0.1\n",
    "    \n",
    "    # Temporal classification thresholds\n",
    "    early_bloom_size: float = 0.02\n",
    "    peak_bloom_size: float = 0.05\n",
    "    \n",
    "    # Density categorization\n",
    "    sparse_flower_count: int = 1\n",
    "    moderate_flower_count: int = 3\n",
    "    dense_flower_count: int = 6\n",
    "\n",
    "class FlowerDistributionAnalyzer:\n",
    "    \"\"\"Modular analyzer for flower distribution with batch processing\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: FlowerAnalysisThresholds = None):\n",
    "        self.thresholds = thresholds or FlowerAnalysisThresholds()\n",
    "        self.results = self._initialize_metrics()\n",
    "    \n",
    "    def _initialize_metrics(self) -> Dict:\n",
    "        \"\"\"Initialize metrics structure\"\"\"\n",
    "        return {\n",
    "            'basic_statistics': {\n",
    "                'total_flowers': 0,\n",
    "                'images_analyzed': 0,\n",
    "                'flowers_per_image': [],\n",
    "                'images_with_flowers': 0,\n",
    "                'empty_images': 0\n",
    "            },\n",
    "            'bloom_stage_analysis': {\n",
    "                'stage_distribution': defaultdict(int),\n",
    "                'stage_size_correlation': [],\n",
    "                'stage_color_correlation': []\n",
    "            },\n",
    "            'spatial_analysis': {\n",
    "                'flower_positions': {'x_coords': [], 'y_coords': []},\n",
    "                'clustering_coefficient': 0,\n",
    "                'spatial_entropy': 0,\n",
    "                'edge_preference': 0,\n",
    "                'center_preference': 0\n",
    "            },\n",
    "            'size_analysis': {\n",
    "                'size_distribution': {'tiny': 0, 'small': 0, 'medium': 0, 'large': 0, 'huge': 0},\n",
    "                'size_statistics': [],\n",
    "                'aspect_ratio_distribution': [],\n",
    "                'size_variability_per_image': []\n",
    "            },\n",
    "            'density_analysis': {\n",
    "                'density_categories': {'sparse': 0, 'moderate': 0, 'dense': 0, 'very_dense': 0},\n",
    "                'density_correlation_with_size': [],\n",
    "                'crowding_effects': []\n",
    "            },\n",
    "            'temporal_indicators': {\n",
    "                'early_bloom': 0, \n",
    "                'peak_bloom': 0, \n",
    "                'late_bloom': 0,\n",
    "                'mixed_stages': 0\n",
    "            },\n",
    "            'morphological_diversity': {\n",
    "                'shape_variety': [],\n",
    "                'size_diversity_index': [],\n",
    "                'color_diversity_index': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_single_image(self, image_data: Tuple) -> Dict:\n",
    "        \"\"\"Analyze a single image for flower distribution\"\"\"\n",
    "        idx, targets, dataset = image_data\n",
    "        image_metrics = {\n",
    "            'flowers_in_image': 0,\n",
    "            'flower_data': [],\n",
    "            'positions': [],\n",
    "            'sizes': [],\n",
    "            'aspects': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if targets.numel() == 0:\n",
    "                return {'empty_image': True, 'metrics': image_metrics}\n",
    "            \n",
    "            num_flowers = len(targets)\n",
    "            image_metrics['flowers_in_image'] = num_flowers\n",
    "            \n",
    "            # Process each flower in the image\n",
    "            for target in targets:\n",
    "                if len(target) >= 5:\n",
    "                    flower_data = self._process_single_flower(target, dataset)\n",
    "                    if flower_data:\n",
    "                        image_metrics['flower_data'].append(flower_data)\n",
    "                        image_metrics['positions'].append(flower_data['position'])\n",
    "                        image_metrics['sizes'].append(flower_data['area'])\n",
    "                        image_metrics['aspects'].append(flower_data['aspect_ratio'])\n",
    "            \n",
    "            # Calculate image-level metrics\n",
    "            if image_metrics['sizes']:\n",
    "                image_metrics['size_variability'] = np.std(image_metrics['sizes']) / (np.mean(image_metrics['sizes']) + 1e-6)\n",
    "                image_metrics['morphological_diversity'] = self._calculate_morphological_diversity(image_metrics)\n",
    "            \n",
    "            return {'empty_image': False, 'metrics': image_metrics}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to analyze image {idx}: {e}\")\n",
    "            return {'empty_image': True, 'metrics': image_metrics, 'error': str(e)}\n",
    "    \n",
    "    def _process_single_flower(self, target: torch.Tensor, dataset) -> Dict:\n",
    "        \"\"\"Process individual flower data\"\"\"\n",
    "        try:\n",
    "            cls, x_center, y_center, width, height = target[:5]\n",
    "            \n",
    "            area = width * height\n",
    "            aspect_ratio = width / height if height > 0 else 1.0\n",
    "            \n",
    "            flower_data = {\n",
    "                'class': int(cls),\n",
    "                'position': [float(x_center), float(y_center)],\n",
    "                'area': float(area),\n",
    "                'aspect_ratio': float(aspect_ratio),\n",
    "                'size_category': self._categorize_size(area),\n",
    "                'stage_name': self._get_stage_name(int(cls), dataset)\n",
    "            }\n",
    "            \n",
    "            return flower_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Failed to process flower: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _categorize_size(self, area: float) -> str:\n",
    "        \"\"\"Categorize flower size based on area\"\"\"\n",
    "        if area < self.thresholds.tiny_flower_area:\n",
    "            return 'tiny'\n",
    "        elif area < self.thresholds.small_flower_area:\n",
    "            return 'small'\n",
    "        elif area < self.thresholds.medium_flower_area:\n",
    "            return 'medium'\n",
    "        elif area < self.thresholds.large_flower_area:\n",
    "            return 'large'\n",
    "        else:\n",
    "            return 'huge'\n",
    "    \n",
    "    def _get_stage_name(self, class_int: int, dataset) -> str:\n",
    "        \"\"\"Get stage name from class index\"\"\"\n",
    "        if hasattr(dataset, 'class_names') and class_int < len(dataset.class_names):\n",
    "            return dataset.class_names[class_int]\n",
    "        return f'stage_{class_int}'\n",
    "    \n",
    "    def _calculate_morphological_diversity(self, image_metrics: Dict) -> float:\n",
    "        \"\"\"Calculate morphological diversity within image\"\"\"\n",
    "        if len(image_metrics['sizes']) <= 1:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use entropy of size distribution\n",
    "        hist, _ = np.histogram(image_metrics['sizes'], bins=5)\n",
    "        hist = hist + 1e-6  # Avoid log(0)\n",
    "        normalized_hist = hist / np.sum(hist)\n",
    "        return entropy(normalized_hist)\n",
    "    \n",
    "    def aggregate_results(self, image_results: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate results from all processed images\"\"\"\n",
    "        aggregated = self._initialize_metrics()\n",
    "        \n",
    "        for result in image_results:\n",
    "            if result.get('empty_image', True):\n",
    "                aggregated['basic_statistics']['empty_images'] += 1\n",
    "                aggregated['basic_statistics']['flowers_per_image'].append(0)\n",
    "                aggregated['density_analysis']['density_categories']['sparse'] += 1\n",
    "            else:\n",
    "                metrics = result['metrics']\n",
    "                self._aggregate_image_metrics(aggregated, metrics)\n",
    "        \n",
    "        # Post-process spatial analysis\n",
    "        self._calculate_spatial_metrics(aggregated)\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _aggregate_image_metrics(self, aggregated: Dict, image_metrics: Dict):\n",
    "        \"\"\"Aggregate metrics from a single image\"\"\"\n",
    "        num_flowers = image_metrics['flowers_in_image']\n",
    "        \n",
    "        # Basic statistics\n",
    "        aggregated['basic_statistics']['images_analyzed'] += 1\n",
    "        aggregated['basic_statistics']['total_flowers'] += num_flowers\n",
    "        aggregated['basic_statistics']['flowers_per_image'].append(num_flowers)\n",
    "        aggregated['basic_statistics']['images_with_flowers'] += 1\n",
    "        \n",
    "        # Density categorization\n",
    "        self._categorize_density(aggregated, num_flowers)\n",
    "        \n",
    "        # Process individual flowers\n",
    "        for flower_data in image_metrics['flower_data']:\n",
    "            self._process_flower_for_aggregation(aggregated, flower_data)\n",
    "        \n",
    "        # Image-level metrics\n",
    "        if image_metrics['sizes']:\n",
    "            aggregated['size_analysis']['size_variability_per_image'].append(\n",
    "                image_metrics.get('size_variability', 0)\n",
    "            )\n",
    "            \n",
    "            if 'morphological_diversity' in image_metrics:\n",
    "                aggregated['morphological_diversity']['size_diversity_index'].append(\n",
    "                    image_metrics['morphological_diversity']\n",
    "                )\n",
    "        \n",
    "        # Spatial and temporal analysis\n",
    "        self._analyze_spatial_patterns(aggregated, image_metrics)\n",
    "        self._analyze_temporal_patterns(aggregated, image_metrics)\n",
    "    \n",
    "    def _categorize_density(self, aggregated: Dict, num_flowers: int):\n",
    "        \"\"\"Categorize flower density\"\"\"\n",
    "        if num_flowers <= self.thresholds.sparse_flower_count:\n",
    "            aggregated['density_analysis']['density_categories']['sparse'] += 1\n",
    "        elif num_flowers <= self.thresholds.moderate_flower_count:\n",
    "            aggregated['density_analysis']['density_categories']['moderate'] += 1\n",
    "        elif num_flowers <= self.thresholds.dense_flower_count:\n",
    "            aggregated['density_analysis']['density_categories']['dense'] += 1\n",
    "        else:\n",
    "            aggregated['density_analysis']['density_categories']['very_dense'] += 1\n",
    "    \n",
    "    def _process_flower_for_aggregation(self, aggregated: Dict, flower_data: Dict):\n",
    "        \"\"\"Process individual flower data for aggregation\"\"\"\n",
    "        # Spatial positions\n",
    "        x, y = flower_data['position']\n",
    "        aggregated['spatial_analysis']['flower_positions']['x_coords'].append(x)\n",
    "        aggregated['spatial_analysis']['flower_positions']['y_coords'].append(y)\n",
    "        \n",
    "        # Size analysis\n",
    "        area = flower_data['area']\n",
    "        aggregated['size_analysis']['size_statistics'].append(area)\n",
    "        aggregated['size_analysis']['aspect_ratio_distribution'].append(flower_data['aspect_ratio'])\n",
    "        aggregated['size_analysis']['size_distribution'][flower_data['size_category']] += 1\n",
    "        \n",
    "        # Bloom stage analysis\n",
    "        stage_name = flower_data['stage_name']\n",
    "        aggregated['bloom_stage_analysis']['stage_distribution'][stage_name] += 1\n",
    "        aggregated['bloom_stage_analysis']['stage_size_correlation'].append({\n",
    "            'stage': stage_name,\n",
    "            'size': area,\n",
    "            'aspect_ratio': flower_data['aspect_ratio']\n",
    "        })\n",
    "    \n",
    "    def _analyze_spatial_patterns(self, aggregated: Dict, image_metrics: Dict):\n",
    "        \"\"\"Analyze spatial patterns within image\"\"\"\n",
    "        positions = image_metrics['positions']\n",
    "        \n",
    "        if len(positions) >= 2:\n",
    "            positions_array = np.array(positions)\n",
    "            \n",
    "            # Calculate distances and crowding\n",
    "            distances = pdist(positions_array)\n",
    "            avg_distance = np.mean(distances)\n",
    "            crowding_score = 1 / (avg_distance + 1e-6)\n",
    "            aggregated['density_analysis']['crowding_effects'].append(crowding_score)\n",
    "            \n",
    "            # Edge vs center preference\n",
    "            edge_distances = np.minimum(\n",
    "                np.minimum(positions_array[:, 0], 1 - positions_array[:, 0]),\n",
    "                np.minimum(positions_array[:, 1], 1 - positions_array[:, 1])\n",
    "            )\n",
    "            \n",
    "            edge_flowers = np.sum(edge_distances < self.thresholds.edge_preference_threshold)\n",
    "            center_flowers = np.sum(edge_distances > self.thresholds.center_preference_threshold)\n",
    "            \n",
    "            aggregated['spatial_analysis']['edge_preference'] += edge_flowers\n",
    "            aggregated['spatial_analysis']['center_preference'] += center_flowers\n",
    "    \n",
    "    def _analyze_temporal_patterns(self, aggregated: Dict, image_metrics: Dict):\n",
    "        \"\"\"Analyze temporal bloom patterns\"\"\"\n",
    "        if not image_metrics['flower_data']:\n",
    "            return\n",
    "        \n",
    "        sizes = image_metrics['sizes']\n",
    "        classes = [f['class'] for f in image_metrics['flower_data']]\n",
    "        \n",
    "        unique_classes = len(set(classes))\n",
    "        avg_size = np.mean(sizes)\n",
    "        \n",
    "        # Temporal stage determination\n",
    "        if unique_classes == 1 and avg_size < self.thresholds.early_bloom_size:\n",
    "            aggregated['temporal_indicators']['early_bloom'] += 1\n",
    "        elif unique_classes == 1 and avg_size > self.thresholds.peak_bloom_size:\n",
    "            aggregated['temporal_indicators']['peak_bloom'] += 1\n",
    "        elif unique_classes > 1:\n",
    "            aggregated['temporal_indicators']['mixed_stages'] += 1\n",
    "        else:\n",
    "            aggregated['temporal_indicators']['late_bloom'] += 1\n",
    "    \n",
    "    def _calculate_spatial_metrics(self, aggregated: Dict):\n",
    "        \"\"\"Calculate complex spatial metrics after aggregation\"\"\"\n",
    "        spatial_data = aggregated['spatial_analysis']\n",
    "        x_coords = spatial_data['flower_positions']['x_coords']\n",
    "        y_coords = spatial_data['flower_positions']['y_coords']\n",
    "        \n",
    "        if not x_coords:\n",
    "            return\n",
    "        \n",
    "        # Spatial entropy\n",
    "        try:\n",
    "            hist, _, _ = np.histogram2d(x_coords, y_coords, bins=10, range=[[0, 1], [0, 1]])\n",
    "            hist = hist + 1e-6\n",
    "            hist_norm = hist / np.sum(hist)\n",
    "            spatial_entropy = entropy(hist_norm.flatten())\n",
    "            spatial_data['spatial_entropy'] = float(spatial_entropy)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to calculate spatial entropy: {e}\")\n",
    "            spatial_data['spatial_entropy'] = 0.0\n",
    "        \n",
    "        # Clustering coefficient (optimized version)\n",
    "        if len(x_coords) > 10:\n",
    "            try:\n",
    "                clustering_coeff = self._calculate_clustering_coefficient(x_coords, y_coords)\n",
    "                spatial_data['clustering_coefficient'] = float(clustering_coeff)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to calculate clustering coefficient: {e}\")\n",
    "                spatial_data['clustering_coefficient'] = 0.0\n",
    "    \n",
    "    def _calculate_clustering_coefficient(self, x_coords: List, y_coords: List) -> float:\n",
    "        \"\"\"Calculate clustering coefficient efficiently\"\"\"\n",
    "        positions = np.column_stack([x_coords, y_coords])\n",
    "        \n",
    "        # Sample for efficiency\n",
    "        max_samples = min(100, len(positions))\n",
    "        indices = np.random.choice(len(positions), max_samples, replace=False)\n",
    "        sample_positions = positions[indices]\n",
    "        \n",
    "        # Calculate adjacency matrix\n",
    "        distances = squareform(pdist(sample_positions))\n",
    "        np.fill_diagonal(distances, np.inf)\n",
    "        adjacency = distances < self.thresholds.clustering_distance_threshold\n",
    "        \n",
    "        clustering_scores = []\n",
    "        for i in range(len(sample_positions)):\n",
    "            neighbors = np.where(adjacency[i])[0]\n",
    "            if len(neighbors) > 1:\n",
    "                # Count connections between neighbors\n",
    "                neighbor_connections = np.sum(adjacency[np.ix_(neighbors, neighbors)]) // 2\n",
    "                max_connections = len(neighbors) * (len(neighbors) - 1) // 2\n",
    "                \n",
    "                if max_connections > 0:\n",
    "                    clustering_scores.append(neighbor_connections / max_connections)\n",
    "        \n",
    "        return np.mean(clustering_scores) if clustering_scores else 0.0\n",
    "\n",
    "def analyze_flower_distribution_enhanced(dataset, dataset_name: str, max_samples: int = 500) -> Dict:\n",
    "    \"\"\"Enhanced flower distribution analysis with batch processing and validation\"\"\"\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Invalid dataset for {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(f\"Analyzing flower distribution in {dataset_name}\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = FlowerDistributionAnalyzer()\n",
    "    \n",
    "    # Determine sample size and create batches\n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    # Prepare data for batch processing\n",
    "    batch_size = config.max_batch_size\n",
    "    image_data = []\n",
    "    \n",
    "    for i in indices:\n",
    "        try:\n",
    "            _, targets, _ = dataset[i]\n",
    "            image_data.append((i, targets, dataset))\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Failed to load image {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not image_data:\n",
    "        logger.error(f\"No valid images found in {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Process images in batches\n",
    "    image_results = []\n",
    "    for i in range(0, len(image_data), batch_size):\n",
    "        batch = image_data[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Process batch\n",
    "            batch_results = [analyzer.analyze_single_image(data) for data in batch]\n",
    "            image_results.extend(batch_results)\n",
    "            \n",
    "            # Memory cleanup every few batches\n",
    "            if (i // batch_size) % 5 == 0:\n",
    "                MemoryManager.clear_memory()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch processing failed for {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not image_results:\n",
    "        logger.error(f\"No images processed successfully for {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Aggregate results\n",
    "    try:\n",
    "        final_results = analyzer.aggregate_results(image_results)\n",
    "        \n",
    "        # Add metadata\n",
    "        final_results['metadata'] = {\n",
    "            'dataset_name': dataset_name,\n",
    "            'sample_size': len(image_results),\n",
    "            'processing_timestamp': datetime.now().isoformat(),\n",
    "            'thresholds_used': analyzer.thresholds.__dict__\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Successfully analyzed {len(image_results)} images from {dataset_name}\")\n",
    "        return final_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to aggregate results for {dataset_name}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_flower_distribution_visualization(flower_distribution_results: Dict):\n",
    "    \"\"\"Create memory-efficient flower distribution visualization\"\"\"\n",
    "    \n",
    "    if not flower_distribution_results:\n",
    "        logger.error(\"No results available for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create figure with memory-efficient settings\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        for idx, (dataset_name, metrics) in enumerate(flower_distribution_results.items()):\n",
    "            if idx >= 1:  # Limit to one dataset for memory efficiency\n",
    "                break\n",
    "                \n",
    "            # Create focused visualizations for key metrics\n",
    "            create_distribution_plots(fig, gs, dataset_name, metrics)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with compression\n",
    "        output_path = notebook_results_dir / 'visualizations' / 'flower_distribution_comprehensive.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', optimize=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        plt.close(fig)\n",
    "        MemoryManager.clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "\n",
    "def create_distribution_plots(fig, gs, dataset_name: str, metrics: Dict):\n",
    "    \"\"\"Create individual distribution plots\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Flowers per image distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        flowers_per_image = metrics['basic_statistics']['flowers_per_image']\n",
    "        if flowers_per_image:\n",
    "            ax1.hist(flowers_per_image, bins=range(max(flowers_per_image)+2), alpha=0.7, density=True)\n",
    "            ax1.set_title('Flowers per Image', fontweight='bold', fontsize=11)\n",
    "            ax1.set_xlabel('Number of Flowers')\n",
    "            ax1.set_ylabel('Density')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Size distribution\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        size_dist = metrics['size_analysis']['size_distribution']\n",
    "        size_labels = list(size_dist.keys())\n",
    "        size_values = list(size_dist.values())\n",
    "        \n",
    "        if any(size_values):\n",
    "            ax2.pie(size_values, labels=[s.title() for s in size_labels], autopct='%1.1f%%',\n",
    "                   colors=sns.color_palette(\"viridis\", len(size_labels)))\n",
    "            ax2.set_title(f'Size Distribution\\n{dataset_name}', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # 3. Spatial entropy heatmap\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        spatial_data = metrics['spatial_analysis']\n",
    "        x_coords = spatial_data['flower_positions']['x_coords']\n",
    "        y_coords = spatial_data['flower_positions']['y_coords']\n",
    "        \n",
    "        if x_coords and y_coords:\n",
    "            hist, _, _ = np.histogram2d(x_coords, y_coords, bins=15, range=[[0, 1], [0, 1]])\n",
    "            im = ax3.imshow(hist.T, origin='lower', extent=[0, 1, 0, 1], cmap='viridis', alpha=0.8)\n",
    "            ax3.set_title('Spatial Distribution', fontweight='bold', fontsize=11)\n",
    "            ax3.set_xlabel('X Coordinate')\n",
    "            ax3.set_ylabel('Y Coordinate')\n",
    "        \n",
    "        # 4. Summary statistics\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        basic_stats = metrics['basic_statistics']\n",
    "        \n",
    "        summary_text = f\"\"\"Summary - {dataset_name}\n",
    "        \n",
    "Images: {basic_stats['images_analyzed']}\n",
    "Flowers: {basic_stats['total_flowers']}\n",
    "Avg/Image: {np.mean(basic_stats['flowers_per_image']):.1f}\n",
    "Empty Images: {basic_stats['empty_images']}\n",
    "\n",
    "Spatial Entropy: {spatial_data['spatial_entropy']:.3f}\n",
    "Clustering: {spatial_data['clustering_coefficient']:.3f}\"\"\"\n",
    "        \n",
    "        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
    "                fontsize=9, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "        ax4.set_title('Statistics Summary', fontweight='bold', fontsize=11)\n",
    "        ax4.axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create distribution plots: {e}\")\n",
    "\n",
    "# Execute enhanced flower distribution analysis with error handling\n",
    "def execute_flower_distribution_analysis():\n",
    "    \"\"\"Execute the complete flower distribution analysis pipeline\"\"\"\n",
    "    \n",
    "    if 'datasets' not in locals() or not datasets:\n",
    "        logger.error(\"No datasets available for flower distribution analysis\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Starting comprehensive flower distribution analysis\")\n",
    "    \n",
    "    flower_distribution_results = {}\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        logger.info(f\"Analyzing {name}\")\n",
    "        \n",
    "        try:\n",
    "            flower_metrics = safe_operation(\n",
    "                f\"Flower distribution analysis for {name}\",\n",
    "                analyze_flower_distribution_enhanced,\n",
    "                dataset, name, 400\n",
    "            )\n",
    "            \n",
    "            if flower_metrics and 'basic_statistics' in flower_metrics:\n",
    "                flower_distribution_results[name] = flower_metrics\n",
    "                \n",
    "                # Log key findings\n",
    "                basic_stats = flower_metrics['basic_statistics']\n",
    "                logger.info(f\"Analysis complete for {name}:\")\n",
    "                logger.info(f\"  Total flowers: {basic_stats['total_flowers']}\")\n",
    "                logger.info(f\"  Images analyzed: {basic_stats['images_analyzed']}\")\n",
    "                logger.info(f\"  Images with flowers: {basic_stats['images_with_flowers']}\")\n",
    "                \n",
    "            else:\n",
    "                logger.warning(f\"Analysis failed or returned empty results for {name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to analyze {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create visualization if we have results\n",
    "    if flower_distribution_results:\n",
    "        safe_operation(\n",
    "            \"Creating flower distribution visualization\",\n",
    "            create_flower_distribution_visualization,\n",
    "            flower_distribution_results\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            save_flower_distribution_results(flower_distribution_results)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save results: {e}\")\n",
    "    \n",
    "    return flower_distribution_results\n",
    "\n",
    "def save_flower_distribution_results(results: Dict):\n",
    "    \"\"\"Save flower distribution results with proper serialization\"\"\"\n",
    "    \n",
    "    serializable_results = {}\n",
    "    \n",
    "    for dataset_name, metrics in results.items():\n",
    "        basic_stats = metrics['basic_statistics']\n",
    "        \n",
    "        serializable_results[dataset_name] = {\n",
    "            'basic_statistics': {\n",
    "                'total_flowers': basic_stats['total_flowers'],\n",
    "                'images_analyzed': basic_stats['images_analyzed'],\n",
    "                'images_with_flowers': basic_stats['images_with_flowers'],\n",
    "                'empty_images': basic_stats['empty_images'],\n",
    "                'avg_flowers_per_image': float(np.mean(basic_stats['flowers_per_image'])) if basic_stats['flowers_per_image'] else 0,\n",
    "                'std_flowers_per_image': float(np.std(basic_stats['flowers_per_image'])) if basic_stats['flowers_per_image'] else 0,\n",
    "                'max_flowers_per_image': int(max(basic_stats['flowers_per_image'])) if basic_stats['flowers_per_image'] else 0,\n",
    "                'min_flowers_per_image': int(min(basic_stats['flowers_per_image'])) if basic_stats['flowers_per_image'] else 0\n",
    "            },\n",
    "            'size_analysis': {\n",
    "                'size_distribution': metrics['size_analysis']['size_distribution'],\n",
    "                'avg_size': float(np.mean(metrics['size_analysis']['size_statistics'])) if metrics['size_analysis']['size_statistics'] else 0,\n",
    "                'size_std': float(np.std(metrics['size_analysis']['size_statistics'])) if metrics['size_analysis']['size_statistics'] else 0,\n",
    "                'avg_aspect_ratio': float(np.mean(metrics['size_analysis']['aspect_ratio_distribution'])) if metrics['size_analysis']['aspect_ratio_distribution'] else 0\n",
    "            },\n",
    "            'spatial_analysis': {\n",
    "                'spatial_entropy': float(metrics['spatial_analysis']['spatial_entropy']),\n",
    "                'clustering_coefficient': float(metrics['spatial_analysis']['clustering_coefficient']),\n",
    "                'edge_preference_ratio': float(metrics['spatial_analysis']['edge_preference'] / max(1, metrics['basic_statistics']['total_flowers'])),\n",
    "                'center_preference_ratio': float(metrics['spatial_analysis']['center_preference'] / max(1, metrics['basic_statistics']['total_flowers']))\n",
    "            },\n",
    "            'density_analysis': metrics['density_analysis']['density_categories'],\n",
    "            'bloom_stage_analysis': dict(metrics['bloom_stage_analysis']['stage_distribution']),\n",
    "            'temporal_indicators': metrics['temporal_indicators'],\n",
    "            'metadata': metrics.get('metadata', {})\n",
    "        }\n",
    "    \n",
    "    output_path = notebook_results_dir / 'statistics' / 'flower_distribution_analysis.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Execute the analysis\n",
    "flower_distribution_results = execute_flower_distribution_analysis()\n",
    "\n",
    "if not flower_distribution_results:\n",
    "    logger.error(\"Flower distribution analysis failed completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f758a",
   "metadata": {},
   "source": [
    "## 4. Color Analysis and Flower Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced color analysis with advanced flower characteristic assessment\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "@dataclass\n",
    "class ColorAnalysisThresholds:\n",
    "    \"\"\"Centralized color analysis thresholds with scientific basis\"\"\"\n",
    "    # Health classification thresholds\n",
    "    vibrant_saturation: float = 0.6    # High saturation for vibrant flowers\n",
    "    vibrant_brightness: float = 0.4    # Minimum brightness for vibrant flowers\n",
    "    vibrant_hue_variance: float = 0.05  # Low hue variance for healthy flowers\n",
    "    \n",
    "    moderate_saturation: float = 0.3\n",
    "    moderate_brightness: float = 0.25\n",
    "    \n",
    "    diseased_hue_variance: float = 0.1\n",
    "    diseased_saturation: float = 0.2\n",
    "    \n",
    "    # Pollination contrast thresholds\n",
    "    high_brightness_contrast: float = 0.3\n",
    "    high_saturation_contrast: float = 0.4\n",
    "    medium_brightness_contrast: float = 0.15\n",
    "    medium_saturation_contrast: float = 0.2\n",
    "    \n",
    "    # Background similarity thresholds\n",
    "    high_similarity: float = 0.8\n",
    "    medium_similarity: float = 0.6\n",
    "    \n",
    "    # Seasonal classification hue ranges\n",
    "    spring_hue_range: Tuple[float, float] = (0.0, 0.1)  # Red/Pink\n",
    "    spring_hue_range_alt: Tuple[float, float] = (0.8, 1.0)\n",
    "    summer_hue_range: Tuple[float, float] = (0.1, 0.3)  # Yellow/Orange  \n",
    "    winter_hue_range: Tuple[float, float] = (0.3, 0.7)  # Green/Blue\n",
    "    autumn_hue_range: Tuple[float, float] = (0.7, 0.8)  # Purple\n",
    "\n",
    "class ColorSpaceConverter:\n",
    "    \"\"\"Handles color space conversions with error handling and caching\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=128)\n",
    "    def convert_image_to_colorspaces(image_hash: str, img_np: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Convert image to multiple color spaces with caching\"\"\"\n",
    "        try:\n",
    "            colorspaces = {\n",
    "                'hsv': color.rgb2hsv(img_np),\n",
    "                'lab': color.rgb2lab(img_np)\n",
    "            }\n",
    "            return colorspaces\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Color space conversion failed: {e}\")\n",
    "            return {\n",
    "                'hsv': np.zeros_like(img_np),\n",
    "                'lab': np.zeros_like(img_np)\n",
    "            }\n",
    "\n",
    "class FlowerColorAnalyzer:\n",
    "    \"\"\"Modular analyzer for flower color characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: ColorAnalysisThresholds = None):\n",
    "        self.thresholds = thresholds or ColorAnalysisThresholds()\n",
    "        self.converter = ColorSpaceConverter()\n",
    "        self.results = self._initialize_metrics()\n",
    "    \n",
    "    def _initialize_metrics(self) -> Dict:\n",
    "        \"\"\"Initialize color metrics structure\"\"\"\n",
    "        return {\n",
    "            'color_space_analysis': {\n",
    "                'rgb_distributions': {'r': [], 'g': [], 'b': []},\n",
    "                'hsv_distributions': {'hue': [], 'saturation': [], 'value': []},\n",
    "                'lab_distributions': {'l': [], 'a': [], 'b': []}\n",
    "            },\n",
    "            'dominant_colors': {\n",
    "                'cluster_centers_rgb': [],\n",
    "                'cluster_centers_hsv': [],\n",
    "                'cluster_sizes': [],\n",
    "                'color_dominance_scores': []\n",
    "            },\n",
    "            'flower_specific_colors': {\n",
    "                'petal_colors': [],\n",
    "                'center_colors': [],\n",
    "                'background_colors': [],\n",
    "                'color_contrast_scores': []\n",
    "            },\n",
    "            'seasonal_color_analysis': {\n",
    "                'spring_indicators': {'count': 0, 'colors': []},\n",
    "                'summer_indicators': {'count': 0, 'colors': []},\n",
    "                'autumn_indicators': {'count': 0, 'colors': []},\n",
    "                'winter_indicators': {'count': 0, 'colors': []}\n",
    "            },\n",
    "            'color_uniformity': {\n",
    "                'within_flower_uniformity': [],\n",
    "                'between_flower_uniformity': [],\n",
    "                'image_level_uniformity': []\n",
    "            },\n",
    "            'color_health_indicators': {\n",
    "                'vibrant_flowers': 0,\n",
    "                'moderate_flowers': 0,\n",
    "                'faded_flowers': 0,\n",
    "                'diseased_indicators': 0\n",
    "            },\n",
    "            'background_similarity': {\n",
    "                'high_similarity': 0,\n",
    "                'medium_similarity': 0,\n",
    "                'low_similarity': 0,\n",
    "                'similarity_scores': []\n",
    "            },\n",
    "            'pollination_color_cues': {\n",
    "                'high_contrast_centers': 0,\n",
    "                'medium_contrast_centers': 0,\n",
    "                'low_contrast_centers': 0,\n",
    "                'uv_reflection_indicators': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_image_batch(self, image_batch: List[Tuple]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of images for color analysis\"\"\"\n",
    "        batch_results = []\n",
    "        all_flower_pixels = []\n",
    "        all_background_pixels = []\n",
    "        \n",
    "        for image_data in image_batch:\n",
    "            try:\n",
    "                result = self.analyze_single_image(image_data, all_flower_pixels, all_background_pixels)\n",
    "                batch_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to process image in batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Process clustering for the batch\n",
    "        if len(all_flower_pixels) > 50:\n",
    "            self._perform_color_clustering(all_flower_pixels)\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    def analyze_single_image(self, image_data: Tuple, all_flower_pixels: List, all_background_pixels: List) -> Dict:\n",
    "        \"\"\"Analyze color characteristics of a single image\"\"\"\n",
    "        idx, image, targets, path = image_data\n",
    "        \n",
    "        # Convert image format\n",
    "        img_np = self._convert_image_format(image)\n",
    "        if img_np is None:\n",
    "            return {'error': 'Invalid image format', 'processed': False}\n",
    "        \n",
    "        if targets.numel() == 0:\n",
    "            return {'error': 'No targets', 'processed': False}\n",
    "        \n",
    "        h, w = img_np.shape[:2]\n",
    "        \n",
    "        # Convert to color spaces efficiently\n",
    "        image_hash = str(hash(img_np.tobytes()))\n",
    "        colorspaces = self.converter.convert_image_to_colorspaces(image_hash, img_np)\n",
    "        hsv_img = colorspaces['hsv']\n",
    "        lab_img = colorspaces['lab']\n",
    "        \n",
    "        # Process flowers and background\n",
    "        flower_regions, flower_mask = self._extract_flower_regions(img_np, hsv_img, lab_img, targets, h, w)\n",
    "        \n",
    "        if not flower_regions:\n",
    "            return {'error': 'No valid flower regions', 'processed': False}\n",
    "        \n",
    "        # Analyze flower pixels\n",
    "        self._analyze_flower_pixels(img_np, hsv_img, lab_img, flower_mask, all_flower_pixels)\n",
    "        \n",
    "        # Analyze background pixels\n",
    "        self._analyze_background_pixels(img_np, hsv_img, flower_mask, all_background_pixels)\n",
    "        \n",
    "        # Analyze individual flower regions\n",
    "        image_results = self._analyze_flower_regions(flower_regions)\n",
    "        \n",
    "        # Calculate background similarity for this image\n",
    "        self._calculate_background_similarity(all_flower_pixels, all_background_pixels)\n",
    "        \n",
    "        return {'processed': True, 'flower_count': len(flower_regions), 'results': image_results}\n",
    "    \n",
    "    def _convert_image_format(self, image) -> Optional[np.ndarray]:\n",
    "        \"\"\"Convert image to proper numpy format\"\"\"\n",
    "        try:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:\n",
    "                    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "                elif img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                return img_np\n",
    "            elif isinstance(image, np.ndarray):\n",
    "                return image\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Image conversion failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_flower_regions(self, img_np: np.ndarray, hsv_img: np.ndarray, \n",
    "                               lab_img: np.ndarray, targets: torch.Tensor, h: int, w: int) -> Tuple[List[Dict], np.ndarray]:\n",
    "        \"\"\"Extract flower regions from image\"\"\"\n",
    "        flower_mask = np.zeros((h, w), dtype=bool)\n",
    "        flower_regions = []\n",
    "        \n",
    "        for target in targets:\n",
    "            if len(target) >= 5:\n",
    "                cls, x_center, y_center, width, height = target[:5]\n",
    "                \n",
    "                # Convert to pixel coordinates\n",
    "                x1 = max(0, int((x_center - width/2) * w))\n",
    "                y1 = max(0, int((y_center - height/2) * h))\n",
    "                x2 = min(w, int((x_center + width/2) * w))\n",
    "                y2 = min(h, int((y_center + height/2) * h))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    flower_mask[y1:y2, x1:x2] = True\n",
    "                    \n",
    "                    flower_region = {\n",
    "                        'rgb': img_np[y1:y2, x1:x2],\n",
    "                        'hsv': hsv_img[y1:y2, x1:x2],\n",
    "                        'lab': lab_img[y1:y2, x1:x2],\n",
    "                        'bbox': (x1, y1, x2, y2),\n",
    "                        'class': int(cls)\n",
    "                    }\n",
    "                    flower_regions.append(flower_region)\n",
    "        \n",
    "        return flower_regions, flower_mask\n",
    "    \n",
    "    def _analyze_flower_pixels(self, img_np: np.ndarray, hsv_img: np.ndarray, \n",
    "                              lab_img: np.ndarray, flower_mask: np.ndarray, all_flower_pixels: List):\n",
    "        \"\"\"Analyze flower pixel distributions\"\"\"\n",
    "        if not np.any(flower_mask):\n",
    "            return\n",
    "        \n",
    "        flower_pixels_rgb = img_np[flower_mask]\n",
    "        flower_pixels_hsv = hsv_img[flower_mask]\n",
    "        flower_pixels_lab = lab_img[flower_mask]\n",
    "        \n",
    "        # Filter valid pixels\n",
    "        valid_mask = (flower_pixels_hsv[:, 2] > 0.1) & (flower_pixels_hsv[:, 2] < 0.95)\n",
    "        if not np.any(valid_mask):\n",
    "            return\n",
    "        \n",
    "        valid_rgb = flower_pixels_rgb[valid_mask]\n",
    "        valid_hsv = flower_pixels_hsv[valid_mask]\n",
    "        valid_lab = flower_pixels_lab[valid_mask]\n",
    "        \n",
    "        # Store color distributions efficiently\n",
    "        self._store_color_distributions(valid_rgb, valid_hsv, valid_lab)\n",
    "        \n",
    "        # Sample for clustering\n",
    "        sample_size = min(100, len(valid_rgb))\n",
    "        sample_indices = np.random.choice(len(valid_rgb), sample_size, replace=False)\n",
    "        all_flower_pixels.extend(valid_rgb[sample_indices].tolist())\n",
    "    \n",
    "    def _store_color_distributions(self, rgb: np.ndarray, hsv: np.ndarray, lab: np.ndarray):\n",
    "        \"\"\"Store color distributions in results\"\"\"\n",
    "        # RGB distributions\n",
    "        self.results['color_space_analysis']['rgb_distributions']['r'].extend(rgb[:, 0].tolist())\n",
    "        self.results['color_space_analysis']['rgb_distributions']['g'].extend(rgb[:, 1].tolist())\n",
    "        self.results['color_space_analysis']['rgb_distributions']['b'].extend(rgb[:, 2].tolist())\n",
    "        \n",
    "        # HSV distributions\n",
    "        self.results['color_space_analysis']['hsv_distributions']['hue'].extend(hsv[:, 0].tolist())\n",
    "        self.results['color_space_analysis']['hsv_distributions']['saturation'].extend(hsv[:, 1].tolist())\n",
    "        self.results['color_space_analysis']['hsv_distributions']['value'].extend(hsv[:, 2].tolist())\n",
    "        \n",
    "        # LAB distributions\n",
    "        self.results['color_space_analysis']['lab_distributions']['l'].extend(lab[:, 0].tolist())\n",
    "        self.results['color_space_analysis']['lab_distributions']['a'].extend(lab[:, 1].tolist())\n",
    "        self.results['color_space_analysis']['lab_distributions']['b'].extend(lab[:, 2].tolist())\n",
    "    \n",
    "    def _analyze_background_pixels(self, img_np: np.ndarray, hsv_img: np.ndarray, \n",
    "                                  flower_mask: np.ndarray, all_background_pixels: List):\n",
    "        \"\"\"Analyze background pixel characteristics\"\"\"\n",
    "        background_mask = ~flower_mask\n",
    "        if not np.any(background_mask):\n",
    "            return\n",
    "        \n",
    "        bg_pixels_rgb = img_np[background_mask]\n",
    "        \n",
    "        # Sample background pixels efficiently\n",
    "        sample_size = min(200, len(bg_pixels_rgb))\n",
    "        sample_indices = np.random.choice(len(bg_pixels_rgb), sample_size, replace=False)\n",
    "        sampled_bg_rgb = bg_pixels_rgb[sample_indices]\n",
    "        \n",
    "        all_background_pixels.extend(sampled_bg_rgb.tolist())\n",
    "    \n",
    "    def _analyze_flower_regions(self, flower_regions: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze individual flower regions for detailed characteristics\"\"\"\n",
    "        region_results = {\n",
    "            'flowers_processed': 0,\n",
    "            'petal_center_contrasts': [],\n",
    "            'health_assessments': [],\n",
    "            'seasonal_classifications': []\n",
    "        }\n",
    "        \n",
    "        for flower_data in flower_regions:\n",
    "            flower_rgb = flower_data['rgb']\n",
    "            flower_hsv = flower_data['hsv']\n",
    "            \n",
    "            if flower_rgb.size < 100:  # Skip very small regions\n",
    "                continue\n",
    "            \n",
    "            region_results['flowers_processed'] += 1\n",
    "            \n",
    "            # Analyze petal vs center colors\n",
    "            contrast_result = self._analyze_petal_center_contrast(flower_rgb, flower_hsv)\n",
    "            if contrast_result:\n",
    "                region_results['petal_center_contrasts'].append(contrast_result)\n",
    "            \n",
    "            # Health assessment\n",
    "            health_result = self._assess_flower_health(flower_rgb, flower_hsv)\n",
    "            if health_result:\n",
    "                region_results['health_assessments'].append(health_result)\n",
    "            \n",
    "            # Seasonal classification\n",
    "            seasonal_result = self._classify_seasonal_indicators(flower_hsv)\n",
    "            if seasonal_result:\n",
    "                region_results['seasonal_classifications'].append(seasonal_result)\n",
    "        \n",
    "        return region_results\n",
    "    \n",
    "    def _analyze_petal_center_contrast(self, flower_rgb: np.ndarray, flower_hsv: np.ndarray) -> Optional[Dict]:\n",
    "        \"\"\"Analyze contrast between flower center and petals\"\"\"\n",
    "        try:\n",
    "            fh, fw = flower_rgb.shape[:2]\n",
    "            \n",
    "            # Extract center region\n",
    "            center_region = flower_rgb[fh//3:2*fh//3, fw//3:2*fw//3]\n",
    "            if center_region.size < 10:\n",
    "                return None\n",
    "            \n",
    "            center_hsv = color.rgb2hsv(center_region.reshape(-1, 1, 3))\n",
    "            center_brightness = np.mean(center_hsv[:, 0, 2])\n",
    "            center_saturation = np.mean(center_hsv[:, 0, 1])\n",
    "            \n",
    "            # Store center colors\n",
    "            center_colors = center_region.reshape(-1, 3)\n",
    "            sample_indices = np.arange(0, len(center_colors), max(1, len(center_colors)//10))\n",
    "            self.results['flower_specific_colors']['center_colors'].extend(center_colors[sample_indices].tolist())\n",
    "            \n",
    "            # Extract petal region (excluding center)\n",
    "            petal_region = flower_rgb.copy()\n",
    "            petal_region[fh//3:2*fh//3, fw//3:2*fw//3] = 0\n",
    "            petal_pixels = petal_region[petal_region.sum(axis=2) > 0]\n",
    "            \n",
    "            if len(petal_pixels) < 10:\n",
    "                return None\n",
    "            \n",
    "            petal_hsv = color.rgb2hsv(petal_pixels.reshape(-1, 1, 3))\n",
    "            petal_brightness = np.mean(petal_hsv[:, 0, 2])\n",
    "            petal_saturation = np.mean(petal_hsv[:, 0, 1])\n",
    "            \n",
    "            # Store petal colors\n",
    "            sample_indices = np.arange(0, len(petal_pixels), max(1, len(petal_pixels)//10))\n",
    "            self.results['flower_specific_colors']['petal_colors'].extend(petal_pixels[sample_indices].tolist())\n",
    "            \n",
    "            # Calculate contrast\n",
    "            brightness_contrast = abs(center_brightness - petal_brightness)\n",
    "            saturation_contrast = abs(center_saturation - petal_saturation)\n",
    "            \n",
    "            # Classify contrast level\n",
    "            if (brightness_contrast > self.thresholds.high_brightness_contrast or \n",
    "                saturation_contrast > self.thresholds.high_saturation_contrast):\n",
    "                self.results['pollination_color_cues']['high_contrast_centers'] += 1\n",
    "                contrast_level = 'high'\n",
    "            elif (brightness_contrast > self.thresholds.medium_brightness_contrast or \n",
    "                  saturation_contrast > self.thresholds.medium_saturation_contrast):\n",
    "                self.results['pollination_color_cues']['medium_contrast_centers'] += 1\n",
    "                contrast_level = 'medium'\n",
    "            else:\n",
    "                self.results['pollination_color_cues']['low_contrast_centers'] += 1\n",
    "                contrast_level = 'low'\n",
    "            \n",
    "            return {\n",
    "                'brightness_contrast': brightness_contrast,\n",
    "                'saturation_contrast': saturation_contrast,\n",
    "                'contrast_level': contrast_level\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Petal-center analysis failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _assess_flower_health(self, flower_rgb: np.ndarray, flower_hsv: np.ndarray) -> Optional[Dict]:\n",
    "        \"\"\"Assess flower health based on color characteristics\"\"\"\n",
    "        try:\n",
    "            flower_hsv_flat = flower_hsv.reshape(-1, 3)\n",
    "            valid_hsv = flower_hsv_flat[flower_hsv_flat[:, 2] > 0.1]\n",
    "            \n",
    "            if len(valid_hsv) < 10:\n",
    "                return None\n",
    "            \n",
    "            avg_saturation = np.mean(valid_hsv[:, 1])\n",
    "            avg_brightness = np.mean(valid_hsv[:, 2])\n",
    "            hue_variance = np.var(valid_hsv[:, 0])\n",
    "            \n",
    "            # Color uniformity assessment\n",
    "            flower_flat = flower_rgb.reshape(-1, 3)\n",
    "            valid_pixels = flower_flat[flower_flat.sum(axis=1) > 0.1]\n",
    "            \n",
    "            if len(valid_pixels) > 10:\n",
    "                color_std = np.std(valid_pixels, axis=0)\n",
    "                color_mean = np.mean(valid_pixels, axis=0)\n",
    "                color_cv = np.mean(color_std / (color_mean + 1e-6))\n",
    "                self.results['color_uniformity']['within_flower_uniformity'].append(color_cv)\n",
    "            \n",
    "            # Health classification\n",
    "            if (avg_saturation > self.thresholds.vibrant_saturation and \n",
    "                avg_brightness > self.thresholds.vibrant_brightness and \n",
    "                hue_variance < self.thresholds.vibrant_hue_variance):\n",
    "                self.results['color_health_indicators']['vibrant_flowers'] += 1\n",
    "                health_class = 'vibrant'\n",
    "            elif (avg_saturation > self.thresholds.moderate_saturation and \n",
    "                  avg_brightness > self.thresholds.moderate_brightness):\n",
    "                self.results['color_health_indicators']['moderate_flowers'] += 1\n",
    "                health_class = 'moderate'\n",
    "            elif (hue_variance > self.thresholds.diseased_hue_variance or \n",
    "                  avg_saturation < self.thresholds.diseased_saturation):\n",
    "                self.results['color_health_indicators']['diseased_indicators'] += 1\n",
    "                health_class = 'diseased'\n",
    "            else:\n",
    "                self.results['color_health_indicators']['faded_flowers'] += 1\n",
    "                health_class = 'faded'\n",
    "            \n",
    "            return {\n",
    "                'saturation': avg_saturation,\n",
    "                'brightness': avg_brightness,\n",
    "                'hue_variance': hue_variance,\n",
    "                'health_class': health_class\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Health assessment failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _classify_seasonal_indicators(self, flower_hsv: np.ndarray) -> Optional[Dict]:\n",
    "        \"\"\"Classify seasonal color indicators\"\"\"\n",
    "        try:\n",
    "            flower_hsv_flat = flower_hsv.reshape(-1, 3)\n",
    "            valid_hsv = flower_hsv_flat[flower_hsv_flat[:, 2] > 0.1]\n",
    "            \n",
    "            if len(valid_hsv) < 10:\n",
    "                return None\n",
    "            \n",
    "            avg_hue = np.mean(valid_hsv[:, 0])\n",
    "            avg_saturation = np.mean(valid_hsv[:, 1])\n",
    "            avg_brightness = np.mean(valid_hsv[:, 2])\n",
    "            \n",
    "            color_data = [avg_hue, avg_saturation, avg_brightness]\n",
    "            \n",
    "            # Seasonal classification\n",
    "            if ((self.thresholds.spring_hue_range[0] <= avg_hue <= self.thresholds.spring_hue_range[1]) or\n",
    "                (self.thresholds.spring_hue_range_alt[0] <= avg_hue <= self.thresholds.spring_hue_range_alt[1])):\n",
    "                if avg_saturation > 0.5:\n",
    "                    self.results['seasonal_color_analysis']['spring_indicators']['count'] += 1\n",
    "                    self.results['seasonal_color_analysis']['spring_indicators']['colors'].append(color_data)\n",
    "                    return {'season': 'spring', 'confidence': avg_saturation}\n",
    "            \n",
    "            elif self.thresholds.summer_hue_range[0] < avg_hue <= self.thresholds.summer_hue_range[1]:\n",
    "                self.results['seasonal_color_analysis']['summer_indicators']['count'] += 1\n",
    "                self.results['seasonal_color_analysis']['summer_indicators']['colors'].append(color_data)\n",
    "                return {'season': 'summer', 'confidence': avg_saturation}\n",
    "            \n",
    "            elif self.thresholds.winter_hue_range[0] < avg_hue <= self.thresholds.winter_hue_range[1]:\n",
    "                if avg_saturation < 0.4:\n",
    "                    self.results['seasonal_color_analysis']['winter_indicators']['count'] += 1\n",
    "                    self.results['seasonal_color_analysis']['winter_indicators']['colors'].append(color_data)\n",
    "                    return {'season': 'winter', 'confidence': 1.0 - avg_saturation}\n",
    "            \n",
    "            else:  # Autumn range\n",
    "                self.results['seasonal_color_analysis']['autumn_indicators']['count'] += 1\n",
    "                self.results['seasonal_color_analysis']['autumn_indicators']['colors'].append(color_data)\n",
    "                return {'season': 'autumn', 'confidence': avg_saturation}\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Seasonal classification failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_background_similarity(self, all_flower_pixels: List, all_background_pixels: List):\n",
    "        \"\"\"Calculate background similarity efficiently\"\"\"\n",
    "        if (len(all_flower_pixels) < 10 or len(all_background_pixels) < 10):\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Use recent pixels for efficiency\n",
    "            flower_pixels = np.array(all_flower_pixels[-100:])\n",
    "            bg_pixels = np.array(all_background_pixels[-100:])\n",
    "            \n",
    "            flower_mean = np.mean(flower_pixels, axis=0)\n",
    "            bg_mean = np.mean(bg_pixels, axis=0)\n",
    "            \n",
    "            color_distance = np.linalg.norm(flower_mean - bg_mean)\n",
    "            similarity_score = 1 / (1 + color_distance)\n",
    "            \n",
    "            self.results['background_similarity']['similarity_scores'].append(similarity_score)\n",
    "            \n",
    "            if similarity_score > self.thresholds.high_similarity:\n",
    "                self.results['background_similarity']['high_similarity'] += 1\n",
    "            elif similarity_score > self.thresholds.medium_similarity:\n",
    "                self.results['background_similarity']['medium_similarity'] += 1\n",
    "            else:\n",
    "                self.results['background_similarity']['low_similarity'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Background similarity calculation failed: {e}\")\n",
    "    \n",
    "    def _perform_color_clustering(self, all_flower_pixels: List):\n",
    "        \"\"\"Perform K-means clustering on flower colors\"\"\"\n",
    "        try:\n",
    "            flower_pixels_array = np.array(all_flower_pixels)\n",
    "            n_clusters = min(12, len(flower_pixels_array) // 20)\n",
    "            \n",
    "            if n_clusters < 2:\n",
    "                return\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(flower_pixels_array)\n",
    "            \n",
    "            # Store cluster centers\n",
    "            self.results['dominant_colors']['cluster_centers_rgb'] = kmeans.cluster_centers_.tolist()\n",
    "            \n",
    "            # Convert to HSV\n",
    "            cluster_centers_hsv = []\n",
    "            for center in kmeans.cluster_centers_:\n",
    "                try:\n",
    "                    hsv_center = color.rgb2hsv(center.reshape(1, 1, 3))[0, 0]\n",
    "                    cluster_centers_hsv.append(hsv_center.tolist())\n",
    "                except:\n",
    "                    cluster_centers_hsv.append([0, 0, 0])\n",
    "            \n",
    "            self.results['dominant_colors']['cluster_centers_hsv'] = cluster_centers_hsv\n",
    "            \n",
    "            # Calculate cluster sizes and dominance\n",
    "            unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "            self.results['dominant_colors']['cluster_sizes'] = counts.tolist()\n",
    "            \n",
    "            total_pixels = len(flower_pixels_array)\n",
    "            dominance_scores = [count / total_pixels for count in counts]\n",
    "            self.results['dominant_colors']['color_dominance_scores'] = dominance_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Color clustering failed: {e}\")\n",
    "    \n",
    "    def finalize_analysis(self):\n",
    "        \"\"\"Finalize analysis with inter-flower uniformity calculation\"\"\"\n",
    "        uniformity_values = self.results['color_uniformity']['within_flower_uniformity']\n",
    "        if len(uniformity_values) > 1:\n",
    "            between_flower_uniformity = np.std(uniformity_values)\n",
    "            self.results['color_uniformity']['between_flower_uniformity'] = [between_flower_uniformity]\n",
    "\n",
    "def analyze_flower_colors_comprehensive(dataset, dataset_name: str, max_samples: int = 300) -> Dict:\n",
    "    \"\"\"Enhanced color analysis with batch processing and optimization\"\"\"\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Invalid dataset for color analysis: {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(f\"Analyzing flower colors in {dataset_name}\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = FlowerColorAnalyzer()\n",
    "    \n",
    "    # Prepare sample data\n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = config.max_batch_size\n",
    "    processed_count = 0\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        batch_data = []\n",
    "        \n",
    "        # Load batch data\n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                image, targets, path = dataset[idx]\n",
    "                batch_data.append((idx, image, targets, path))\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to load image {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_data:\n",
    "            # Process batch\n",
    "            batch_results = analyzer.process_image_batch(batch_data)\n",
    "            processed_count += len([r for r in batch_results if r.get('processed', False)])\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if (i // batch_size) % 5 == 0:\n",
    "            MemoryManager.clear_memory()\n",
    "    \n",
    "    # Finalize analysis\n",
    "    analyzer.finalize_analysis()\n",
    "    \n",
    "    # Add metadata\n",
    "    analyzer.results['metadata'] = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'images_processed': processed_count,\n",
    "        'total_sampled': sample_size,\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Color analysis completed for {dataset_name}: {processed_count} images processed\")\n",
    "    return analyzer.results\n",
    "\n",
    "def create_comprehensive_color_visualization(color_analysis_results: Dict):\n",
    "    \"\"\"Create memory-efficient color visualization\"\"\"\n",
    "    \n",
    "    if not color_analysis_results:\n",
    "        logger.error(\"No color analysis results for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create figure with memory-efficient layout\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        for idx, (dataset_name, metrics) in enumerate(color_analysis_results.items()):\n",
    "            if idx >= 1:  # Process only first dataset for memory efficiency\n",
    "                break\n",
    "                \n",
    "            create_color_plots(fig, gs, dataset_name, metrics)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with optimization\n",
    "        output_path = notebook_results_dir / 'color_analysis' / 'comprehensive_color_analysis.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', optimize=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        plt.close(fig)\n",
    "        MemoryManager.clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Color visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "\n",
    "def create_color_plots(fig, gs, dataset_name: str, metrics: Dict):\n",
    "    \"\"\"Create individual color analysis plots\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. HSV Distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        hsv_dist = metrics['color_space_analysis']['hsv_distributions']\n",
    "        \n",
    "        if hsv_dist['hue']:\n",
    "            n, bins, patches = ax1.hist(hsv_dist['hue'], bins=24, alpha=0.8, density=True)\n",
    "            \n",
    "            # Color bars by hue\n",
    "            for i, (patch, bin_center) in enumerate(zip(patches, (bins[:-1] + bins[1:]) / 2)):\n",
    "                hue_color = plt.cm.hsv(bin_center)\n",
    "                patch.set_facecolor(hue_color)\n",
    "            \n",
    "            ax1.set_title('Hue Distribution', fontweight='bold', fontsize=11)\n",
    "            ax1.set_xlabel('Hue')\n",
    "            ax1.set_ylabel('Density')\n",
    "        \n",
    "        # 2. Health Assessment\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        health_data = metrics['color_health_indicators']\n",
    "        health_labels = ['Vibrant', 'Moderate', 'Faded', 'Diseased']\n",
    "        health_values = [\n",
    "            health_data['vibrant_flowers'],\n",
    "            health_data['moderate_flowers'], \n",
    "            health_data['faded_flowers'],\n",
    "            health_data['diseased_indicators']\n",
    "        ]\n",
    "        \n",
    "        if any(health_values):\n",
    "            colors = ['darkgreen', 'lightgreen', 'orange', 'red']\n",
    "            ax2.pie(health_values, labels=health_labels, autopct='%1.1f%%',\n",
    "                   colors=colors, startangle=90)\n",
    "            ax2.set_title(f'Health Assessment\\n{dataset_name}', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # 3. Background Similarity\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        bg_sim = metrics['background_similarity']\n",
    "        sim_labels = ['High', 'Medium', 'Low']\n",
    "        sim_values = [bg_sim['high_similarity'], bg_sim['medium_similarity'], bg_sim['low_similarity']]\n",
    "        \n",
    "        if any(sim_values):\n",
    "            ax3.bar(sim_labels, sim_values, color=['red', 'orange', 'green'], alpha=0.8)\n",
    "            ax3.set_title('Background Similarity', fontweight='bold', fontsize=11)\n",
    "            ax3.set_ylabel('Count')\n",
    "        \n",
    "        # 4. Seasonal Indicators\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        seasonal_data = metrics['seasonal_color_analysis']\n",
    "        seasonal_counts = [\n",
    "            seasonal_data['spring_indicators']['count'],\n",
    "            seasonal_data['summer_indicators']['count'],\n",
    "            seasonal_data['autumn_indicators']['count'],\n",
    "            seasonal_data['winter_indicators']['count']\n",
    "        ]\n",
    "        \n",
    "        if any(seasonal_counts):\n",
    "            seasonal_labels = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "            ax4.bar(seasonal_labels, seasonal_counts, \n",
    "                   color=['lightgreen', 'gold', 'orange', 'lightblue'], alpha=0.8)\n",
    "            ax4.set_title('Seasonal Indicators', fontweight='bold', fontsize=11)\n",
    "            ax4.set_ylabel('Count')\n",
    "        \n",
    "        # 5. Summary Statistics\n",
    "        ax5 = fig.add_subplot(gs[1, :])\n",
    "        \n",
    "        if hsv_dist['hue']:\n",
    "            summary_text = f\"\"\"Color Analysis Summary - {dataset_name}\n",
    "            \n",
    "HSV Statistics:\n",
    "  Hue: ={np.mean(hsv_dist['hue']):.3f}, ={np.std(hsv_dist['hue']):.3f}\n",
    "  Saturation: ={np.mean(hsv_dist['saturation']):.3f}, ={np.std(hsv_dist['saturation']):.3f}  \n",
    "  Brightness: ={np.mean(hsv_dist['value']):.3f}, ={np.std(hsv_dist['value']):.3f}\n",
    "\n",
    "Color Clusters: {len(metrics['dominant_colors']['cluster_centers_rgb'])}\n",
    "\n",
    "Health Assessment - Total: {sum(health_values)}\n",
    "  Vibrant: {health_values[0]} ({health_values[0]/max(1,sum(health_values))*100:.1f}%)\n",
    "  Moderate: {health_values[1]} ({health_values[1]/max(1,sum(health_values))*100:.1f}%)\n",
    "  Problematic: {health_values[2]+health_values[3]} ({(health_values[2]+health_values[3])/max(1,sum(health_values))*100:.1f}%)\n",
    "\n",
    "Background Similarity - Risk Assessment:\n",
    "  High Risk: {bg_sim['high_similarity']} cases\n",
    "  Overall Risk: {'HIGH' if bg_sim['high_similarity'] > bg_sim['low_similarity'] else 'MODERATE' if bg_sim['medium_similarity'] > bg_sim['low_similarity'] else 'LOW'}\"\"\"\n",
    "        else:\n",
    "            summary_text = f\"No valid color data processed for {dataset_name}\"\n",
    "        \n",
    "        ax5.text(0.05, 0.95, summary_text, transform=ax5.transAxes,\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "        ax5.set_title('Comprehensive Color Analysis Summary', fontweight='bold', fontsize=12)\n",
    "        ax5.axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create color plots: {e}\")\n",
    "\n",
    "# Execute comprehensive color analysis with error handling\n",
    "def execute_color_analysis():\n",
    "    \"\"\"Execute the complete color analysis pipeline\"\"\"\n",
    "    \n",
    "    if 'datasets' not in locals() or not datasets:\n",
    "        logger.error(\"No datasets available for color analysis\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Starting comprehensive color analysis\")\n",
    "    \n",
    "    color_analysis_results = {}\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        logger.info(f\"Analyzing colors in {name}\")\n",
    "        \n",
    "        try:\n",
    "            color_metrics = safe_operation(\n",
    "                f\"Color analysis for {name}\",\n",
    "                analyze_flower_colors_comprehensive,\n",
    "                dataset, name, 300\n",
    "            )\n",
    "            \n",
    "            if color_metrics and 'color_space_analysis' in color_metrics:\n",
    "                color_analysis_results[name] = color_metrics\n",
    "                \n",
    "                # Log findings\n",
    "                hsv_dist = color_metrics['color_space_analysis']['hsv_distributions']\n",
    "                health_data = color_metrics['color_health_indicators']\n",
    "                \n",
    "                logger.info(f\"Color analysis complete for {name}:\")\n",
    "                if hsv_dist['hue']:\n",
    "                    logger.info(f\"  Average hue: {np.mean(hsv_dist['hue']):.3f}\")\n",
    "                    logger.info(f\"  Average saturation: {np.mean(hsv_dist['saturation']):.3f}\")\n",
    "                    logger.info(f\"  Average brightness: {np.mean(hsv_dist['value']):.3f}\")\n",
    "                \n",
    "                logger.info(f\"  Color clusters: {len(color_metrics['dominant_colors']['cluster_centers_rgb'])}\")\n",
    "                \n",
    "                total_health = sum(health_data.values())\n",
    "                if total_health > 0:\n",
    "                    logger.info(f\"  Vibrant flowers: {health_data['vibrant_flowers']} ({health_data['vibrant_flowers']/total_health*100:.1f}%)\")\n",
    "            \n",
    "            else:\n",
    "                logger.warning(f\"Color analysis failed or returned empty results for {name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to analyze colors in {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create visualization if we have results\n",
    "    if color_analysis_results:\n",
    "        safe_operation(\n",
    "            \"Creating color analysis visualization\",\n",
    "            create_comprehensive_color_visualization,\n",
    "            color_analysis_results\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            save_color_analysis_results(color_analysis_results)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save color analysis results: {e}\")\n",
    "    \n",
    "    return color_analysis_results\n",
    "\n",
    "def save_color_analysis_results(results: Dict):\n",
    "    \"\"\"Save color analysis results with proper serialization\"\"\"\n",
    "    \n",
    "    serializable_results = {}\n",
    "    \n",
    "    for dataset_name, metrics in results.items():\n",
    "        hsv_dist = metrics['color_space_analysis']['hsv_distributions']\n",
    "        \n",
    "        serializable_results[dataset_name] = {\n",
    "            'color_statistics': {\n",
    "                'hue_mean': float(np.mean(hsv_dist['hue'])) if hsv_dist['hue'] else 0,\n",
    "                'hue_std': float(np.std(hsv_dist['hue'])) if hsv_dist['hue'] else 0,\n",
    "                'saturation_mean': float(np.mean(hsv_dist['saturation'])) if hsv_dist['saturation'] else 0,\n",
    "                'saturation_std': float(np.std(hsv_dist['saturation'])) if hsv_dist['saturation'] else 0,\n",
    "                'brightness_mean': float(np.mean(hsv_dist['value'])) if hsv_dist['value'] else 0,\n",
    "                'brightness_std': float(np.std(hsv_dist['value'])) if hsv_dist['value'] else 0\n",
    "            },\n",
    "            'dominant_colors': {\n",
    "                'num_clusters': len(metrics['dominant_colors']['cluster_centers_rgb']),\n",
    "                'cluster_centers_rgb': metrics['dominant_colors']['cluster_centers_rgb'],\n",
    "                'cluster_sizes': metrics['dominant_colors']['cluster_sizes'],\n",
    "                'dominance_scores': metrics['dominant_colors']['color_dominance_scores']\n",
    "            },\n",
    "            'health_assessment': metrics['color_health_indicators'],\n",
    "            'background_similarity': {\n",
    "                'high_similarity': metrics['background_similarity']['high_similarity'],\n",
    "                'medium_similarity': metrics['background_similarity']['medium_similarity'],\n",
    "                'low_similarity': metrics['background_similarity']['low_similarity'],\n",
    "                'avg_similarity': float(np.mean(metrics['background_similarity']['similarity_scores'])) if metrics['background_similarity']['similarity_scores'] else 0\n",
    "            },\n",
    "            'seasonal_indicators': {\n",
    "                'spring_count': metrics['seasonal_color_analysis']['spring_indicators']['count'],\n",
    "                'summer_count': metrics['seasonal_color_analysis']['summer_indicators']['count'],\n",
    "                'autumn_count': metrics['seasonal_color_analysis']['autumn_indicators']['count'],\n",
    "                'winter_count': metrics['seasonal_color_analysis']['winter_indicators']['count']\n",
    "            },\n",
    "            'pollination_cues': metrics['pollination_color_cues'],\n",
    "            'color_uniformity': {\n",
    "                'avg_within_flower': float(np.mean(metrics['color_uniformity']['within_flower_uniformity'])) if metrics['color_uniformity']['within_flower_uniformity'] else 0,\n",
    "                'std_within_flower': float(np.std(metrics['color_uniformity']['within_flower_uniformity'])) if metrics['color_uniformity']['within_flower_uniformity'] else 0\n",
    "            },\n",
    "            'metadata': metrics.get('metadata', {})\n",
    "        }\n",
    "    \n",
    "    output_path = notebook_results_dir / 'color_analysis' / 'comprehensive_color_analysis.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Color analysis results saved to {output_path}\")\n",
    "\n",
    "# Execute the analysis\n",
    "color_analysis_results = execute_color_analysis()\n",
    "\n",
    "if not color_analysis_results:\n",
    "    logger.error(\"Color analysis failed completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ded9f0",
   "metadata": {},
   "source": [
    "## 5. Pollination State and Health Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5933a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced pollination state and health assessment with advanced metrics\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "@dataclass\n",
    "class HealthAssessmentThresholds:\n",
    "    \"\"\"Scientific thresholds for health and pollination assessment\"\"\"\n",
    "    # Health scoring weights\n",
    "    color_health_weight: float = 0.3\n",
    "    texture_health_weight: float = 0.3\n",
    "    size_health_weight: float = 0.2\n",
    "    uniformity_health_weight: float = 0.2\n",
    "    \n",
    "    # Health classification thresholds\n",
    "    excellent_health_threshold: float = 0.8\n",
    "    good_health_threshold: float = 0.6\n",
    "    moderate_health_threshold: float = 0.4\n",
    "    poor_health_threshold: float = 0.2\n",
    "    \n",
    "    # Pollination state thresholds\n",
    "    unpollinated_brightness: float = 0.7\n",
    "    unpollinated_saturation: float = 0.6\n",
    "    \n",
    "    active_brightness_range: Tuple[float, float] = (0.4, 0.7)\n",
    "    active_variance_threshold: float = 0.01\n",
    "    \n",
    "    post_pollination_brightness: float = 0.4\n",
    "    post_pollination_saturation: float = 0.4\n",
    "    \n",
    "    seed_development_brightness: float = 0.3\n",
    "    \n",
    "    # Maturity assessment thresholds\n",
    "    early_bud_area: float = 0.01\n",
    "    late_bud_area: float = 0.03\n",
    "    early_bloom_area: float = 0.08\n",
    "    peak_bloom_area: float = 0.15\n",
    "    late_bloom_area: float = 0.25\n",
    "    \n",
    "    # Structural integrity thresholds\n",
    "    perfect_structural_threshold: float = 0.8\n",
    "    minor_damage_threshold: float = 0.6\n",
    "    moderate_damage_threshold: float = 0.4\n",
    "    \n",
    "    # Environmental stress thresholds\n",
    "    no_stress_threshold: float = 0.2\n",
    "    mild_stress_threshold: float = 0.4\n",
    "    moderate_stress_threshold: float = 0.7\n",
    "    \n",
    "    # Attractiveness thresholds  \n",
    "    high_attractiveness_threshold: float = 0.7\n",
    "    medium_attractiveness_threshold: float = 0.4\n",
    "\n",
    "class FlowerHealthAnalyzer:\n",
    "    \"\"\"Modular analyzer for comprehensive flower health assessment\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: HealthAssessmentThresholds = None):\n",
    "        self.thresholds = thresholds or HealthAssessmentThresholds()\n",
    "        self.results = self._initialize_metrics()\n",
    "    \n",
    "    def _initialize_metrics(self) -> Dict:\n",
    "        \"\"\"Initialize health metrics structure\"\"\"\n",
    "        return {\n",
    "            'pollination_assessment': {\n",
    "                'unpollinated_indicators': 0,\n",
    "                'active_pollination': 0,\n",
    "                'post_pollination': 0,\n",
    "                'seed_development': 0,\n",
    "                'pollination_confidence_scores': []\n",
    "            },\n",
    "            'health_indicators': {\n",
    "                'excellent_health': 0,\n",
    "                'good_health': 0,\n",
    "                'moderate_health': 0,\n",
    "                'poor_health': 0,\n",
    "                'diseased': 0,\n",
    "                'health_scores': []\n",
    "            },\n",
    "            'flower_maturity': {\n",
    "                'early_bud': 0,\n",
    "                'late_bud': 0,\n",
    "                'early_bloom': 0,\n",
    "                'peak_bloom': 0,\n",
    "                'late_bloom': 0,\n",
    "                'senescence': 0,\n",
    "                'maturity_progression': []\n",
    "            },\n",
    "            'structural_integrity': {\n",
    "                'perfect_petals': 0,\n",
    "                'minor_damage': 0,\n",
    "                'moderate_damage': 0,\n",
    "                'severe_damage': 0,\n",
    "                'structural_scores': []\n",
    "            },\n",
    "            'environmental_stress': {\n",
    "                'no_stress': 0,\n",
    "                'mild_stress': 0,\n",
    "                'moderate_stress': 0,\n",
    "                'severe_stress': 0,\n",
    "                'stress_indicators': []\n",
    "            },\n",
    "            'pollinator_attractiveness': {\n",
    "                'high_attractiveness': 0,\n",
    "                'medium_attractiveness': 0,\n",
    "                'low_attractiveness': 0,\n",
    "                'attractiveness_scores': []\n",
    "            },\n",
    "            'temporal_health_patterns': {\n",
    "                'morning_optimal': 0,\n",
    "                'midday_stress': 0,\n",
    "                'evening_recovery': 0,\n",
    "                'time_indicators': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_image_batch(self, image_batch: List[Tuple]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of images for health assessment\"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        for image_data in image_batch:\n",
    "            try:\n",
    "                result = self.analyze_single_image(image_data)\n",
    "                if result and result.get('processed', False):\n",
    "                    batch_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to process image in health analysis batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    def analyze_single_image(self, image_data: Tuple) -> Dict:\n",
    "        \"\"\"Analyze health characteristics of a single image\"\"\"\n",
    "        idx, image, targets, path = image_data\n",
    "        \n",
    "        # Convert image format\n",
    "        img_np = self._convert_image_format(image)\n",
    "        if img_np is None:\n",
    "            return {'error': 'Invalid image format', 'processed': False}\n",
    "        \n",
    "        if targets.numel() == 0:\n",
    "            return {'error': 'No targets', 'processed': False}\n",
    "        \n",
    "        h, w = img_np.shape[:2]\n",
    "        \n",
    "        # Convert to analysis formats\n",
    "        try:\n",
    "            hsv_img = color.rgb2hsv(img_np)\n",
    "            gray = cv2.cvtColor((img_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Color space conversion failed: {e}\")\n",
    "            return {'error': 'Color conversion failed', 'processed': False}\n",
    "        \n",
    "        # Calculate global environmental indicators\n",
    "        overall_brightness = np.mean(gray) / 255.0\n",
    "        overall_contrast = np.std(gray) / 255.0\n",
    "        \n",
    "        # Process each flower in the image\n",
    "        flowers_processed = 0\n",
    "        for target in targets:\n",
    "            if len(target) >= 5:\n",
    "                processed = self._analyze_single_flower(target, img_np, hsv_img, gray, \n",
    "                                                      overall_brightness, h, w)\n",
    "                if processed:\n",
    "                    flowers_processed += 1\n",
    "        \n",
    "        return {\n",
    "            'processed': True, \n",
    "            'flowers_processed': flowers_processed,\n",
    "            'overall_brightness': overall_brightness,\n",
    "            'overall_contrast': overall_contrast\n",
    "        }\n",
    "    \n",
    "    def _convert_image_format(self, image) -> Optional[np.ndarray]:\n",
    "        \"\"\"Convert image to proper numpy format with validation\"\"\"\n",
    "        try:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:\n",
    "                    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "                elif img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                return img_np\n",
    "            elif isinstance(image, np.ndarray):\n",
    "                return np.clip(image, 0, 1)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Image conversion failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_single_flower(self, target: torch.Tensor, img_np: np.ndarray, \n",
    "                              hsv_img: np.ndarray, gray: np.ndarray, \n",
    "                              overall_brightness: float, h: int, w: int) -> bool:\n",
    "        \"\"\"Analyze individual flower for health characteristics\"\"\"\n",
    "        try:\n",
    "            cls, x_center, y_center, width, height = target[:5]\n",
    "            \n",
    "            # Extract flower region\n",
    "            x1 = max(0, int((x_center - width/2) * w))\n",
    "            y1 = max(0, int((y_center - height/2) * h))\n",
    "            x2 = min(w, int((x_center + width/2) * w))\n",
    "            y2 = min(h, int((y_center + height/2) * h))\n",
    "            \n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                return False\n",
    "            \n",
    "            flower_region = img_np[y1:y2, x1:x2]\n",
    "            flower_hsv = hsv_img[y1:y2, x1:x2]\n",
    "            flower_gray = gray[y1:y2, x1:x2]\n",
    "            \n",
    "            if flower_region.size < 50:\n",
    "                return False\n",
    "            \n",
    "            # Run comprehensive analysis\n",
    "            self._assess_pollination_state(flower_region, flower_hsv)\n",
    "            health_score = self._assess_flower_health(flower_region, flower_hsv, flower_gray, width, height)\n",
    "            self._assess_maturity_stage(width, height)\n",
    "            self._assess_structural_integrity(flower_gray)\n",
    "            self._assess_environmental_stress(flower_hsv)\n",
    "            self._assess_pollinator_attractiveness(flower_hsv)\n",
    "            self._assess_temporal_patterns(overall_brightness)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Single flower analysis failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _assess_pollination_state(self, flower_region: np.ndarray, flower_hsv: np.ndarray):\n",
    "        \"\"\"Assess pollination state based on center characteristics\"\"\"\n",
    "        try:\n",
    "            fh, fw = flower_region.shape[:2]\n",
    "            \n",
    "            # Extract center region (reproductive parts)\n",
    "            center_h_start, center_h_end = fh//3, 2*fh//3\n",
    "            center_w_start, center_w_end = fw//3, 2*fw//3\n",
    "            center_hsv = flower_hsv[center_h_start:center_h_end, center_w_start:center_w_end]\n",
    "            \n",
    "            if center_hsv.size < 10:\n",
    "                return\n",
    "            \n",
    "            center_brightness = np.mean(center_hsv[:, :, 2])\n",
    "            center_saturation = np.mean(center_hsv[:, :, 1])\n",
    "            center_variance = np.var(center_hsv[:, :, 2])\n",
    "            \n",
    "            # Classify pollination state\n",
    "            pollination_score = 0\n",
    "            \n",
    "            if (center_brightness > self.thresholds.unpollinated_brightness and \n",
    "                center_saturation > self.thresholds.unpollinated_saturation):\n",
    "                self.results['pollination_assessment']['unpollinated_indicators'] += 1\n",
    "                pollination_score = 0.2\n",
    "                \n",
    "            elif (self.thresholds.active_brightness_range[0] < center_brightness < self.thresholds.active_brightness_range[1] and \n",
    "                  center_variance > self.thresholds.active_variance_threshold):\n",
    "                self.results['pollination_assessment']['active_pollination'] += 1\n",
    "                pollination_score = 0.6\n",
    "                \n",
    "            elif (center_brightness < self.thresholds.post_pollination_brightness and \n",
    "                  center_saturation < self.thresholds.post_pollination_saturation):\n",
    "                self.results['pollination_assessment']['post_pollination'] += 1\n",
    "                pollination_score = 0.8\n",
    "                \n",
    "            elif center_brightness < self.thresholds.seed_development_brightness:\n",
    "                self.results['pollination_assessment']['seed_development'] += 1\n",
    "                pollination_score = 1.0\n",
    "            \n",
    "            self.results['pollination_assessment']['pollination_confidence_scores'].append(pollination_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Pollination assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_flower_health(self, flower_region: np.ndarray, flower_hsv: np.ndarray, \n",
    "                             flower_gray: np.ndarray, width: float, height: float) -> float:\n",
    "        \"\"\"Comprehensive flower health assessment\"\"\"\n",
    "        try:\n",
    "            # Color-based health indicators\n",
    "            flower_brightness = np.mean(flower_hsv[:, :, 2])\n",
    "            flower_saturation = np.mean(flower_hsv[:, :, 1])\n",
    "            flower_hue_variance = np.var(flower_hsv[:, :, 0])\n",
    "            \n",
    "            # Texture analysis for health\n",
    "            laplacian_var = cv2.Laplacian(flower_gray, cv2.CV_64F).var()\n",
    "            edges = cv2.Canny(flower_gray, 50, 150)\n",
    "            edge_density = np.sum(edges) / flower_gray.size\n",
    "            \n",
    "            # Calculate health components\n",
    "            color_health = self._calculate_color_health(flower_saturation, flower_brightness)\n",
    "            texture_health = self._calculate_texture_health(laplacian_var, edge_density)\n",
    "            size_health = self._calculate_size_health(width * height)\n",
    "            uniformity_health = self._calculate_uniformity_health(flower_hue_variance)\n",
    "            \n",
    "            # Weighted health score\n",
    "            health_score = (\n",
    "                color_health * self.thresholds.color_health_weight +\n",
    "                texture_health * self.thresholds.texture_health_weight +\n",
    "                size_health * self.thresholds.size_health_weight +\n",
    "                uniformity_health * self.thresholds.uniformity_health_weight\n",
    "            )\n",
    "            \n",
    "            self.results['health_indicators']['health_scores'].append(health_score)\n",
    "            \n",
    "            # Classify health status\n",
    "            if health_score > self.thresholds.excellent_health_threshold:\n",
    "                self.results['health_indicators']['excellent_health'] += 1\n",
    "            elif health_score > self.thresholds.good_health_threshold:\n",
    "                self.results['health_indicators']['good_health'] += 1\n",
    "            elif health_score > self.thresholds.moderate_health_threshold:\n",
    "                self.results['health_indicators']['moderate_health'] += 1\n",
    "            elif health_score > self.thresholds.poor_health_threshold:\n",
    "                self.results['health_indicators']['poor_health'] += 1\n",
    "            else:\n",
    "                self.results['health_indicators']['diseased'] += 1\n",
    "            \n",
    "            return health_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Health assessment failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_color_health(self, saturation: float, brightness: float) -> float:\n",
    "        \"\"\"Calculate color-based health score\"\"\"\n",
    "        if saturation > 0.6 and brightness > 0.4:\n",
    "            return 1.0\n",
    "        elif saturation > 0.3 and brightness > 0.2:\n",
    "            return 0.67\n",
    "        else:\n",
    "            return 0.33\n",
    "    \n",
    "    def _calculate_texture_health(self, laplacian_var: float, edge_density: float) -> float:\n",
    "        \"\"\"Calculate texture-based health score\"\"\"\n",
    "        if 20 < laplacian_var < 200 and edge_density < 0.1:\n",
    "            return 1.0\n",
    "        elif laplacian_var < 500 and edge_density < 0.2:\n",
    "            return 0.67\n",
    "        else:\n",
    "            return 0.33\n",
    "    \n",
    "    def _calculate_size_health(self, area: float) -> float:\n",
    "        \"\"\"Calculate size-based health score\"\"\"\n",
    "        if 0.02 < area < 0.2:\n",
    "            return 1.0\n",
    "        elif 0.01 < area < 0.3:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    \n",
    "    def _calculate_uniformity_health(self, hue_variance: float) -> float:\n",
    "        \"\"\"Calculate uniformity-based health score\"\"\"\n",
    "        if hue_variance < 0.05:\n",
    "            return 1.0\n",
    "        elif hue_variance < 0.1:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.25\n",
    "    \n",
    "    def _assess_maturity_stage(self, width: float, height: float):\n",
    "        \"\"\"Assess flower maturity based on size\"\"\"\n",
    "        try:\n",
    "            area = width * height\n",
    "            \n",
    "            # Determine maturity score and category\n",
    "            if area < self.thresholds.early_bud_area:\n",
    "                self.results['flower_maturity']['early_bud'] += 1\n",
    "                maturity_score = 0.1\n",
    "            elif area < self.thresholds.late_bud_area:\n",
    "                self.results['flower_maturity']['late_bud'] += 1\n",
    "                maturity_score = 0.3\n",
    "            elif area < self.thresholds.early_bloom_area:\n",
    "                self.results['flower_maturity']['early_bloom'] += 1\n",
    "                maturity_score = 0.5\n",
    "            elif area < self.thresholds.peak_bloom_area:\n",
    "                self.results['flower_maturity']['peak_bloom'] += 1\n",
    "                maturity_score = 0.8\n",
    "            elif area < self.thresholds.late_bloom_area:\n",
    "                self.results['flower_maturity']['late_bloom'] += 1\n",
    "                maturity_score = 0.6\n",
    "            else:\n",
    "                self.results['flower_maturity']['senescence'] += 1\n",
    "                maturity_score = 0.2\n",
    "            \n",
    "            self.results['flower_maturity']['maturity_progression'].append(maturity_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Maturity assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_structural_integrity(self, flower_gray: np.ndarray):\n",
    "        \"\"\"Assess structural integrity from edge analysis\"\"\"\n",
    "        try:\n",
    "            edges = cv2.Canny(flower_gray, 50, 150)\n",
    "            edge_density = np.sum(edges) / flower_gray.size\n",
    "            laplacian_var = cv2.Laplacian(flower_gray, cv2.CV_64F).var()\n",
    "            \n",
    "            # Calculate structural score\n",
    "            edge_smoothness = 1 / (edge_density + 1e-6)\n",
    "            boundary_regularity = 1 / (laplacian_var / 1000 + 1)\n",
    "            structural_score = min(1.0, (edge_smoothness + boundary_regularity) / 2)\n",
    "            \n",
    "            self.results['structural_integrity']['structural_scores'].append(structural_score)\n",
    "            \n",
    "            # Classify structural integrity\n",
    "            if structural_score > self.thresholds.perfect_structural_threshold:\n",
    "                self.results['structural_integrity']['perfect_petals'] += 1\n",
    "            elif structural_score > self.thresholds.minor_damage_threshold:\n",
    "                self.results['structural_integrity']['minor_damage'] += 1\n",
    "            elif structural_score > self.thresholds.moderate_damage_threshold:\n",
    "                self.results['structural_integrity']['moderate_damage'] += 1\n",
    "            else:\n",
    "                self.results['structural_integrity']['severe_damage'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Structural integrity assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_environmental_stress(self, flower_hsv: np.ndarray):\n",
    "        \"\"\"Assess environmental stress indicators\"\"\"\n",
    "        try:\n",
    "            flower_brightness = np.mean(flower_hsv[:, :, 2])\n",
    "            flower_saturation = np.mean(flower_hsv[:, :, 1])\n",
    "            \n",
    "            # Calculate stress indicators\n",
    "            brightness_stress = abs(flower_brightness - 0.5) * 2  # Optimal around 0.5\n",
    "            saturation_stress = max(0, 0.5 - flower_saturation) * 2  # Low saturation indicates stress\n",
    "            \n",
    "            stress_score = (brightness_stress + saturation_stress) / 2\n",
    "            self.results['environmental_stress']['stress_indicators'].append(stress_score)\n",
    "            \n",
    "            # Classify stress level\n",
    "            if stress_score < self.thresholds.no_stress_threshold:\n",
    "                self.results['environmental_stress']['no_stress'] += 1\n",
    "            elif stress_score < self.thresholds.mild_stress_threshold:\n",
    "                self.results['environmental_stress']['mild_stress'] += 1\n",
    "            elif stress_score < self.thresholds.moderate_stress_threshold:\n",
    "                self.results['environmental_stress']['moderate_stress'] += 1\n",
    "            else:\n",
    "                self.results['environmental_stress']['severe_stress'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Environmental stress assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_pollinator_attractiveness(self, flower_hsv: np.ndarray):\n",
    "        \"\"\"Assess pollinator attractiveness based on color characteristics\"\"\"\n",
    "        try:\n",
    "            fh, fw = flower_hsv.shape[:2]\n",
    "            \n",
    "            # Extract center and petal regions\n",
    "            center_hsv = flower_hsv[fh//3:2*fh//3, fw//3:2*fw//3]\n",
    "            petal_brightness = np.mean(flower_hsv[:, :, 2])\n",
    "            flower_saturation = np.mean(flower_hsv[:, :, 1])\n",
    "            \n",
    "            if center_hsv.size > 10:\n",
    "                center_brightness = np.mean(center_hsv[:, :, 2])\n",
    "                center_petal_contrast = abs(center_brightness - petal_brightness)\n",
    "                \n",
    "                # UV reflection approximation\n",
    "                uv_indicator = petal_brightness * flower_saturation\n",
    "                \n",
    "                # Calculate attractiveness\n",
    "                attractiveness = (center_petal_contrast + uv_indicator + flower_saturation) / 3\n",
    "                self.results['pollinator_attractiveness']['attractiveness_scores'].append(attractiveness)\n",
    "                \n",
    "                # Classify attractiveness\n",
    "                if attractiveness > self.thresholds.high_attractiveness_threshold:\n",
    "                    self.results['pollinator_attractiveness']['high_attractiveness'] += 1\n",
    "                elif attractiveness > self.thresholds.medium_attractiveness_threshold:\n",
    "                    self.results['pollinator_attractiveness']['medium_attractiveness'] += 1\n",
    "                else:\n",
    "                    self.results['pollinator_attractiveness']['low_attractiveness'] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Pollinator attractiveness assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_temporal_patterns(self, overall_brightness: float):\n",
    "        \"\"\"Assess temporal health patterns based on lighting conditions\"\"\"\n",
    "        try:\n",
    "            # Simulate time-of-day effects\n",
    "            if overall_brightness > 0.7:\n",
    "                self.results['temporal_health_patterns']['morning_optimal'] += 1\n",
    "                time_score = 1.0\n",
    "            elif overall_brightness > 0.3:\n",
    "                self.results['temporal_health_patterns']['midday_stress'] += 1\n",
    "                time_score = 0.5\n",
    "            else:\n",
    "                self.results['temporal_health_patterns']['evening_recovery'] += 1\n",
    "                time_score = 0.8\n",
    "            \n",
    "            self.results['temporal_health_patterns']['time_indicators'].append(time_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Temporal pattern assessment failed: {e}\")\n",
    "\n",
    "def analyze_pollination_health_comprehensive(dataset, dataset_name: str, max_samples: int = 350) -> Dict:\n",
    "    \"\"\"Enhanced health and pollination analysis with batch processing\"\"\"\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Invalid dataset for health analysis: {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(f\"Analyzing health and pollination in {dataset_name}\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = FlowerHealthAnalyzer()\n",
    "    \n",
    "    # Prepare sample data\n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = config.max_batch_size\n",
    "    processed_count = 0\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        batch_data = []\n",
    "        \n",
    "        # Load batch data\n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                image, targets, path = dataset[idx]\n",
    "                batch_data.append((idx, image, targets, path))\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to load image {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_data:\n",
    "            # Process batch\n",
    "            batch_results = analyzer.process_image_batch(batch_data)\n",
    "            processed_count += len(batch_results)\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if (i // batch_size) % 3 == 0:\n",
    "            MemoryManager.clear_memory()\n",
    "    \n",
    "    # Add metadata\n",
    "    analyzer.results['metadata'] = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'images_processed': processed_count,\n",
    "        'total_sampled': sample_size,\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Health analysis completed for {dataset_name}: {processed_count} images processed\")\n",
    "    return analyzer.results\n",
    "\n",
    "def create_health_assessment_visualization(health_analysis_results: Dict):\n",
    "    \"\"\"Create memory-efficient health assessment visualization\"\"\"\n",
    "    \n",
    "    if not health_analysis_results:\n",
    "        logger.error(\"No health analysis results for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create figure with memory-efficient layout\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        for idx, (dataset_name, metrics) in enumerate(health_analysis_results.items()):\n",
    "            if idx >= 1:  # Process only first dataset for memory efficiency\n",
    "                break\n",
    "                \n",
    "            create_health_plots(fig, gs, dataset_name, metrics)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with optimization\n",
    "        output_path = notebook_results_dir / 'health_assessment' / 'comprehensive_health_analysis.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', optimize=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        plt.close(fig)\n",
    "        MemoryManager.clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "\n",
    "def create_health_plots(fig, gs, dataset_name: str, metrics: Dict):\n",
    "    \"\"\"Create individual health assessment plots\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Health Status Distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        health_data = metrics['health_indicators']\n",
    "        health_labels = ['Excellent', 'Good', 'Moderate', 'Poor', 'Diseased']\n",
    "        health_values = [\n",
    "            health_data['excellent_health'],\n",
    "            health_data['good_health'],\n",
    "            health_data['moderate_health'],\n",
    "            health_data['poor_health'],\n",
    "            health_data['diseased']\n",
    "        ]\n",
    "        \n",
    "        if any(health_values):\n",
    "            colors = ['darkgreen', 'lightgreen', 'yellow', 'orange', 'red']\n",
    "            bars = ax1.bar(range(len(health_labels)), health_values, color=colors, alpha=0.8)\n",
    "            ax1.set_title('Health Status', fontweight='bold', fontsize=11)\n",
    "            ax1.set_ylabel('Count')\n",
    "            ax1.set_xticks(range(len(health_labels)))\n",
    "            ax1.set_xticklabels([label[:4] for label in health_labels])\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, health_values):\n",
    "                if value > 0:\n",
    "                    height = bar.get_height()\n",
    "                    ax1.text(bar.get_x() + bar.get_width()/2., height + max(health_values)*0.01,\n",
    "                           f'{value}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 2. Pollination State\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        poll_data = metrics['pollination_assessment']\n",
    "        poll_labels = ['Unpoll.', 'Active', 'Post', 'Seed']\n",
    "        poll_values = [\n",
    "            poll_data['unpollinated_indicators'],\n",
    "            poll_data['active_pollination'],\n",
    "            poll_data['post_pollination'],\n",
    "            poll_data['seed_development']\n",
    "        ]\n",
    "        \n",
    "        if any(poll_values):\n",
    "            ax2.pie(poll_values, labels=poll_labels, autopct='%1.1f%%',\n",
    "                   colors=sns.color_palette(\"Blues\", len(poll_labels)))\n",
    "            ax2.set_title('Pollination State', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # 3. Maturity Stages\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        maturity_data = metrics['flower_maturity']\n",
    "        maturity_labels = ['E.Bud', 'L.Bud', 'E.Bloom', 'Peak', 'L.Bloom', 'Senes.']\n",
    "        maturity_values = [\n",
    "            maturity_data['early_bud'],\n",
    "            maturity_data['late_bud'],\n",
    "            maturity_data['early_bloom'],\n",
    "            maturity_data['peak_bloom'],\n",
    "            maturity_data['late_bloom'],\n",
    "            maturity_data['senescence']\n",
    "        ]\n",
    "        \n",
    "        if any(maturity_values):\n",
    "            ax3.bar(range(len(maturity_labels)), maturity_values,\n",
    "                   color=sns.color_palette(\"viridis\", len(maturity_labels)), alpha=0.8)\n",
    "            ax3.set_title('Maturity Stages', fontweight='bold', fontsize=11)\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.set_xticks(range(len(maturity_labels)))\n",
    "            ax3.set_xticklabels(maturity_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # 4. Environmental Stress\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        stress_data = metrics['environmental_stress']\n",
    "        stress_labels = ['None', 'Mild', 'Mod.', 'Severe']\n",
    "        stress_values = [\n",
    "            stress_data['no_stress'],\n",
    "            stress_data['mild_stress'],\n",
    "            stress_data['moderate_stress'],\n",
    "            stress_data['severe_stress']\n",
    "        ]\n",
    "        \n",
    "        if any(stress_values):\n",
    "            ax4.bar(range(len(stress_labels)), stress_values,\n",
    "                   color=['green', 'yellow', 'orange', 'red'], alpha=0.8)\n",
    "            ax4.set_title('Environmental Stress', fontweight='bold', fontsize=11)\n",
    "            ax4.set_ylabel('Count')\n",
    "            ax4.set_xticks(range(len(stress_labels)))\n",
    "            ax4.set_xticklabels(stress_labels)\n",
    "        \n",
    "        # 5. Health Score Distribution\n",
    "        ax5 = fig.add_subplot(gs[1, 0:2])\n",
    "        if health_data['health_scores']:\n",
    "            ax5.hist(health_data['health_scores'], bins=20, alpha=0.7, \n",
    "                    color='lightblue', density=True, edgecolor='black')\n",
    "            ax5.set_title('Health Score Distribution', fontweight='bold', fontsize=11)\n",
    "            ax5.set_xlabel('Health Score (0-1)')\n",
    "            ax5.set_ylabel('Density')\n",
    "            ax5.grid(True, alpha=0.3)\n",
    "            \n",
    "            mean_health = np.mean(health_data['health_scores'])\n",
    "            ax5.axvline(mean_health, color='red', linestyle='--', alpha=0.8,\n",
    "                       label=f'Mean: {mean_health:.3f}')\n",
    "            ax5.legend()\n",
    "        \n",
    "        # 6. Structural Integrity\n",
    "        ax6 = fig.add_subplot(gs[1, 2:4])\n",
    "        struct_data = metrics['structural_integrity']\n",
    "        struct_labels = ['Perfect', 'Minor Damage', 'Moderate Damage', 'Severe Damage']\n",
    "        struct_values = [\n",
    "            struct_data['perfect_petals'],\n",
    "            struct_data['minor_damage'],\n",
    "            struct_data['moderate_damage'],\n",
    "            struct_data['severe_damage']\n",
    "        ]\n",
    "        \n",
    "        if any(struct_values):\n",
    "            ax6.pie(struct_values, labels=struct_labels, autopct='%1.1f%%',\n",
    "                   colors=['green', 'yellow', 'orange', 'red'])\n",
    "            ax6.set_title('Structural Integrity', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # 7. Summary Statistics\n",
    "        ax7 = fig.add_subplot(gs[2, :])\n",
    "        \n",
    "        total_flowers = sum(health_values)\n",
    "        healthy_flowers = health_values[0] + health_values[1] if total_flowers > 0 else 0\n",
    "        pollinated_flowers = poll_values[2] + poll_values[3] if sum(poll_values) > 0 else 0\n",
    "        \n",
    "        summary_text = f\"\"\"Health Assessment Summary - {dataset_name}\n",
    "        \n",
    "Total Flowers: {total_flowers}\n",
    "        \n",
    "Health Status:\n",
    "  Healthy (Excellent + Good): {healthy_flowers} ({healthy_flowers/max(1,total_flowers)*100:.1f}%)\n",
    "  Problematic (Poor + Diseased): {health_values[3] + health_values[4]} ({(health_values[3] + health_values[4])/max(1,total_flowers)*100:.1f}%)\n",
    "  \n",
    "Pollination Progress:\n",
    "  Pre-pollination: {poll_values[0]} ({poll_values[0]/max(1,sum(poll_values))*100:.1f}%)\n",
    "  Completed: {pollinated_flowers} ({pollinated_flowers/max(1,sum(poll_values))*100:.1f}%)\n",
    "  \n",
    "Environmental Assessment:\n",
    "  Low Stress: {stress_values[0] + stress_values[1]} ({(stress_values[0] + stress_values[1])/max(1,sum(stress_values))*100:.1f}%)\n",
    "  High Stress: {stress_values[3]} ({stress_values[3]/max(1,sum(stress_values))*100:.1f}%)\n",
    "  \n",
    "Recommendations:\n",
    " {'Monitor health closely' if healthy_flowers/max(1,total_flowers) < 0.7 else 'Health status good'}\n",
    " {'Support pollination' if sum(poll_values[:2])/max(1,sum(poll_values)) > 0.6 else 'Pollination progressing'}\n",
    " {'Address stress factors' if (stress_values[2] + stress_values[3])/max(1,sum(stress_values)) > 0.3 else 'Stress levels acceptable'}\"\"\"\n",
    "        \n",
    "        ax7.text(0.05, 0.95, summary_text, transform=ax7.transAxes,\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "        ax7.set_title('Comprehensive Health Analysis Summary', fontweight='bold', fontsize=12)\n",
    "        ax7.axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create health plots: {e}\")\n",
    "\n",
    "# Execute comprehensive health and pollination analysis\n",
    "def execute_health_analysis():\n",
    "    \"\"\"Execute the complete health analysis pipeline\"\"\"\n",
    "    \n",
    "    if 'datasets' not in locals() or not datasets:\n",
    "        logger.error(\"No datasets available for health analysis\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Starting comprehensive health and pollination analysis\")\n",
    "    \n",
    "    health_analysis_results = {}\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        logger.info(f\"Analyzing health and pollination in {name}\")\n",
    "        \n",
    "        try:\n",
    "            health_metrics = safe_operation(\n",
    "                f\"Health and pollination analysis for {name}\",\n",
    "                analyze_pollination_health_comprehensive,\n",
    "                dataset, name, 350\n",
    "            )\n",
    "            \n",
    "            if health_metrics and 'health_indicators' in health_metrics:\n",
    "                health_analysis_results[name] = health_metrics\n",
    "                \n",
    "                # Log key findings\n",
    "                health_data = health_metrics['health_indicators']\n",
    "                poll_data = health_metrics['pollination_assessment']\n",
    "                \n",
    "                logger.info(f\"Health analysis complete for {name}:\")\n",
    "                \n",
    "                # Health summary\n",
    "                total_health = sum([health_data['excellent_health'], health_data['good_health'], \n",
    "                                  health_data['moderate_health'], health_data['poor_health'], \n",
    "                                  health_data['diseased']])\n",
    "                \n",
    "                if total_health > 0:\n",
    "                    healthy_ratio = (health_data['excellent_health'] + health_data['good_health']) / total_health\n",
    "                    logger.info(f\"  Healthy flowers: {healthy_ratio*100:.1f}%\")\n",
    "                \n",
    "                if health_data['health_scores']:\n",
    "                    avg_health = np.mean(health_data['health_scores'])\n",
    "                    logger.info(f\"  Average health score: {avg_health:.3f}\")\n",
    "                \n",
    "                # Pollination summary\n",
    "                total_poll = sum([poll_data['unpollinated_indicators'], poll_data['active_pollination'], \n",
    "                                poll_data['post_pollination'], poll_data['seed_development']])\n",
    "                \n",
    "                if total_poll > 0:\n",
    "                    completed_ratio = (poll_data['post_pollination'] + poll_data['seed_development']) / total_poll\n",
    "                    logger.info(f\"  Pollination completed: {completed_ratio*100:.1f}%\")\n",
    "            \n",
    "            else:\n",
    "                logger.warning(f\"Health analysis failed or returned empty results for {name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to analyze health in {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create visualization if we have results\n",
    "    if health_analysis_results:\n",
    "        safe_operation(\n",
    "            \"Creating health assessment visualization\",\n",
    "            create_health_assessment_visualization,\n",
    "            health_analysis_results\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            save_health_analysis_results(health_analysis_results)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save health analysis results: {e}\")\n",
    "    \n",
    "    return health_analysis_results\n",
    "\n",
    "def save_health_analysis_results(results: Dict):\n",
    "    \"\"\"Save health analysis results with proper serialization\"\"\"\n",
    "    \n",
    "    serializable_results = {}\n",
    "    \n",
    "    for dataset_name, metrics in results.items():\n",
    "        serializable_results[dataset_name] = {\n",
    "            'pollination_assessment': {\n",
    "                'unpollinated': metrics['pollination_assessment']['unpollinated_indicators'],\n",
    "                'active_pollination': metrics['pollination_assessment']['active_pollination'],\n",
    "                'post_pollination': metrics['pollination_assessment']['post_pollination'],\n",
    "                'seed_development': metrics['pollination_assessment']['seed_development'],\n",
    "                'avg_confidence': float(np.mean(metrics['pollination_assessment']['pollination_confidence_scores'])) if metrics['pollination_assessment']['pollination_confidence_scores'] else 0\n",
    "            },\n",
    "            'health_status': {\n",
    "                'excellent': metrics['health_indicators']['excellent_health'],\n",
    "                'good': metrics['health_indicators']['good_health'],\n",
    "                'moderate': metrics['health_indicators']['moderate_health'],\n",
    "                'poor': metrics['health_indicators']['poor_health'],\n",
    "                'diseased': metrics['health_indicators']['diseased'],\n",
    "                'avg_health_score': float(np.mean(metrics['health_indicators']['health_scores'])) if metrics['health_indicators']['health_scores'] else 0,\n",
    "                'health_score_std': float(np.std(metrics['health_indicators']['health_scores'])) if metrics['health_indicators']['health_scores'] else 0\n",
    "            },\n",
    "            'maturity_stages': {\n",
    "                'early_bud': metrics['flower_maturity']['early_bud'],\n",
    "                'late_bud': metrics['flower_maturity']['late_bud'],\n",
    "                'early_bloom': metrics['flower_maturity']['early_bloom'],\n",
    "                'peak_bloom': metrics['flower_maturity']['peak_bloom'],\n",
    "                'late_bloom': metrics['flower_maturity']['late_bloom'],\n",
    "                'senescence': metrics['flower_maturity']['senescence'],\n",
    "                'avg_maturity': float(np.mean(metrics['flower_maturity']['maturity_progression'])) if metrics['flower_maturity']['maturity_progression'] else 0\n",
    "            },\n",
    "            'structural_integrity': {\n",
    "                'perfect_petals': metrics['structural_integrity']['perfect_petals'],\n",
    "                'minor_damage': metrics['structural_integrity']['minor_damage'],\n",
    "                'moderate_damage': metrics['structural_integrity']['moderate_damage'],\n",
    "                'severe_damage': metrics['structural_integrity']['severe_damage'],\n",
    "                'avg_structural_score': float(np.mean(metrics['structural_integrity']['structural_scores'])) if metrics['structural_integrity']['structural_scores'] else 0\n",
    "            },\n",
    "            'environmental_stress': {\n",
    "                'no_stress': metrics['environmental_stress']['no_stress'],\n",
    "                'mild_stress': metrics['environmental_stress']['mild_stress'],\n",
    "                'moderate_stress': metrics['environmental_stress']['moderate_stress'],\n",
    "                'severe_stress': metrics['environmental_stress']['severe_stress'],\n",
    "                'avg_stress_level': float(np.mean(metrics['environmental_stress']['stress_indicators'])) if metrics['environmental_stress']['stress_indicators'] else 0\n",
    "            },\n",
    "            'pollinator_attractiveness': {\n",
    "                'high': metrics['pollinator_attractiveness']['high_attractiveness'],\n",
    "                'medium': metrics['pollinator_attractiveness']['medium_attractiveness'],\n",
    "                'low': metrics['pollinator_attractiveness']['low_attractiveness'],\n",
    "                'avg_attractiveness': float(np.mean(metrics['pollinator_attractiveness']['attractiveness_scores'])) if metrics['pollinator_attractiveness']['attractiveness_scores'] else 0\n",
    "            },\n",
    "            'temporal_patterns': {\n",
    "                'morning_optimal': metrics['temporal_health_patterns']['morning_optimal'],\n",
    "                'midday_stress': metrics['temporal_health_patterns']['midday_stress'],\n",
    "                'evening_recovery': metrics['temporal_health_patterns']['evening_recovery']\n",
    "            },\n",
    "            'metadata': metrics.get('metadata', {})\n",
    "        }\n",
    "    \n",
    "    output_path = notebook_results_dir / 'health_assessment' / 'comprehensive_health_analysis.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Health analysis results saved to {output_path}\")\n",
    "\n",
    "# Execute the analysis\n",
    "health_analysis_results = execute_health_analysis()\n",
    "\n",
    "if not health_analysis_results:\n",
    "    logger.error(\"Health analysis failed completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a925db5",
   "metadata": {},
   "source": [
    "## 6. Flower Sample Visualization and Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced flower sample visualization and comprehensive morphological analysis\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "@dataclass\n",
    "class MorphologicalThresholds:\n",
    "    \"\"\"Scientific thresholds for morphological analysis\"\"\"\n",
    "    # Size categorization thresholds (based on agricultural studies)\n",
    "    micro_area_threshold: float = 0.005\n",
    "    small_area_threshold: float = 0.02\n",
    "    medium_area_threshold: float = 0.08\n",
    "    large_area_threshold: float = 0.2\n",
    "    \n",
    "    # Shape classification thresholds\n",
    "    round_aspect_ratio_range: Tuple[float, float] = (0.8, 1.2)\n",
    "    horizontal_aspect_ratio: float = 1.5\n",
    "    vertical_aspect_ratio: float = 0.67\n",
    "    \n",
    "    # Texture analysis thresholds\n",
    "    edge_density_low: float = 0.05\n",
    "    edge_density_high: float = 0.15\n",
    "    texture_uniformity_threshold: float = 100\n",
    "    \n",
    "    # Spatial distribution thresholds\n",
    "    clustered_variance_ratio: float = 0.5\n",
    "    regular_variance_ratio: float = 0.1\n",
    "    \n",
    "    # Health assessment thresholds\n",
    "    vibrant_saturation: float = 0.6\n",
    "    vibrant_brightness: float = 0.4\n",
    "    healthy_saturation: float = 0.3\n",
    "    healthy_brightness: float = 0.2\n",
    "\n",
    "class MorphologicalAnalyzer:\n",
    "    \"\"\"Modular analyzer for comprehensive morphological characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: MorphologicalThresholds = None):\n",
    "        self.thresholds = thresholds or MorphologicalThresholds()\n",
    "        self.results = self._initialize_metrics()\n",
    "    \n",
    "    def _initialize_metrics(self) -> Dict:\n",
    "        \"\"\"Initialize morphological metrics structure\"\"\"\n",
    "        return {\n",
    "            'shape_descriptors': {\n",
    "                'aspect_ratios': [],\n",
    "                'compactness_scores': [],\n",
    "                'elongation_indices': [],\n",
    "                'circularity_measures': [],\n",
    "                'solidity_values': [],\n",
    "                'convexity_scores': []\n",
    "            },\n",
    "            'size_analysis': {\n",
    "                'area_distribution': [],\n",
    "                'perimeter_analysis': [],\n",
    "                'equivalent_diameters': [],\n",
    "                'size_categories': {'micro': 0, 'small': 0, 'medium': 0, 'large': 0, 'macro': 0},\n",
    "                'size_entropy': 0\n",
    "            },\n",
    "            'geometric_properties': {\n",
    "                'major_axis_lengths': [],\n",
    "                'minor_axis_lengths': [],\n",
    "                'eccentricity_values': [],\n",
    "                'orientation_angles': [],\n",
    "                'bounding_box_ratios': []\n",
    "            },\n",
    "            'texture_features': {\n",
    "                'edge_density_scores': [],\n",
    "                'texture_uniformity': [],\n",
    "                'gradient_magnitudes': [],\n",
    "                'local_binary_patterns': [],\n",
    "                'surface_roughness': []\n",
    "            },\n",
    "            'spatial_relationships': {\n",
    "                'nearest_neighbor_distances': [],\n",
    "                'cluster_formations': [],\n",
    "                'spatial_randomness_index': 0,\n",
    "                'distribution_patterns': {'clustered': 0, 'regular': 0, 'random': 0}\n",
    "            },\n",
    "            'developmental_indicators': {\n",
    "                'maturity_shape_correlation': [],\n",
    "                'growth_pattern_indicators': [],\n",
    "                'symmetry_measures': [],\n",
    "                'structural_complexity': []\n",
    "            },\n",
    "            'species_characteristics': {\n",
    "                'morphological_diversity_index': 0,\n",
    "                'shape_variability_coefficient': 0,\n",
    "                'characteristic_ratios': [],\n",
    "                'taxonomic_indicators': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_image_batch(self, image_batch: List[Tuple]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of images for morphological analysis\"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        for image_data in image_batch:\n",
    "            try:\n",
    "                result = self.analyze_single_image(image_data)\n",
    "                if result and result.get('processed', False):\n",
    "                    batch_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to process image in morphological batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    def analyze_single_image(self, image_data: Tuple) -> Dict:\n",
    "        \"\"\"Analyze morphological characteristics of a single image\"\"\"\n",
    "        idx, image, targets, path = image_data\n",
    "        \n",
    "        # Convert image format\n",
    "        img_np = self._convert_image_format(image)\n",
    "        if img_np is None:\n",
    "            return {'error': 'Invalid image format', 'processed': False}\n",
    "        \n",
    "        if targets.numel() == 0:\n",
    "            return {'error': 'No targets', 'processed': False}\n",
    "        \n",
    "        h, w = img_np.shape[:2]\n",
    "        \n",
    "        # Convert to grayscale for texture analysis\n",
    "        try:\n",
    "            gray = cv2.cvtColor((img_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Grayscale conversion failed: {e}\")\n",
    "            return {'error': 'Grayscale conversion failed', 'processed': False}\n",
    "        \n",
    "        # Process each flower in the image\n",
    "        flower_positions = []\n",
    "        flowers_processed = 0\n",
    "        \n",
    "        for target in targets:\n",
    "            if len(target) >= 5:\n",
    "                processed = self._analyze_single_flower(target, img_np, gray, h, w, flower_positions)\n",
    "                if processed:\n",
    "                    flowers_processed += 1\n",
    "        \n",
    "        # Analyze spatial relationships\n",
    "        if len(flower_positions) >= 2:\n",
    "            self._analyze_spatial_relationships(flower_positions)\n",
    "        \n",
    "        return {\n",
    "            'processed': True,\n",
    "            'flowers_processed': flowers_processed,\n",
    "            'flower_positions': flower_positions\n",
    "        }\n",
    "    \n",
    "    def _convert_image_format(self, image) -> Optional[np.ndarray]:\n",
    "        \"\"\"Convert image to proper numpy format\"\"\"\n",
    "        try:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:\n",
    "                    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "                elif img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                return img_np\n",
    "            elif isinstance(image, np.ndarray):\n",
    "                return np.clip(image, 0, 1)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Image conversion failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_single_flower(self, target: torch.Tensor, img_np: np.ndarray,\n",
    "                              gray: np.ndarray, h: int, w: int, flower_positions: List) -> bool:\n",
    "        \"\"\"Analyze morphological characteristics of a single flower\"\"\"\n",
    "        try:\n",
    "            cls, x_center, y_center, width, height = target[:5]\n",
    "            \n",
    "            # Extract flower region\n",
    "            x1 = max(0, int((x_center - width/2) * w))\n",
    "            y1 = max(0, int((y_center - height/2) * h))\n",
    "            x2 = min(w, int((x_center + width/2) * w))\n",
    "            y2 = min(h, int((y_center + height/2) * h))\n",
    "            \n",
    "            if x2 <= x1 or y2 <= y1 or (x2-x1)*(y2-y1) < 100:\n",
    "                return False\n",
    "            \n",
    "            flower_region = img_np[y1:y2, x1:x2]\n",
    "            flower_gray = gray[y1:y2, x1:x2]\n",
    "            \n",
    "            # Perform comprehensive analysis\n",
    "            self._analyze_shape_descriptors(width, height)\n",
    "            self._analyze_size_characteristics(width, height)\n",
    "            self._analyze_geometric_properties(width, height)\n",
    "            self._analyze_texture_features(flower_gray)\n",
    "            self._analyze_developmental_indicators(width, height, flower_gray)\n",
    "            \n",
    "            # Store position for spatial analysis\n",
    "            flower_positions.append([float(x_center), float(y_center)])\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Single flower morphological analysis failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _analyze_shape_descriptors(self, width: float, height: float):\n",
    "        \"\"\"Analyze shape descriptors\"\"\"\n",
    "        try:\n",
    "            area = width * height\n",
    "            perimeter = 2 * (width + height)\n",
    "            \n",
    "            # Aspect ratio\n",
    "            aspect_ratio = width / height if height > 0 else 1.0\n",
    "            self.results['shape_descriptors']['aspect_ratios'].append(aspect_ratio)\n",
    "            \n",
    "            # Compactness (perimeter/(4*area))\n",
    "            compactness = (perimeter ** 2) / (4 * np.pi * area) if area > 0 else 0\n",
    "            self.results['shape_descriptors']['compactness_scores'].append(compactness)\n",
    "            \n",
    "            # Elongation index\n",
    "            elongation = max(width, height) / min(width, height) if min(width, height) > 0 else 1.0\n",
    "            self.results['shape_descriptors']['elongation_indices'].append(elongation)\n",
    "            \n",
    "            # Circularity approximation\n",
    "            circularity = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0\n",
    "            self.results['shape_descriptors']['circularity_measures'].append(circularity)\n",
    "            \n",
    "            # Solidity approximation (area/convex_area)\n",
    "            solidity = min(1.0, area / (width * height)) if width * height > 0 else 0\n",
    "            self.results['shape_descriptors']['solidity_values'].append(solidity)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Shape descriptor analysis failed: {e}\")\n",
    "    \n",
    "    def _analyze_size_characteristics(self, width: float, height: float):\n",
    "        \"\"\"Analyze size-related characteristics\"\"\"\n",
    "        try:\n",
    "            area = width * height\n",
    "            perimeter = 2 * (width + height)\n",
    "            equivalent_diameter = 2 * np.sqrt(area / np.pi)\n",
    "            \n",
    "            # Store distributions\n",
    "            self.results['size_analysis']['area_distribution'].append(area)\n",
    "            self.results['size_analysis']['perimeter_analysis'].append(perimeter)\n",
    "            self.results['size_analysis']['equivalent_diameters'].append(equivalent_diameter)\n",
    "            \n",
    "            # Size categorization\n",
    "            if area < self.thresholds.micro_area_threshold:\n",
    "                self.results['size_analysis']['size_categories']['micro'] += 1\n",
    "            elif area < self.thresholds.small_area_threshold:\n",
    "                self.results['size_analysis']['size_categories']['small'] += 1\n",
    "            elif area < self.thresholds.medium_area_threshold:\n",
    "                self.results['size_analysis']['size_categories']['medium'] += 1\n",
    "            elif area < self.thresholds.large_area_threshold:\n",
    "                self.results['size_analysis']['size_categories']['large'] += 1\n",
    "            else:\n",
    "                self.results['size_analysis']['size_categories']['macro'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Size analysis failed: {e}\")\n",
    "    \n",
    "    def _analyze_geometric_properties(self, width: float, height: float):\n",
    "        \"\"\"Analyze geometric properties\"\"\"\n",
    "        try:\n",
    "            major_axis = max(width, height)\n",
    "            minor_axis = min(width, height)\n",
    "            \n",
    "            self.results['geometric_properties']['major_axis_lengths'].append(major_axis)\n",
    "            self.results['geometric_properties']['minor_axis_lengths'].append(minor_axis)\n",
    "            \n",
    "            # Eccentricity\n",
    "            if major_axis > 0:\n",
    "                eccentricity = np.sqrt(1 - (minor_axis ** 2) / (major_axis ** 2))\n",
    "                self.results['geometric_properties']['eccentricity_values'].append(eccentricity)\n",
    "            \n",
    "            # Bounding box ratio\n",
    "            bbox_ratio = width / height if height > 0 else 1.0\n",
    "            self.results['geometric_properties']['bounding_box_ratios'].append(bbox_ratio)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Geometric properties analysis failed: {e}\")\n",
    "    \n",
    "    def _analyze_texture_features(self, flower_gray: np.ndarray):\n",
    "        \"\"\"Analyze texture features\"\"\"\n",
    "        try:\n",
    "            # Edge density\n",
    "            edges = cv2.Canny(flower_gray, 50, 150)\n",
    "            edge_density = np.sum(edges) / flower_gray.size\n",
    "            self.results['texture_features']['edge_density_scores'].append(edge_density)\n",
    "            \n",
    "            # Texture uniformity (inverse of variance)\n",
    "            texture_uniformity = 1 / (np.var(flower_gray) + 1)\n",
    "            self.results['texture_features']['texture_uniformity'].append(texture_uniformity)\n",
    "            \n",
    "            # Gradient magnitude\n",
    "            grad_x = cv2.Sobel(flower_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(flower_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            gradient_magnitude = np.mean(np.sqrt(grad_x**2 + grad_y**2))\n",
    "            self.results['texture_features']['gradient_magnitudes'].append(gradient_magnitude)\n",
    "            \n",
    "            # Local Binary Pattern approximation\n",
    "            lbp_variance = np.var(flower_gray)\n",
    "            self.results['texture_features']['local_binary_patterns'].append(lbp_variance)\n",
    "            \n",
    "            # Surface roughness\n",
    "            surface_roughness = np.std(grad_x) + np.std(grad_y)\n",
    "            self.results['texture_features']['surface_roughness'].append(surface_roughness)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Texture feature analysis failed: {e}\")\n",
    "    \n",
    "    def _analyze_developmental_indicators(self, width: float, height: float, flower_gray: np.ndarray):\n",
    "        \"\"\"Analyze developmental indicators\"\"\"\n",
    "        try:\n",
    "            area = width * height\n",
    "            aspect_ratio = width / height if height > 0 else 1.0\n",
    "            \n",
    "            # Maturity-shape correlation (larger, rounder flowers tend to be more mature)\n",
    "            elongation = max(width, height) / min(width, height) if min(width, height) > 0 else 1.0\n",
    "            maturity_indicator = area * (1 / elongation)\n",
    "            self.results['developmental_indicators']['maturity_shape_correlation'].append(maturity_indicator)\n",
    "            \n",
    "            # Symmetry measure\n",
    "            symmetry = 1 / aspect_ratio if aspect_ratio > 1 else aspect_ratio\n",
    "            self.results['developmental_indicators']['symmetry_measures'].append(symmetry)\n",
    "            \n",
    "            # Structural complexity\n",
    "            try:\n",
    "                edges = cv2.Canny(flower_gray, 50, 150)\n",
    "                edge_density = np.sum(edges) / flower_gray.size\n",
    "                grad_magnitude = np.mean(cv2.Sobel(flower_gray, cv2.CV_64F, 1, 0, ksize=3)**2 + \n",
    "                                       cv2.Sobel(flower_gray, cv2.CV_64F, 0, 1, ksize=3)**2)\n",
    "                complexity = edge_density * grad_magnitude\n",
    "                self.results['developmental_indicators']['structural_complexity'].append(complexity)\n",
    "            except Exception:\n",
    "                self.results['developmental_indicators']['structural_complexity'].append(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Developmental indicators analysis failed: {e}\")\n",
    "    \n",
    "    def _analyze_spatial_relationships(self, flower_positions: List):\n",
    "        \"\"\"Analyze spatial relationships between flowers\"\"\"\n",
    "        try:\n",
    "            positions = np.array(flower_positions)\n",
    "            \n",
    "            # Calculate nearest neighbor distances\n",
    "            for i, pos in enumerate(positions):\n",
    "                distances = [np.linalg.norm(pos - other_pos) \n",
    "                           for j, other_pos in enumerate(positions) if i != j]\n",
    "                if distances:\n",
    "                    self.results['spatial_relationships']['nearest_neighbor_distances'].append(min(distances))\n",
    "            \n",
    "            # Analyze spatial distribution pattern\n",
    "            if len(self.results['spatial_relationships']['nearest_neighbor_distances']) >= 2:\n",
    "                recent_distances = self.results['spatial_relationships']['nearest_neighbor_distances'][-len(positions):]\n",
    "                mean_distance = np.mean(recent_distances)\n",
    "                distance_variance = np.var(recent_distances)\n",
    "                \n",
    "                # Pattern classification\n",
    "                variance_ratio = distance_variance / (mean_distance + 1e-6)\n",
    "                \n",
    "                if variance_ratio < self.thresholds.regular_variance_ratio:\n",
    "                    self.results['spatial_relationships']['distribution_patterns']['regular'] += 1\n",
    "                elif variance_ratio > self.thresholds.clustered_variance_ratio:\n",
    "                    self.results['spatial_relationships']['distribution_patterns']['clustered'] += 1\n",
    "                else:\n",
    "                    self.results['spatial_relationships']['distribution_patterns']['random'] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Spatial relationship analysis failed: {e}\")\n",
    "    \n",
    "    def finalize_analysis(self):\n",
    "        \"\"\"Finalize analysis with post-processing calculations\"\"\"\n",
    "        try:\n",
    "            # Calculate size entropy\n",
    "            if self.results['size_analysis']['area_distribution']:\n",
    "                area_hist, _ = np.histogram(self.results['size_analysis']['area_distribution'], bins=10)\n",
    "                area_hist = area_hist + 1  # Pseudocount\n",
    "                area_probs = area_hist / np.sum(area_hist)\n",
    "                self.results['size_analysis']['size_entropy'] = entropy(area_probs)\n",
    "            \n",
    "            # Calculate morphological diversity\n",
    "            self._calculate_morphological_diversity()\n",
    "            \n",
    "            # Calculate spatial randomness index\n",
    "            self._calculate_spatial_randomness()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Analysis finalization failed: {e}\")\n",
    "    \n",
    "    def _calculate_morphological_diversity(self):\n",
    "        \"\"\"Calculate morphological diversity metrics\"\"\"\n",
    "        try:\n",
    "            shape_features = ['aspect_ratios', 'compactness_scores', 'circularity_measures', 'elongation_indices']\n",
    "            cvs = []\n",
    "            \n",
    "            for feature in shape_features:\n",
    "                values = self.results['shape_descriptors'][feature]\n",
    "                if values:\n",
    "                    cv = np.std(values) / (np.mean(values) + 1e-6)\n",
    "                    cvs.append(cv)\n",
    "            \n",
    "            if cvs:\n",
    "                self.results['species_characteristics']['morphological_diversity_index'] = np.mean(cvs)\n",
    "                self.results['species_characteristics']['shape_variability_coefficient'] = np.std(cvs)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Morphological diversity calculation failed: {e}\")\n",
    "    \n",
    "    def _calculate_spatial_randomness(self):\n",
    "        \"\"\"Calculate spatial randomness index\"\"\"\n",
    "        try:\n",
    "            distances = self.results['spatial_relationships']['nearest_neighbor_distances']\n",
    "            if distances:\n",
    "                observed_mean = np.mean(distances)\n",
    "                expected_mean = 0.5  # Expected for random distribution\n",
    "                self.results['spatial_relationships']['spatial_randomness_index'] = (\n",
    "                    observed_mean / expected_mean if expected_mean > 0 else 1\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Spatial randomness calculation failed: {e}\")\n",
    "\n",
    "class FlowerSampleAnalyzer:\n",
    "    \"\"\"Analyzer for individual flower sample characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: MorphologicalThresholds = None):\n",
    "        self.thresholds = thresholds or MorphologicalThresholds()\n",
    "    \n",
    "    def analyze_sample_flowers(self, img_np: np.ndarray, targets: torch.Tensor, dataset) -> Dict:\n",
    "        \"\"\"Comprehensive analysis of flowers in a sample image\"\"\"\n",
    "        \n",
    "        if targets.numel() == 0:\n",
    "            return {'num_flowers': 0, 'analyses': []}\n",
    "        \n",
    "        h, w = img_np.shape[:2]\n",
    "        flower_analyses = []\n",
    "        \n",
    "        for j, target in enumerate(targets):\n",
    "            if len(target) >= 5:\n",
    "                analysis = self._analyze_single_flower_sample(\n",
    "                    target, img_np, h, w, j, dataset\n",
    "                )\n",
    "                if analysis:\n",
    "                    flower_analyses.append(analysis)\n",
    "        \n",
    "        return {\n",
    "            'num_flowers': len(flower_analyses),\n",
    "            'analyses': flower_analyses,\n",
    "            'diversity_score': self._calculate_image_diversity_score(flower_analyses)\n",
    "        }\n",
    "    \n",
    "    def _analyze_single_flower_sample(self, target: torch.Tensor, img_np: np.ndarray,\n",
    "                                     h: int, w: int, flower_id: int, dataset) -> Optional[Dict]:\n",
    "        \"\"\"Analyze a single flower in a sample\"\"\"\n",
    "        try:\n",
    "            cls, x_center, y_center, width, height = target[:5]\n",
    "            \n",
    "            # Extract flower region\n",
    "            x1 = max(0, int((x_center - width/2) * w))\n",
    "            y1 = max(0, int((y_center - height/2) * h))\n",
    "            x2 = min(w, int((x_center + width/2) * w))\n",
    "            y2 = min(h, int((y_center + height/2) * h))\n",
    "            \n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                return None\n",
    "            \n",
    "            flower_region = img_np[y1:y2, x1:x2]\n",
    "            area = width * height\n",
    "            aspect_ratio = width / height if height > 0 else 1.0\n",
    "            \n",
    "            # Classify size\n",
    "            size_class = self._classify_size(area)\n",
    "            \n",
    "            # Classify shape\n",
    "            shape_class = self._classify_shape(aspect_ratio)\n",
    "            \n",
    "            # Analyze color\n",
    "            color_analysis = self._analyze_flower_color(flower_region)\n",
    "            \n",
    "            # Assess health\n",
    "            health_status = self._assess_flower_health(color_analysis)\n",
    "            \n",
    "            # Get class name\n",
    "            class_name = self._get_class_name(int(cls), dataset)\n",
    "            \n",
    "            return {\n",
    "                'id': flower_id + 1,\n",
    "                'class': class_name,\n",
    "                'size_class': size_class,\n",
    "                'shape_class': shape_class,\n",
    "                'color_name': color_analysis['color_name'],\n",
    "                'health_status': health_status,\n",
    "                'area': area,\n",
    "                'aspect_ratio': aspect_ratio,\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'center': (x_center, y_center),\n",
    "                'color_metrics': color_analysis['metrics']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Single flower sample analysis failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _classify_size(self, area: float) -> str:\n",
    "        \"\"\"Classify flower size\"\"\"\n",
    "        if area < self.thresholds.micro_area_threshold:\n",
    "            return \"Tiny\"\n",
    "        elif area < self.thresholds.small_area_threshold:\n",
    "            return \"Small\"\n",
    "        elif area < self.thresholds.medium_area_threshold:\n",
    "            return \"Medium\"\n",
    "        elif area < self.thresholds.large_area_threshold:\n",
    "            return \"Large\"\n",
    "        else:\n",
    "            return \"Huge\"\n",
    "    \n",
    "    def _classify_shape(self, aspect_ratio: float) -> str:\n",
    "        \"\"\"Classify flower shape\"\"\"\n",
    "        if (self.thresholds.round_aspect_ratio_range[0] <= aspect_ratio <= \n",
    "            self.thresholds.round_aspect_ratio_range[1]):\n",
    "            return \"Round\"\n",
    "        elif aspect_ratio > self.thresholds.horizontal_aspect_ratio:\n",
    "            return \"Horizontal\"\n",
    "        elif aspect_ratio < self.thresholds.vertical_aspect_ratio:\n",
    "            return \"Vertical\"\n",
    "        else:\n",
    "            return \"Oval\"\n",
    "    \n",
    "    def _analyze_flower_color(self, flower_region: np.ndarray) -> Dict:\n",
    "        \"\"\"Analyze flower color characteristics\"\"\"\n",
    "        try:\n",
    "            if len(flower_region.shape) != 3:\n",
    "                return {'color_name': 'Unknown', 'metrics': {'hue': 0, 'saturation': 0, 'brightness': 0}}\n",
    "            \n",
    "            hsv_region = color.rgb2hsv(flower_region)\n",
    "            avg_hue = np.mean(hsv_region[:, :, 0])\n",
    "            avg_saturation = np.mean(hsv_region[:, :, 1])\n",
    "            avg_brightness = np.mean(hsv_region[:, :, 2])\n",
    "            \n",
    "            # Color name classification\n",
    "            if 0.0 <= avg_hue < 0.1 or 0.9 <= avg_hue <= 1.0:\n",
    "                color_name = \"Red/Pink\"\n",
    "            elif 0.1 <= avg_hue < 0.2:\n",
    "                color_name = \"Orange\"\n",
    "            elif 0.2 <= avg_hue < 0.4:\n",
    "                color_name = \"Yellow\"\n",
    "            elif 0.4 <= avg_hue < 0.7:\n",
    "                color_name = \"Green/Blue\"\n",
    "            else:\n",
    "                color_name = \"Purple\"\n",
    "            \n",
    "            return {\n",
    "                'color_name': color_name,\n",
    "                'metrics': {\n",
    "                    'hue': avg_hue,\n",
    "                    'saturation': avg_saturation,\n",
    "                    'brightness': avg_brightness\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Color analysis failed: {e}\")\n",
    "            return {'color_name': 'Unknown', 'metrics': {'hue': 0, 'saturation': 0, 'brightness': 0}}\n",
    "    \n",
    "    def _assess_flower_health(self, color_analysis: Dict) -> str:\n",
    "        \"\"\"Assess flower health based on color metrics\"\"\"\n",
    "        metrics = color_analysis['metrics']\n",
    "        saturation = metrics['saturation']\n",
    "        brightness = metrics['brightness']\n",
    "        \n",
    "        if (saturation > self.thresholds.vibrant_saturation and \n",
    "            brightness > self.thresholds.vibrant_brightness):\n",
    "            return \"Vibrant\"\n",
    "        elif (saturation > self.thresholds.healthy_saturation and \n",
    "              brightness > self.thresholds.healthy_brightness):\n",
    "            return \"Healthy\"\n",
    "        elif saturation > 0.1:\n",
    "            return \"Moderate\"\n",
    "        else:\n",
    "            return \"Stressed\"\n",
    "    \n",
    "    def _get_class_name(self, cls: int, dataset) -> str:\n",
    "        \"\"\"Get class name from dataset\"\"\"\n",
    "        if hasattr(dataset, 'class_names') and cls < len(dataset.class_names):\n",
    "            return dataset.class_names[cls]\n",
    "        else:\n",
    "            return f\"Class_{cls}\"\n",
    "    \n",
    "    def _calculate_image_diversity_score(self, flower_analyses: List[Dict]) -> float:\n",
    "        \"\"\"Calculate diversity score for an image\"\"\"\n",
    "        if not flower_analyses:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            # Size diversity\n",
    "            areas = [a['area'] for a in flower_analyses]\n",
    "            size_diversity = np.std(areas) / (np.mean(areas) + 1e-6) if areas else 0\n",
    "            \n",
    "            # Shape diversity\n",
    "            aspect_ratios = [a['aspect_ratio'] for a in flower_analyses]\n",
    "            shape_diversity = np.std(aspect_ratios) / (np.mean(aspect_ratios) + 1e-6) if aspect_ratios else 0\n",
    "            \n",
    "            # Color diversity\n",
    "            hues = [a['color_metrics']['hue'] for a in flower_analyses]\n",
    "            color_diversity = np.std(hues) if hues else 0\n",
    "            \n",
    "            # Health diversity\n",
    "            health_types = len(set(a['health_status'] for a in flower_analyses))\n",
    "            health_diversity = health_types / 4.0  # Normalize by max possible\n",
    "            \n",
    "            # Combined diversity score\n",
    "            diversity_score = (size_diversity * 0.3 + shape_diversity * 0.3 + \n",
    "                              color_diversity * 0.2 + health_diversity * 0.2)\n",
    "            \n",
    "            return diversity_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Diversity score calculation failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "def analyze_flower_morphology_comprehensive(dataset, dataset_name: str, max_samples: int = 400) -> Tuple[Dict, List]:\n",
    "    \"\"\"Enhanced morphological analysis with batch processing\"\"\"\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Invalid dataset for morphological analysis: {dataset_name}\")\n",
    "        return {}, []\n",
    "    \n",
    "    logger.info(f\"Analyzing flower morphology in {dataset_name}\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = MorphologicalAnalyzer()\n",
    "    \n",
    "    # Prepare sample data\n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = config.max_batch_size\n",
    "    processed_count = 0\n",
    "    all_flower_properties = []\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        batch_data = []\n",
    "        \n",
    "        # Load batch data\n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                image, targets, path = dataset[idx]\n",
    "                batch_data.append((idx, image, targets, path))\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to load image {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_data:\n",
    "            # Process batch\n",
    "            batch_results = analyzer.process_image_batch(batch_data)\n",
    "            processed_count += len([r for r in batch_results if r.get('processed', False)])\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if (i // batch_size) % 3 == 0:\n",
    "            MemoryManager.clear_memory()\n",
    "    \n",
    "    # Finalize analysis\n",
    "    analyzer.finalize_analysis()\n",
    "    \n",
    "    # Add metadata\n",
    "    analyzer.results['metadata'] = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'images_processed': processed_count,\n",
    "        'total_sampled': sample_size,\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Morphological analysis completed for {dataset_name}: {processed_count} images processed\")\n",
    "    return analyzer.results, all_flower_properties\n",
    "\n",
    "def select_diverse_samples(dataset, num_samples: int) -> List[int]:\n",
    "    \"\"\"Select diverse samples based on morphological characteristics\"\"\"\n",
    "    \n",
    "    analysis_subset = min(200, len(dataset))\n",
    "    subset_indices = np.random.choice(len(dataset), analysis_subset, replace=False)\n",
    "    \n",
    "    diversity_scores = []\n",
    "    \n",
    "    for idx in subset_indices:\n",
    "        try:\n",
    "            _, targets, _ = dataset[idx]\n",
    "            \n",
    "            if targets.numel() == 0:\n",
    "                diversity_scores.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Calculate diversity metrics\n",
    "            num_flowers = len(targets)\n",
    "            areas = []\n",
    "            aspect_ratios = []\n",
    "            \n",
    "            for target in targets:\n",
    "                if len(target) >= 5:\n",
    "                    _, _, _, width, height = target[:5]\n",
    "                    area = width * height\n",
    "                    aspect_ratio = width / height if height > 0 else 1.0\n",
    "                    \n",
    "                    areas.append(area)\n",
    "                    aspect_ratios.append(aspect_ratio)\n",
    "            \n",
    "            size_variety = 0\n",
    "            shape_variety = 0\n",
    "            \n",
    "            if areas:\n",
    "                size_variety = np.std(areas) / (np.mean(areas) + 1e-6)\n",
    "                shape_variety = np.std(aspect_ratios) / (np.mean(aspect_ratios) + 1e-6)\n",
    "            \n",
    "            # Combined diversity score\n",
    "            diversity_score = (num_flowers * 0.3 + size_variety * 0.4 + shape_variety * 0.3)\n",
    "            diversity_scores.append(diversity_score)\n",
    "            \n",
    "        except Exception:\n",
    "            diversity_scores.append(0)\n",
    "    \n",
    "    # Select top diverse samples\n",
    "    diversity_indices = np.argsort(diversity_scores)[-num_samples:]\n",
    "    selected_indices = [subset_indices[i] for i in diversity_indices]\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "def visualize_flower_samples_enhanced(dataset, dataset_name: str, num_samples: int = 12, \n",
    "                                    diversity_selection: bool = True) -> List[Dict]:\n",
    "    \"\"\"Enhanced flower sample visualization with diversity-based selection\"\"\"\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Invalid dataset for sample visualization: {dataset_name}\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Visualizing enhanced flower samples from {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Select samples\n",
    "        if diversity_selection:\n",
    "            sample_indices = select_diverse_samples(dataset, num_samples)\n",
    "        else:\n",
    "            sample_indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "        \n",
    "        # Create grid layout\n",
    "        cols = 4\n",
    "        rows = (num_samples + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        axes_flat = axes.flatten()\n",
    "        sample_analyses = []\n",
    "        sample_analyzer = FlowerSampleAnalyzer()\n",
    "        \n",
    "        # Process each sample\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            if i >= len(axes_flat):\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                image, targets, path = dataset[idx]\n",
    "                \n",
    "                # Convert image for display\n",
    "                img_np = sample_analyzer._convert_image_format(image)\n",
    "                if img_np is None:\n",
    "                    continue\n",
    "                \n",
    "                # Display image\n",
    "                axes_flat[i].imshow(img_np)\n",
    "                \n",
    "                # Analyze flowers\n",
    "                flower_analysis = sample_analyzer.analyze_sample_flowers(img_np, targets, dataset)\n",
    "                sample_analyses.append(flower_analysis)\n",
    "                \n",
    "                # Draw annotations\n",
    "                draw_enhanced_annotations(axes_flat[i], targets, flower_analysis)\n",
    "                \n",
    "                # Create title\n",
    "                title_text = create_enhanced_title(flower_analysis, path, i+1)\n",
    "                axes_flat[i].set_title(title_text, fontsize=9, pad=8)\n",
    "                axes_flat[i].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes_flat[i].text(0.5, 0.5, f'Error loading\\nsample {i+1}', \n",
    "                                 ha='center', va='center', transform=axes_flat[i].transAxes,\n",
    "                                 bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "                axes_flat[i].axis('off')\n",
    "                logger.debug(f\"Failed to process sample {i+1}: {e}\")\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(sample_indices), len(axes_flat)):\n",
    "            axes_flat[i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Enhanced Flower Sample Analysis - {dataset_name}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with optimization\n",
    "        output_path = notebook_results_dir / 'samples' / f'enhanced_flower_samples_{dataset_name.lower()}.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', optimize=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        plt.close(fig)\n",
    "        MemoryManager.clear_memory()\n",
    "        \n",
    "        # Generate summary\n",
    "        generate_sample_analysis_summary(sample_analyses, dataset_name)\n",
    "        \n",
    "        return sample_analyses\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sample visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "        return []\n",
    "\n",
    "def draw_enhanced_annotations(ax, targets: torch.Tensor, flower_analysis: Dict):\n",
    "    \"\"\"Draw enhanced annotations with morphological information\"\"\"\n",
    "    \n",
    "    if flower_analysis['num_flowers'] == 0:\n",
    "        ax.text(0.5, 0.1, 'No flowers detected', transform=ax.transAxes,\n",
    "                ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
    "        return\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'cyan']\n",
    "    \n",
    "    for analysis in flower_analysis['analyses']:\n",
    "        color = colors[analysis['id'] % len(colors)]\n",
    "        x1, y1, x2, y2 = analysis['bbox']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        from matplotlib.patches import Rectangle\n",
    "        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                        linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Draw flower ID\n",
    "        ax.text(x1, y1-3, f\"{analysis['id']}\", color=color, fontsize=10, \n",
    "                fontweight='bold', bbox=dict(boxstyle='round,pad=0.2', \n",
    "                facecolor='white', alpha=0.9))\n",
    "        \n",
    "        # Draw size indicator\n",
    "        size_indicator = analysis['size_class'][0]\n",
    "        ax.text(x2-12, y1+3, size_indicator, color=color, fontsize=8, \n",
    "                fontweight='bold', bbox=dict(boxstyle='circle,pad=0.2', \n",
    "                facecolor='yellow', alpha=0.8))\n",
    "        \n",
    "        # Draw health indicator\n",
    "        health_colors = {\n",
    "            'Vibrant': 'green',\n",
    "            'Healthy': 'lightgreen', \n",
    "            'Moderate': 'orange',\n",
    "            'Stressed': 'red'\n",
    "        }\n",
    "        health_color = health_colors.get(analysis['health_status'], 'gray')\n",
    "        ax.text(x1+3, y2-10, '', color=health_color, fontsize=12, fontweight='bold')\n",
    "\n",
    "def create_enhanced_title(flower_analysis: Dict, path: str, sample_num: int) -> str:\n",
    "    \"\"\"Create comprehensive title with flower analysis summary\"\"\"\n",
    "    \n",
    "    filename = Path(path).stem\n",
    "    num_flowers = flower_analysis['num_flowers']\n",
    "    \n",
    "    if num_flowers == 0:\n",
    "        return f'Sample {sample_num}: No Flowers\\n{filename}'\n",
    "    \n",
    "    analyses = flower_analysis['analyses']\n",
    "    \n",
    "    # Size distribution\n",
    "    size_counts = Counter([a['size_class'] for a in analyses])\n",
    "    most_common_size = size_counts.most_common(1)[0][0] if size_counts else \"Unknown\"\n",
    "    \n",
    "    # Health distribution\n",
    "    health_counts = Counter([a['health_status'] for a in analyses])\n",
    "    healthy_ratio = (health_counts.get('Vibrant', 0) + health_counts.get('Healthy', 0)) / num_flowers\n",
    "    \n",
    "    # Diversity score\n",
    "    diversity = flower_analysis.get('diversity_score', 0)\n",
    "    \n",
    "    title = f'Sample {sample_num}: {num_flowers} flowers\\n'\n",
    "    title += f'Size: {most_common_size} | Health: {healthy_ratio:.0%} | Div: {diversity:.2f}\\n'\n",
    "    title += f'{filename}'\n",
    "    \n",
    "    return title\n",
    "\n",
    "def generate_sample_analysis_summary(sample_analyses: List[Dict], dataset_name: str):\n",
    "    \"\"\"Generate comprehensive summary of sample analyses\"\"\"\n",
    "    \n",
    "    if not sample_analyses:\n",
    "        logger.warning(f\"No sample analyses to summarize for {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        summary = {\n",
    "            'total_samples': len(sample_analyses),\n",
    "            'total_flowers': sum(s['num_flowers'] for s in sample_analyses),\n",
    "            'avg_flowers_per_sample': np.mean([s['num_flowers'] for s in sample_analyses]),\n",
    "            'diversity_scores': [s.get('diversity_score', 0) for s in sample_analyses]\n",
    "        }\n",
    "        \n",
    "        # Aggregate flower characteristics\n",
    "        all_analyses = []\n",
    "        for sample in sample_analyses:\n",
    "            all_analyses.extend(sample['analyses'])\n",
    "        \n",
    "        if all_analyses:\n",
    "            # Distributions\n",
    "            summary.update({\n",
    "                'size_distribution': dict(Counter([a['size_class'] for a in all_analyses])),\n",
    "                'shape_distribution': dict(Counter([a['shape_class'] for a in all_analyses])),\n",
    "                'color_distribution': dict(Counter([a['color_name'] for a in all_analyses])),\n",
    "                'health_distribution': dict(Counter([a['health_status'] for a in all_analyses]))\n",
    "            })\n",
    "            \n",
    "            # Morphological statistics\n",
    "            summary['morphological_stats'] = {\n",
    "                'avg_area': np.mean([a['area'] for a in all_analyses]),\n",
    "                'std_area': np.std([a['area'] for a in all_analyses]),\n",
    "                'avg_aspect_ratio': np.mean([a['aspect_ratio'] for a in all_analyses]),\n",
    "                'std_aspect_ratio': np.std([a['aspect_ratio'] for a in all_analyses])\n",
    "            }\n",
    "        \n",
    "        # Save summary\n",
    "        output_path = notebook_results_dir / 'samples' / f'{dataset_name.lower()}_sample_analysis.json'\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        # Log summary\n",
    "        logger.info(f\"Sample analysis summary for {dataset_name}:\")\n",
    "        logger.info(f\"  Total samples: {summary['total_samples']}\")\n",
    "        logger.info(f\"  Total flowers: {summary['total_flowers']}\")\n",
    "        logger.info(f\"  Avg flowers per sample: {summary['avg_flowers_per_sample']:.1f}\")\n",
    "        \n",
    "        if all_analyses:\n",
    "            logger.info(f\"  Most common size: {max(summary['size_distribution'], key=summary['size_distribution'].get)}\")\n",
    "            logger.info(f\"  Most common health: {max(summary['health_distribution'], key=summary['health_distribution'].get)}\")\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate sample analysis summary: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_morphological_analysis_visualization(morphology_results: Dict):\n",
    "    \"\"\"Create memory-efficient morphological analysis visualization\"\"\"\n",
    "    \n",
    "    if not morphology_results:\n",
    "        logger.error(\"No morphological analysis results for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create figure with memory-efficient layout\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        for idx, (dataset_name, metrics) in enumerate(morphology_results.items()):\n",
    "            if idx >= 1:  # Process only first dataset for memory efficiency\n",
    "                break\n",
    "                \n",
    "            create_morphology_plots(fig, gs, dataset_name, metrics)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with optimization\n",
    "        output_path = notebook_results_dir / 'morphology' / 'comprehensive_morphological_analysis.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', optimize=True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        plt.close(fig)\n",
    "        MemoryManager.clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Morphological visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "\n",
    "def create_morphology_plots(fig, gs, dataset_name: str, metrics: Dict):\n",
    "    \"\"\"Create individual morphological analysis plots\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Shape Descriptors\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        shape_desc = metrics['shape_descriptors']\n",
    "        if shape_desc['aspect_ratios']:\n",
    "            ax1.hist(shape_desc['aspect_ratios'], bins=20, alpha=0.7, density=True)\n",
    "            ax1.set_title('Aspect Ratio Distribution', fontweight='bold', fontsize=11)\n",
    "            ax1.set_xlabel('Aspect Ratio')\n",
    "            ax1.set_ylabel('Density')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Size Categories\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        size_categories = metrics['size_analysis']['size_categories']\n",
    "        cat_labels = list(size_categories.keys())\n",
    "        cat_values = list(size_categories.values())\n",
    "        \n",
    "        if any(cat_values):\n",
    "            ax2.pie(cat_values, labels=cat_labels, autopct='%1.1f%%',\n",
    "                   colors=sns.color_palette(\"viridis\", len(cat_labels)))\n",
    "            ax2.set_title('Size Categories', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # 3. Geometric Properties\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        geom_props = metrics['geometric_properties']\n",
    "        if geom_props['major_axis_lengths'] and geom_props['minor_axis_lengths']:\n",
    "            ax3.scatter(geom_props['major_axis_lengths'], geom_props['minor_axis_lengths'],\n",
    "                       alpha=0.6, s=20)\n",
    "            ax3.set_title('Major vs Minor Axis', fontweight='bold', fontsize=11)\n",
    "            ax3.set_xlabel('Major Axis')\n",
    "            ax3.set_ylabel('Minor Axis')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Texture Features\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        texture_features = metrics['texture_features']\n",
    "        if texture_features['edge_density_scores']:\n",
    "            ax4.hist(texture_features['edge_density_scores'], bins=15, alpha=0.7, density=True)\n",
    "            ax4.set_title('Edge Density', fontweight='bold', fontsize=11)\n",
    "            ax4.set_xlabel('Edge Density')\n",
    "            ax4.set_ylabel('Density')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Diversity Metrics\n",
    "        ax5 = fig.add_subplot(gs[1, 0:2])\n",
    "        species_chars = metrics['species_characteristics']\n",
    "        diversity_metrics = {\n",
    "            'Morphological\\nDiversity': species_chars.get('morphological_diversity_index', 0),\n",
    "            'Shape\\nVariability': species_chars.get('shape_variability_coefficient', 0),\n",
    "            'Size\\nEntropy': metrics['size_analysis'].get('size_entropy', 0),\n",
    "            'Spatial\\nRandomness': metrics['spatial_relationships'].get('spatial_randomness_index', 0)\n",
    "        }\n",
    "        \n",
    "        div_labels = list(diversity_metrics.keys())\n",
    "        div_values = list(diversity_metrics.values())\n",
    "        \n",
    "        if any(div_values):\n",
    "            bars = ax5.bar(range(len(div_labels)), div_values,\n",
    "                          color=sns.color_palette(\"Set3\", len(div_labels)), alpha=0.8)\n",
    "            ax5.set_title('Diversity Metrics', fontweight='bold', fontsize=11)\n",
    "            ax5.set_ylabel('Score')\n",
    "            ax5.set_xticks(range(len(div_labels)))\n",
    "            ax5.set_xticklabels(div_labels)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, div_values):\n",
    "                height = bar.get_height()\n",
    "                ax5.text(bar.get_x() + bar.get_width()/2., height + max(div_values)*0.01,\n",
    "                        f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # 6. Spatial Distribution\n",
    "        ax6 = fig.add_subplot(gs[1, 2:4])\n",
    "        spatial_data = metrics['spatial_relationships']['distribution_patterns']\n",
    "        spatial_labels = list(spatial_data.keys())\n",
    "        spatial_values = list(spatial_data.values())\n",
    "        \n",
    "        if any(spatial_values):\n",
    "            ax6.pie(spatial_values, labels=spatial_labels, autopct='%1.1f%%',\n",
    "                   colors=['blue', 'green', 'orange'])\n",
    "            ax6.set_title('Spatial Distribution Patterns', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # 7. Summary Statistics\n",
    "        ax7 = fig.add_subplot(gs[2, :])\n",
    "        \n",
    "        if shape_desc['aspect_ratios']:\n",
    "            summary_text = f\"\"\"Morphological Analysis Summary - {dataset_name}\n",
    "\n",
    "Shape Characteristics:\n",
    "  Average Aspect Ratio: {np.mean(shape_desc['aspect_ratios']):.3f}\n",
    "  Average Compactness: {np.mean(shape_desc['compactness_scores']):.3f}\n",
    "  Average Circularity: {np.mean(shape_desc['circularity_measures']):.3f}\n",
    "\n",
    "Size Distribution:\n",
    "  Micro: {size_categories['micro']} | Small: {size_categories['small']} | Medium: {size_categories['medium']}\n",
    "  Large: {size_categories['large']} | Macro: {size_categories['macro']}\n",
    "\n",
    "Texture Properties:\n",
    "  Average Edge Density: {np.mean(texture_features['edge_density_scores']):.4f}\n",
    "  Average Texture Uniformity: {np.mean(texture_features['texture_uniformity']):.3f}\n",
    "\n",
    "Diversity Indices:\n",
    "  Morphological Diversity: {species_chars.get('morphological_diversity_index', 0):.3f}\n",
    "  Size Entropy: {metrics['size_analysis'].get('size_entropy', 0):.3f}\n",
    "  Spatial Randomness: {metrics['spatial_relationships'].get('spatial_randomness_index', 0):.3f}\n",
    "\n",
    "Spatial Patterns:\n",
    "  Clustered: {spatial_data.get('clustered', 0)} | Regular: {spatial_data.get('regular', 0)} | Random: {spatial_data.get('random', 0)}\"\"\"\n",
    "        else:\n",
    "            summary_text = f\"No morphological data available for {dataset_name}\"\n",
    "        \n",
    "        ax7.text(0.05, 0.95, summary_text, transform=ax7.transAxes,\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "        ax7.set_title('Comprehensive Morphological Summary', fontweight='bold', fontsize=12)\n",
    "        ax7.axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create morphology plots: {e}\")\n",
    "\n",
    "# Execute comprehensive morphological analysis\n",
    "def execute_morphological_analysis():\n",
    "    \"\"\"Execute the complete morphological analysis pipeline\"\"\"\n",
    "    \n",
    "    if 'datasets' not in locals() or not datasets:\n",
    "        logger.error(\"No datasets available for morphological analysis\")\n",
    "        return None, None\n",
    "    \n",
    "    logger.info(\"Starting comprehensive morphological analysis\")\n",
    "    \n",
    "    morphology_results = {}\n",
    "    all_flower_properties = {}\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        logger.info(f\"Analyzing morphology in {name}\")\n",
    "        \n",
    "        try:\n",
    "            # Morphological analysis\n",
    "            morphology_metrics, flower_properties = safe_operation(\n",
    "                f\"Morphological analysis for {name}\",\n",
    "                analyze_flower_morphology_comprehensive,\n",
    "                dataset, name, 400\n",
    "            )\n",
    "            \n",
    "            if morphology_metrics and 'shape_descriptors' in morphology_metrics:\n",
    "                morphology_results[name] = morphology_metrics\n",
    "                all_flower_properties[name] = flower_properties\n",
    "                \n",
    "                # Log key findings\n",
    "                shape_desc = morphology_metrics['shape_descriptors']\n",
    "                size_analysis = morphology_metrics['size_analysis']\n",
    "                \n",
    "                logger.info(f\"Morphological analysis complete for {name}:\")\n",
    "                if shape_desc['aspect_ratios']:\n",
    "                    logger.info(f\"  Average aspect ratio: {np.mean(shape_desc['aspect_ratios']):.3f}\")\n",
    "                    logger.info(f\"  Average compactness: {np.mean(shape_desc['compactness_scores']):.3f}\")\n",
    "                \n",
    "                # Size distribution\n",
    "                size_cats = size_analysis['size_categories']\n",
    "                total_size = sum(size_cats.values())\n",
    "                if total_size > 0:\n",
    "                    logger.info(f\"  Size distribution - Medium: {size_cats['medium']} ({size_cats['medium']/total_size*100:.1f}%)\")\n",
    "            \n",
    "            # Enhanced sample visualization\n",
    "            logger.info(f\"Creating enhanced sample visualization for {name}\")\n",
    "            safe_operation(\n",
    "                f\"Enhanced sample visualization for {name}\",\n",
    "                visualize_flower_samples_enhanced,\n",
    "                dataset, name, 12, True\n",
    "            )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to analyze morphology in {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create visualization if we have results\n",
    "    if morphology_results:\n",
    "        safe_operation(\n",
    "            \"Creating morphological analysis visualization\",\n",
    "            create_morphological_analysis_visualization,\n",
    "            morphology_results\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        try:\n",
    "            save_morphological_analysis_results(morphology_results)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save morphological results: {e}\")\n",
    "    \n",
    "    return morphology_results, all_flower_properties\n",
    "\n",
    "def save_morphological_analysis_results(results: Dict):\n",
    "    \"\"\"Save morphological analysis results with proper serialization\"\"\"\n",
    "    \n",
    "    serializable_results = {}\n",
    "    \n",
    "    for dataset_name, metrics in results.items():\n",
    "        shape_desc = metrics['shape_descriptors']\n",
    "        \n",
    "        serializable_results[dataset_name] = {\n",
    "            'shape_statistics': {\n",
    "                'avg_aspect_ratio': float(np.mean(shape_desc['aspect_ratios'])) if shape_desc['aspect_ratios'] else 0,\n",
    "                'std_aspect_ratio': float(np.std(shape_desc['aspect_ratios'])) if shape_desc['aspect_ratios'] else 0,\n",
    "                'avg_compactness': float(np.mean(shape_desc['compactness_scores'])) if shape_desc['compactness_scores'] else 0,\n",
    "                'avg_circularity': float(np.mean(shape_desc['circularity_measures'])) if shape_desc['circularity_measures'] else 0,\n",
    "                'avg_elongation': float(np.mean(shape_desc['elongation_indices'])) if shape_desc['elongation_indices'] else 0\n",
    "            },\n",
    "            'size_analysis': {\n",
    "                'size_categories': metrics['size_analysis']['size_categories'],\n",
    "                'avg_area': float(np.mean(metrics['size_analysis']['area_distribution'])) if metrics['size_analysis']['area_distribution'] else 0,\n",
    "                'size_entropy': float(metrics['size_analysis']['size_entropy']),\n",
    "                'avg_equivalent_diameter': float(np.mean(metrics['size_analysis']['equivalent_diameters'])) if metrics['size_analysis']['equivalent_diameters'] else 0\n",
    "            },\n",
    "            'geometric_properties': {\n",
    "                'avg_major_axis': float(np.mean(metrics['geometric_properties']['major_axis_lengths'])) if metrics['geometric_properties']['major_axis_lengths'] else 0,\n",
    "                'avg_minor_axis': float(np.mean(metrics['geometric_properties']['minor_axis_lengths'])) if metrics['geometric_properties']['minor_axis_lengths'] else 0,\n",
    "                'avg_eccentricity': float(np.mean(metrics['geometric_properties']['eccentricity_values'])) if metrics['geometric_properties']['eccentricity_values'] else 0\n",
    "            },\n",
    "            'texture_features': {\n",
    "                'avg_edge_density': float(np.mean(metrics['texture_features']['edge_density_scores'])) if metrics['texture_features']['edge_density_scores'] else 0,\n",
    "                'avg_texture_uniformity': float(np.mean(metrics['texture_features']['texture_uniformity'])) if metrics['texture_features']['texture_uniformity'] else 0,\n",
    "                'avg_gradient_magnitude': float(np.mean(metrics['texture_features']['gradient_magnitudes'])) if metrics['texture_features']['gradient_magnitudes'] else 0\n",
    "            },\n",
    "            'spatial_relationships': {\n",
    "                'distribution_patterns': metrics['spatial_relationships']['distribution_patterns'],\n",
    "                'spatial_randomness_index': float(metrics['spatial_relationships']['spatial_randomness_index']),\n",
    "                'avg_nearest_neighbor_distance': float(np.mean(metrics['spatial_relationships']['nearest_neighbor_distances'])) if metrics['spatial_relationships']['nearest_neighbor_distances'] else 0\n",
    "            },\n",
    "            'diversity_metrics': {\n",
    "                'morphological_diversity_index': float(metrics['species_characteristics']['morphological_diversity_index']),\n",
    "                'shape_variability_coefficient': float(metrics['species_characteristics']['shape_variability_coefficient']),\n",
    "                'size_entropy': float(metrics['size_analysis']['size_entropy'])\n",
    "            },\n",
    "            'developmental_indicators': {\n",
    "                'avg_symmetry': float(np.mean(metrics['developmental_indicators']['symmetry_measures'])) if metrics['developmental_indicators']['symmetry_measures'] else 0,\n",
    "                'avg_structural_complexity': float(np.mean(metrics['developmental_indicators']['structural_complexity'])) if metrics['developmental_indicators']['structural_complexity'] else 0\n",
    "            },\n",
    "            'metadata': metrics.get('metadata', {})\n",
    "        }\n",
    "    \n",
    "    output_path = notebook_results_dir / 'morphology' / 'comprehensive_morphological_analysis.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Morphological analysis results saved to {output_path}\")\n",
    "\n",
    "# Execute the analysis\n",
    "morphology_results, all_flower_properties = execute_morphological_analysis()\n",
    "\n",
    "if not morphology_results:\n",
    "    logger.error(\"Morphological analysis failed completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94647a",
   "metadata": {},
   "source": [
    "## 7. Flower-Specific Challenge Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "File: src/analysis/challenge_assessment.py\n",
    "Enhanced flower-specific challenge assessment with CBAM-STN-TPS-YOLO optimization insights\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class ChallengeAssessmentThresholds:\n",
    "    \"\"\"Scientific thresholds for challenge assessment based on computer vision research\"\"\"\n",
    "    # CBAM attention challenges\n",
    "    severe_bg_similarity: float = 0.15      # Color distance threshold\n",
    "    moderate_bg_similarity: float = 0.30\n",
    "    extreme_scale_complexity: float = 0.01  # Area variance threshold\n",
    "    high_scale_complexity: float = 0.005\n",
    "    \n",
    "    # STN transformation challenges\n",
    "    high_orientation_complexity: float = 0.5    # Aspect ratio deviation\n",
    "    medium_orientation_complexity: float = 0.25\n",
    "    severe_perspective_distortion: float = 5     # Laplacian variance normalized\n",
    "    moderate_perspective_distortion: float = 2\n",
    "    \n",
    "    # TPS deformation challenges\n",
    "    complex_deformation: float = 1000       # Texture variance * edge density\n",
    "    moderate_deformation: float = 500\n",
    "    extreme_shape_variance: float = 0.5     # Shape ratio variance\n",
    "    significant_shape_variance: float = 0.2\n",
    "    \n",
    "    # YOLO detection challenges\n",
    "    very_high_density: int = 8              # Flowers per image\n",
    "    high_density: int = 5\n",
    "    moderate_density: int = 2\n",
    "    extreme_size_variation: float = 1.0     # Coefficient of variation\n",
    "    high_size_variation: float = 0.6\n",
    "    moderate_size_variation: float = 0.3\n",
    "\n",
    "class ChallengeAssessmentAnalyzer:\n",
    "    \"\"\"Modular analyzer for comprehensive challenge assessment\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: ChallengeAssessmentThresholds = None):\n",
    "        self.thresholds = thresholds or ChallengeAssessmentThresholds()\n",
    "        self.results = self._initialize_metrics()\n",
    "    \n",
    "    def _initialize_metrics(self) -> Dict:\n",
    "        \"\"\"Initialize challenge metrics structure\"\"\"\n",
    "        return {\n",
    "            'cbam_attention_challenges': {\n",
    "                'background_similarity': {'severe': 0, 'moderate': 0, 'minimal': 0},\n",
    "                'multi_scale_complexity': {'extreme': 0, 'high': 0, 'manageable': 0},\n",
    "                'channel_discrimination': {'difficult': 0, 'moderate': 0, 'easy': 0},\n",
    "                'spatial_attention_requirements': []\n",
    "            },\n",
    "            'stn_transformation_challenges': {\n",
    "                'orientation_variations': {'high': 0, 'medium': 0, 'low': 0},\n",
    "                'perspective_distortions': {'severe': 0, 'moderate': 0, 'minimal': 0},\n",
    "                'geometric_normalization_needs': [],\n",
    "                'affine_transformation_complexity': []\n",
    "            },\n",
    "            'tps_deformation_challenges': {\n",
    "                'non_rigid_deformations': {'complex': 0, 'moderate': 0, 'simple': 0},\n",
    "                'petal_shape_variations': {'extreme': 0, 'significant': 0, 'minimal': 0},\n",
    "                'wind_deformation_effects': {'severe': 0, 'moderate': 0, 'minimal': 0},\n",
    "                'growth_pattern_complexity': []\n",
    "            },\n",
    "            'yolo_detection_challenges': {\n",
    "                'object_density': {'very_high': 0, 'high': 0, 'moderate': 0, 'low': 0},\n",
    "                'size_variation_issues': {'extreme': 0, 'high': 0, 'moderate': 0, 'low': 0},\n",
    "                'occlusion_problems': {'severe': 0, 'moderate': 0, 'minimal': 0},\n",
    "                'boundary_definition_difficulty': []\n",
    "            },\n",
    "            'environmental_challenges': {\n",
    "                'lighting_variations': {'extreme': 0, 'high': 0, 'moderate': 0, 'stable': 0},\n",
    "                'weather_effects': {'severe': 0, 'moderate': 0, 'minimal': 0},\n",
    "                'seasonal_changes': {'dramatic': 0, 'noticeable': 0, 'subtle': 0},\n",
    "                'temporal_consistency_issues': []\n",
    "            },\n",
    "            'agricultural_specific_challenges': {\n",
    "                'crop_stage_variations': {'multiple_stages': 0, 'few_stages': 0, 'uniform': 0},\n",
    "                'pollination_state_confusion': {'high_risk': 0, 'medium_risk': 0, 'low_risk': 0},\n",
    "                'disease_detection_difficulty': {'very_difficult': 0, 'difficult': 0, 'manageable': 0},\n",
    "                'harvest_timing_complexity': []\n",
    "            },\n",
    "            'overall_complexity_assessment': {\n",
    "                'detection_difficulty': {'very_hard': 0, 'hard': 0, 'moderate': 0, 'easy': 0},\n",
    "                'training_complexity': {'very_complex': 0, 'complex': 0, 'moderate': 0, 'simple': 0},\n",
    "                'deployment_feasibility': {'challenging': 0, 'moderate': 0, 'feasible': 0},\n",
    "                'performance_expectations': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_image_batch(self, image_batch: List[Tuple]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of images for challenge assessment\"\"\"\n",
    "        batch_results = []\n",
    "        for image_data in image_batch:\n",
    "            try:\n",
    "                result = self.analyze_single_image(image_data)\n",
    "                if result and result.get('processed', False):\n",
    "                    batch_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to process image in challenge assessment batch: {e}\")\n",
    "                continue\n",
    "        return batch_results\n",
    "    \n",
    "    def analyze_single_image(self, image_data: Tuple) -> Dict:\n",
    "        \"\"\"Analyze challenges in a single image\"\"\"\n",
    "        idx, image, targets, path = image_data\n",
    "        \n",
    "        # Convert image format\n",
    "        img_np = self._convert_image_format(image)\n",
    "        if img_np is None:\n",
    "            return {'error': 'Invalid image format', 'processed': False}\n",
    "        \n",
    "        if targets.numel() == 0:\n",
    "            return {'error': 'No targets', 'processed': False}\n",
    "        \n",
    "        h, w = img_np.shape[:2]\n",
    "        \n",
    "        # Convert to analysis formats\n",
    "        try:\n",
    "            gray = cv2.cvtColor((img_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "            hsv = color.rgb2hsv(img_np)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Color space conversion failed: {e}\")\n",
    "            return {'error': 'Color conversion failed', 'processed': False}\n",
    "        \n",
    "        # Calculate global characteristics\n",
    "        overall_brightness = np.mean(gray) / 255.0\n",
    "        overall_contrast = np.std(gray) / 255.0\n",
    "        brightness_variance = np.var(gray) / (255.0 ** 2)\n",
    "        \n",
    "        # Extract flower and background regions\n",
    "        flower_data = self._extract_flower_regions(img_np, targets, h, w)\n",
    "        if not flower_data['flower_regions']:\n",
    "            return {'error': 'No valid flower regions', 'processed': False}\n",
    "        \n",
    "        # Perform comprehensive challenge assessment\n",
    "        self._assess_cbam_challenges(flower_data)\n",
    "        self._assess_stn_challenges(targets, gray)\n",
    "        self._assess_tps_challenges(flower_data)\n",
    "        self._assess_yolo_challenges(targets, flower_data)\n",
    "        self._assess_environmental_challenges(overall_brightness, overall_contrast, brightness_variance, flower_data)\n",
    "        self._assess_agricultural_challenges(flower_data)\n",
    "        self._assess_overall_complexity()\n",
    "        \n",
    "        return {\n",
    "            'processed': True,\n",
    "            'flowers_processed': len(flower_data['flower_regions']),\n",
    "            'challenge_scores': self._calculate_challenge_scores()\n",
    "        }\n",
    "    \n",
    "    def _convert_image_format(self, image) -> Optional[np.ndarray]:\n",
    "        \"\"\"Convert image to proper numpy format\"\"\"\n",
    "        try:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:\n",
    "                    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "                elif img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                return img_np\n",
    "            return np.clip(image, 0, 1) if isinstance(image, np.ndarray) else None\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Image conversion failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_flower_regions(self, img_np: np.ndarray, targets: torch.Tensor, h: int, w: int) -> Dict:\n",
    "        \"\"\"Extract flower and background regions for analysis\"\"\"\n",
    "        flower_regions, background_regions, flower_areas, flower_positions = [], [], [], []\n",
    "        \n",
    "        for target in targets:\n",
    "            if len(target) >= 5:\n",
    "                cls, x_center, y_center, width, height = target[:5]\n",
    "                x1, y1 = max(0, int((x_center - width/2) * w)), max(0, int((y_center - height/2) * h))\n",
    "                x2, y2 = min(w, int((x_center + width/2) * w)), min(h, int((y_center + height/2) * h))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1 and (x2-x1)*(y2-y1) > 50:\n",
    "                    flower_region = img_np[y1:y2, x1:x2]\n",
    "                    flower_regions.append(flower_region)\n",
    "                    flower_areas.append(width * height)\n",
    "                    flower_positions.append([x_center, y_center])\n",
    "                    self._extract_background_region(img_np, x1, y1, x2, y2, h, w, background_regions)\n",
    "        \n",
    "        return {\n",
    "            'flower_regions': flower_regions,\n",
    "            'background_regions': background_regions,\n",
    "            'flower_areas': flower_areas,\n",
    "            'flower_positions': flower_positions\n",
    "        }\n",
    "    \n",
    "    def _extract_background_region(self, img_np: np.ndarray, x1: int, y1: int, x2: int, y2: int,\n",
    "                                  h: int, w: int, background_regions: List):\n",
    "        \"\"\"Extract background region around flower\"\"\"\n",
    "        try:\n",
    "            margin = 20\n",
    "            bg_x1, bg_y1 = max(0, x1 - margin), max(0, y1 - margin)\n",
    "            bg_x2, bg_y2 = min(w, x2 + margin), min(h, y2 + margin)\n",
    "            bg_region = img_np[bg_y1:bg_y2, bg_x1:bg_x2]\n",
    "            \n",
    "            flower_mask = np.zeros((bg_y2-bg_y1, bg_x2-bg_x1), dtype=bool)\n",
    "            rel_x1, rel_y1, rel_x2, rel_y2 = x1-bg_x1, y1-bg_y1, x2-bg_x1, y2-bg_y1\n",
    "            \n",
    "            if 0 <= rel_y1 < rel_y2 <= bg_y2-bg_y1 and 0 <= rel_x1 < rel_x2 <= bg_x2-bg_x1:\n",
    "                flower_mask[rel_y1:rel_y2, rel_x1:rel_x2] = True\n",
    "                bg_pixels = bg_region[~flower_mask.reshape(*bg_region.shape[:2], 1).repeat(3, axis=2)]\n",
    "                if len(bg_pixels) > 100:\n",
    "                    background_regions.append(bg_pixels.reshape(-1, 3))\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Background extraction failed: {e}\")\n",
    "    \n",
    "    def _assess_cbam_challenges(self, flower_data: Dict):\n",
    "        \"\"\"Assess CBAM-specific challenges\"\"\"\n",
    "        try:\n",
    "            flower_regions, background_regions, flower_areas = flower_data['flower_regions'], flower_data['background_regions'], flower_data['flower_areas']\n",
    "            \n",
    "            if flower_regions and background_regions:\n",
    "                self._assess_background_similarity(flower_regions, background_regions)\n",
    "            if flower_areas and len(flower_areas) > 1:\n",
    "                self._assess_multiscale_complexity(flower_areas)\n",
    "            if flower_regions:\n",
    "                self._assess_channel_discrimination(flower_regions)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"CBAM challenge assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_background_similarity(self, flower_regions: List, background_regions: List):\n",
    "        \"\"\"Assess background similarity challenges\"\"\"\n",
    "        try:\n",
    "            flower_colors = np.concatenate([f.reshape(-1, 3) for f in flower_regions])\n",
    "            bg_colors = np.concatenate(background_regions)\n",
    "            \n",
    "            flower_sample = self._sample_colors(flower_colors, 1000)\n",
    "            bg_sample = self._sample_colors(bg_colors, 1000)\n",
    "            \n",
    "            color_distance = np.linalg.norm(np.mean(flower_sample, axis=0) - np.mean(bg_sample, axis=0))\n",
    "            attention_requirement = 1 / (color_distance + 0.1)\n",
    "            self.results['cbam_attention_challenges']['spatial_attention_requirements'].append(attention_requirement)\n",
    "            \n",
    "            if color_distance < self.thresholds.severe_bg_similarity:\n",
    "                self.results['cbam_attention_challenges']['background_similarity']['severe'] += 1\n",
    "            elif color_distance < self.thresholds.moderate_bg_similarity:\n",
    "                self.results['cbam_attention_challenges']['background_similarity']['moderate'] += 1\n",
    "            else:\n",
    "                self.results['cbam_attention_challenges']['background_similarity']['minimal'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Background similarity assessment failed: {e}\")\n",
    "    \n",
    "    def _sample_colors(self, colors: np.ndarray, max_samples: int) -> np.ndarray:\n",
    "        \"\"\"Sample colors efficiently\"\"\"\n",
    "        return colors[np.random.choice(len(colors), max_samples, replace=False)] if len(colors) > max_samples else colors\n",
    "    \n",
    "    def _assess_multiscale_complexity(self, flower_areas: List):\n",
    "        \"\"\"Assess multi-scale complexity challenges\"\"\"\n",
    "        try:\n",
    "            scale_complexity = np.var(flower_areas) * (max(flower_areas) - min(flower_areas))\n",
    "            \n",
    "            if scale_complexity > self.thresholds.extreme_scale_complexity:\n",
    "                self.results['cbam_attention_challenges']['multi_scale_complexity']['extreme'] += 1\n",
    "            elif scale_complexity > self.thresholds.high_scale_complexity:\n",
    "                self.results['cbam_attention_challenges']['multi_scale_complexity']['high'] += 1\n",
    "            else:\n",
    "                self.results['cbam_attention_challenges']['multi_scale_complexity']['manageable'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Multi-scale complexity assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_channel_discrimination(self, flower_regions: List):\n",
    "        \"\"\"Assess channel discrimination difficulty\"\"\"\n",
    "        try:\n",
    "            hue_variances = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 50:\n",
    "                    hsv_flower = color.rgb2hsv(flower.reshape(-1, 1, 3))\n",
    "                    hue_variances.append(np.var(hsv_flower[:, 0, 0]))\n",
    "            \n",
    "            if hue_variances:\n",
    "                avg_hue_variance = np.mean(hue_variances)\n",
    "                if avg_hue_variance < 0.01:\n",
    "                    self.results['cbam_attention_challenges']['channel_discrimination']['difficult'] += 1\n",
    "                elif avg_hue_variance < 0.05:\n",
    "                    self.results['cbam_attention_challenges']['channel_discrimination']['moderate'] += 1\n",
    "                else:\n",
    "                    self.results['cbam_attention_challenges']['channel_discrimination']['easy'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Channel discrimination assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_stn_challenges(self, targets: torch.Tensor, gray: np.ndarray):\n",
    "        \"\"\"Assess STN transformation challenges\"\"\"\n",
    "        try:\n",
    "            if len(targets) >= 2:\n",
    "                self._assess_orientation_variations(targets)\n",
    "            self._assess_perspective_distortions(gray)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"STN challenge assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_orientation_variations(self, targets: torch.Tensor):\n",
    "        \"\"\"Assess orientation variation complexity\"\"\"\n",
    "        try:\n",
    "            orientation_complexity, valid_targets = 0, 0\n",
    "            \n",
    "            for target in targets:\n",
    "                if len(target) >= 5:\n",
    "                    _, _, _, width, height = target[:5]\n",
    "                    if height > 0:\n",
    "                        orientation_complexity += abs(width / height - 1.0)\n",
    "                        valid_targets += 1\n",
    "            \n",
    "            if valid_targets > 0:\n",
    "                avg_orientation_complexity = orientation_complexity / valid_targets\n",
    "                self.results['stn_transformation_challenges']['geometric_normalization_needs'].append(avg_orientation_complexity)\n",
    "                \n",
    "                if avg_orientation_complexity > self.thresholds.high_orientation_complexity:\n",
    "                    self.results['stn_transformation_challenges']['orientation_variations']['high'] += 1\n",
    "                elif avg_orientation_complexity > self.thresholds.medium_orientation_complexity:\n",
    "                    self.results['stn_transformation_challenges']['orientation_variations']['medium'] += 1\n",
    "                else:\n",
    "                    self.results['stn_transformation_challenges']['orientation_variations']['low'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Orientation variation assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_perspective_distortions(self, gray: np.ndarray):\n",
    "        \"\"\"Assess perspective distortion complexity\"\"\"\n",
    "        try:\n",
    "            perspective_distortion_indicator = cv2.Laplacian(gray, cv2.CV_64F).var() / 10000\n",
    "            self.results['stn_transformation_challenges']['affine_transformation_complexity'].append(perspective_distortion_indicator)\n",
    "            \n",
    "            if perspective_distortion_indicator > self.thresholds.severe_perspective_distortion:\n",
    "                self.results['stn_transformation_challenges']['perspective_distortions']['severe'] += 1\n",
    "            elif perspective_distortion_indicator > self.thresholds.moderate_perspective_distortion:\n",
    "                self.results['stn_transformation_challenges']['perspective_distortions']['moderate'] += 1\n",
    "            else:\n",
    "                self.results['stn_transformation_challenges']['perspective_distortions']['minimal'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Perspective distortion assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_tps_challenges(self, flower_data: Dict):\n",
    "        \"\"\"Assess TPS deformation challenges\"\"\"\n",
    "        try:\n",
    "            flower_regions = flower_data['flower_regions']\n",
    "            if flower_regions:\n",
    "                self._assess_non_rigid_deformations(flower_regions)\n",
    "                self._assess_petal_shape_variations(flower_regions)\n",
    "                self._assess_wind_deformation_effects(flower_regions)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"TPS challenge assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_non_rigid_deformations(self, flower_regions: List):\n",
    "        \"\"\"Assess non-rigid deformation complexity\"\"\"\n",
    "        try:\n",
    "            deformation_complexities = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 100:\n",
    "                    gray_flower = cv2.cvtColor((flower * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "                    texture_variance = np.var(gray_flower)\n",
    "                    edge_density = np.sum(cv2.Canny(gray_flower, 50, 150)) / gray_flower.size\n",
    "                    deformation_complexities.append(texture_variance * edge_density)\n",
    "            \n",
    "            if deformation_complexities:\n",
    "                avg_deformation = np.mean(deformation_complexities)\n",
    "                self.results['tps_deformation_challenges']['growth_pattern_complexity'].append(avg_deformation)\n",
    "                \n",
    "                if avg_deformation > self.thresholds.complex_deformation:\n",
    "                    self.results['tps_deformation_challenges']['non_rigid_deformations']['complex'] += 1\n",
    "                elif avg_deformation > self.thresholds.moderate_deformation:\n",
    "                    self.results['tps_deformation_challenges']['non_rigid_deformations']['moderate'] += 1\n",
    "                else:\n",
    "                    self.results['tps_deformation_challenges']['non_rigid_deformations']['simple'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Non-rigid deformation assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_petal_shape_variations(self, flower_regions: List):\n",
    "        \"\"\"Assess petal shape variation complexity\"\"\"\n",
    "        try:\n",
    "            if len(flower_regions) <= 1:\n",
    "                return\n",
    "            \n",
    "            shape_ratios = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 50:\n",
    "                    fh, fw = flower.shape[:2]\n",
    "                    if min(fh, fw) > 0:\n",
    "                        shape_ratios.append(max(fh, fw) / min(fh, fw))\n",
    "            \n",
    "            if shape_ratios:\n",
    "                shape_variance = np.var(shape_ratios)\n",
    "                if shape_variance > self.thresholds.extreme_shape_variance:\n",
    "                    self.results['tps_deformation_challenges']['petal_shape_variations']['extreme'] += 1\n",
    "                elif shape_variance > self.thresholds.significant_shape_variance:\n",
    "                    self.results['tps_deformation_challenges']['petal_shape_variations']['significant'] += 1\n",
    "                else:\n",
    "                    self.results['tps_deformation_challenges']['petal_shape_variations']['minimal'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Petal shape variation assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_wind_deformation_effects(self, flower_regions: List):\n",
    "        \"\"\"Assess wind deformation effects\"\"\"\n",
    "        try:\n",
    "            wind_effects = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 100:\n",
    "                    gray_flower = cv2.cvtColor((flower * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "                    edge_smoothness = 1 / (np.sum(cv2.Canny(gray_flower, 30, 100)) / gray_flower.size + 1e-6)\n",
    "                    wind_effects.append(edge_smoothness)\n",
    "            \n",
    "            if wind_effects:\n",
    "                avg_wind_effect = np.mean(wind_effects)\n",
    "                if avg_wind_effect < 100:\n",
    "                    self.results['tps_deformation_challenges']['wind_deformation_effects']['severe'] += 1\n",
    "                elif avg_wind_effect < 500:\n",
    "                    self.results['tps_deformation_challenges']['wind_deformation_effects']['moderate'] += 1\n",
    "                else:\n",
    "                    self.results['tps_deformation_challenges']['wind_deformation_effects']['minimal'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Wind deformation assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_yolo_challenges(self, targets: torch.Tensor, flower_data: Dict):\n",
    "        \"\"\"Assess YOLO detection challenges\"\"\"\n",
    "        try:\n",
    "            num_flowers = len(targets) if targets.numel() > 0 else 0\n",
    "            flower_areas, flower_positions = flower_data['flower_areas'], flower_data['flower_positions']\n",
    "            \n",
    "            self._assess_object_density(num_flowers)\n",
    "            if flower_areas and len(flower_areas) > 1:\n",
    "                self._assess_size_variations(flower_areas)\n",
    "            if len(flower_positions) >= 2:\n",
    "                self._assess_occlusion_problems(flower_positions)\n",
    "            self._assess_boundary_difficulty(flower_data['flower_regions'])\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"YOLO challenge assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_object_density(self, num_flowers: int):\n",
    "        \"\"\"Assess object density challenges\"\"\"\n",
    "        if num_flowers >= self.thresholds.very_high_density:\n",
    "            self.results['yolo_detection_challenges']['object_density']['very_high'] += 1\n",
    "        elif num_flowers >= self.thresholds.high_density:\n",
    "            self.results['yolo_detection_challenges']['object_density']['high'] += 1\n",
    "        elif num_flowers >= self.thresholds.moderate_density:\n",
    "            self.results['yolo_detection_challenges']['object_density']['moderate'] += 1\n",
    "        else:\n",
    "            self.results['yolo_detection_challenges']['object_density']['low'] += 1\n",
    "    \n",
    "    def _assess_size_variations(self, flower_areas: List):\n",
    "        \"\"\"Assess size variation challenges\"\"\"\n",
    "        try:\n",
    "            size_cv = np.std(flower_areas) / (np.mean(flower_areas) + 1e-6)\n",
    "            \n",
    "            if size_cv > self.thresholds.extreme_size_variation:\n",
    "                self.results['yolo_detection_challenges']['size_variation_issues']['extreme'] += 1\n",
    "            elif size_cv > self.thresholds.high_size_variation:\n",
    "                self.results['yolo_detection_challenges']['size_variation_issues']['high'] += 1\n",
    "            elif size_cv > self.thresholds.moderate_size_variation:\n",
    "                self.results['yolo_detection_challenges']['size_variation_issues']['moderate'] += 1\n",
    "            else:\n",
    "                self.results['yolo_detection_challenges']['size_variation_issues']['low'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Size variation assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_occlusion_problems(self, flower_positions: List):\n",
    "        \"\"\"Assess occlusion challenges\"\"\"\n",
    "        try:\n",
    "            positions = np.array(flower_positions)\n",
    "            min_distance = float('inf')\n",
    "            for i in range(len(positions)):\n",
    "                for j in range(i+1, len(positions)):\n",
    "                    min_distance = min(min_distance, np.linalg.norm(positions[i] - positions[j]))\n",
    "            \n",
    "            if min_distance < 0.1:\n",
    "                self.results['yolo_detection_challenges']['occlusion_problems']['severe'] += 1\n",
    "            elif min_distance < 0.2:\n",
    "                self.results['yolo_detection_challenges']['occlusion_problems']['moderate'] += 1\n",
    "            else:\n",
    "                self.results['yolo_detection_challenges']['occlusion_problems']['minimal'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Occlusion assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_boundary_difficulty(self, flower_regions: List):\n",
    "        \"\"\"Assess boundary definition difficulty\"\"\"\n",
    "        try:\n",
    "            boundary_difficulties = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 100:\n",
    "                    gray_flower = cv2.cvtColor((flower * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "                    boundary_clarity = np.sum(cv2.Canny(gray_flower, 50, 150)) / gray_flower.size\n",
    "                    boundary_difficulties.append(1 / (boundary_clarity + 1e-6))\n",
    "            \n",
    "            if boundary_difficulties:\n",
    "                self.results['yolo_detection_challenges']['boundary_definition_difficulty'].append(np.mean(boundary_difficulties))\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Boundary difficulty assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_environmental_challenges(self, overall_brightness: float, overall_contrast: float,\n",
    "                                       brightness_variance: float, flower_data: Dict):\n",
    "        \"\"\"Assess environmental challenges\"\"\"\n",
    "        try:\n",
    "            self._assess_lighting_variations(brightness_variance)\n",
    "            self._assess_weather_effects(overall_brightness, overall_contrast)\n",
    "            self._assess_temporal_consistency(flower_data['flower_regions'])\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Environmental challenge assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_lighting_variations(self, brightness_variance: float):\n",
    "        \"\"\"Assess lighting variation challenges\"\"\"\n",
    "        brightness_uniformity = 1 / (brightness_variance + 1e-6)\n",
    "        \n",
    "        if brightness_uniformity < 10:\n",
    "            self.results['environmental_challenges']['lighting_variations']['extreme'] += 1\n",
    "        elif brightness_uniformity < 50:\n",
    "            self.results['environmental_challenges']['lighting_variations']['high'] += 1\n",
    "        elif brightness_uniformity < 200:\n",
    "            self.results['environmental_challenges']['lighting_variations']['moderate'] += 1\n",
    "        else:\n",
    "            self.results['environmental_challenges']['lighting_variations']['stable'] += 1\n",
    "    \n",
    "    def _assess_weather_effects(self, overall_brightness: float, overall_contrast: float):\n",
    "        \"\"\"Assess weather-related challenges\"\"\"\n",
    "        weather_stress_indicator = abs(overall_brightness - 0.5) + (1 - overall_contrast)\n",
    "        \n",
    "        if weather_stress_indicator > 0.7:\n",
    "            self.results['environmental_challenges']['weather_effects']['severe'] += 1\n",
    "        elif weather_stress_indicator > 0.4:\n",
    "            self.results['environmental_challenges']['weather_effects']['moderate'] += 1\n",
    "        else:\n",
    "            self.results['environmental_challenges']['weather_effects']['minimal'] += 1\n",
    "    \n",
    "    def _assess_temporal_consistency(self, flower_regions: List):\n",
    "        \"\"\"Assess temporal consistency challenges\"\"\"\n",
    "        try:\n",
    "            if not flower_regions:\n",
    "                return\n",
    "            \n",
    "            color_consistencies = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 50:\n",
    "                    hsv_flower = color.rgb2hsv(flower.reshape(-1, 1, 3))\n",
    "                    color_consistencies.append(np.std(hsv_flower[:, 0, 0]) + np.std(hsv_flower[:, 0, 1]))\n",
    "            \n",
    "            if color_consistencies:\n",
    "                temporal_consistency = 1 / (np.mean(color_consistencies) + 1e-6)\n",
    "                self.results['environmental_challenges']['temporal_consistency_issues'].append(temporal_consistency)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Temporal consistency assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_agricultural_challenges(self, flower_data: Dict):\n",
    "        \"\"\"Assess agriculture-specific challenges\"\"\"\n",
    "        try:\n",
    "            flower_areas, flower_regions = flower_data['flower_areas'], flower_data['flower_regions']\n",
    "            \n",
    "            if flower_areas:\n",
    "                self._assess_crop_stage_variations(flower_areas)\n",
    "                size_diversity = np.std(flower_areas) / (np.mean(flower_areas) + 1e-6)\n",
    "                self.results['agricultural_specific_challenges']['harvest_timing_complexity'].append(size_diversity)\n",
    "            if len(flower_regions) > 1:\n",
    "                self._assess_pollination_confusion(flower_regions)\n",
    "            if flower_regions:\n",
    "                self._assess_disease_detection_difficulty(flower_regions)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Agricultural challenge assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_crop_stage_variations(self, flower_areas: List):\n",
    "        \"\"\"Assess crop stage variation challenges\"\"\"\n",
    "        stage_bins = [0.01, 0.05, 0.15, 1.0]\n",
    "        stage_diversity = len(set(np.digitize(flower_areas, bins=stage_bins)))\n",
    "        \n",
    "        if stage_diversity >= 3:\n",
    "            self.results['agricultural_specific_challenges']['crop_stage_variations']['multiple_stages'] += 1\n",
    "        elif stage_diversity == 2:\n",
    "            self.results['agricultural_specific_challenges']['crop_stage_variations']['few_stages'] += 1\n",
    "        else:\n",
    "            self.results['agricultural_specific_challenges']['crop_stage_variations']['uniform'] += 1\n",
    "    \n",
    "    def _assess_pollination_confusion(self, flower_regions: List):\n",
    "        \"\"\"Assess pollination state confusion risk\"\"\"\n",
    "        try:\n",
    "            center_brightness_vars = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 100:\n",
    "                    fh, fw = flower.shape[:2]\n",
    "                    center_region = flower[fh//3:2*fh//3, fw//3:2*fw//3]\n",
    "                    if center_region.size > 10:\n",
    "                        center_hsv = color.rgb2hsv(center_region.reshape(-1, 1, 3))\n",
    "                        center_brightness_vars.append(np.var(center_hsv[:, 0, 2]))\n",
    "            \n",
    "            if center_brightness_vars:\n",
    "                avg_center_var = np.mean(center_brightness_vars)\n",
    "                if avg_center_var < 0.01:\n",
    "                    self.results['agricultural_specific_challenges']['pollination_state_confusion']['high_risk'] += 1\n",
    "                elif avg_center_var < 0.05:\n",
    "                    self.results['agricultural_specific_challenges']['pollination_state_confusion']['medium_risk'] += 1\n",
    "                else:\n",
    "                    self.results['agricultural_specific_challenges']['pollination_state_confusion']['low_risk'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Pollination confusion assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_disease_detection_difficulty(self, flower_regions: List):\n",
    "        \"\"\"Assess disease detection difficulty\"\"\"\n",
    "        try:\n",
    "            disease_detection_scores = []\n",
    "            for flower in flower_regions:\n",
    "                if flower.size > 100:\n",
    "                    hsv_flower = color.rgb2hsv(flower.reshape(-1, 1, 3))\n",
    "                    disease_detection_scores.append(1 / (np.var(hsv_flower[:, 0, :]) + 1e-6))\n",
    "            \n",
    "            if disease_detection_scores:\n",
    "                avg_disease_detection = np.mean(disease_detection_scores)\n",
    "                if avg_disease_detection < 10:\n",
    "                    self.results['agricultural_specific_challenges']['disease_detection_difficulty']['very_difficult'] += 1\n",
    "                elif avg_disease_detection < 50:\n",
    "                    self.results['agricultural_specific_challenges']['disease_detection_difficulty']['difficult'] += 1\n",
    "                else:\n",
    "                    self.results['agricultural_specific_challenges']['disease_detection_difficulty']['manageable'] += 1\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Disease detection assessment failed: {e}\")\n",
    "    \n",
    "    def _assess_overall_complexity(self):\n",
    "        \"\"\"Assess overall complexity metrics\"\"\"\n",
    "        try:\n",
    "            cbam_complexity = self._calculate_cbam_complexity()\n",
    "            stn_complexity = self._calculate_stn_complexity()\n",
    "            tps_complexity = self._calculate_tps_complexity()\n",
    "            yolo_complexity = self._calculate_yolo_complexity()\n",
    "            \n",
    "            total_complexity = cbam_complexity + stn_complexity + tps_complexity + yolo_complexity\n",
    "            \n",
    "            self._classify_detection_difficulty(total_complexity)\n",
    "            self._classify_training_complexity(total_complexity)\n",
    "            self._classify_deployment_feasibility()\n",
    "            \n",
    "            expected_performance = max(0.3, 1.0 - (total_complexity / 20.0))\n",
    "            self.results['overall_complexity_assessment']['performance_expectations'].append(expected_performance)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Overall complexity assessment failed: {e}\")\n",
    "    \n",
    "    def _calculate_cbam_complexity(self) -> int:\n",
    "        \"\"\"Calculate CBAM-specific complexity score\"\"\"\n",
    "        severe_bg = self.results['cbam_attention_challenges']['background_similarity']['severe']\n",
    "        extreme_scale = self.results['cbam_attention_challenges']['multi_scale_complexity']['extreme']\n",
    "        high_attention = len([r for r in self.results['cbam_attention_challenges']['spatial_attention_requirements'] if r > 0.8])\n",
    "        return severe_bg + extreme_scale + min(high_attention, 3)\n",
    "    \n",
    "    def _calculate_stn_complexity(self) -> int:\n",
    "        \"\"\"Calculate STN-specific complexity score\"\"\"\n",
    "        return (self.results['stn_transformation_challenges']['orientation_variations']['high'] +\n",
    "                self.results['stn_transformation_challenges']['perspective_distortions']['severe'])\n",
    "    \n",
    "    def _calculate_tps_complexity(self) -> int:\n",
    "        \"\"\"Calculate TPS-specific complexity score\"\"\"\n",
    "        return (self.results['tps_deformation_challenges']['non_rigid_deformations']['complex'] +\n",
    "                self.results['tps_deformation_challenges']['petal_shape_variations']['extreme'])\n",
    "    \n",
    "    def _calculate_yolo_complexity(self) -> int:\n",
    "        \"\"\"Calculate YOLO-specific complexity score\"\"\"\n",
    "        return (self.results['yolo_detection_challenges']['object_density']['very_high'] +\n",
    "                self.results['yolo_detection_challenges']['size_variation_issues']['extreme'] +\n",
    "                self.results['yolo_detection_challenges']['occlusion_problems']['severe'])\n",
    "    \n",
    "    def _classify_detection_difficulty(self, total_complexity: int):\n",
    "        \"\"\"Classify overall detection difficulty\"\"\"\n",
    "        if total_complexity >= 8:\n",
    "            self.results['overall_complexity_assessment']['detection_difficulty']['very_hard'] += 1\n",
    "        elif total_complexity >= 5:\n",
    "            self.results['overall_complexity_assessment']['detection_difficulty']['hard'] += 1\n",
    "        elif total_complexity >= 2:\n",
    "            self.results['overall_complexity_assessment']['detection_difficulty']['moderate'] += 1\n",
    "        else:\n",
    "            self.results['overall_complexity_assessment']['detection_difficulty']['easy'] += 1\n",
    "    \n",
    "    def _classify_training_complexity(self, total_complexity: int):\n",
    "        \"\"\"Classify training complexity\"\"\"\n",
    "        environmental_complexity = (self.results['environmental_challenges']['lighting_variations']['extreme'] +\n",
    "                                   self.results['environmental_challenges']['weather_effects']['severe'])\n",
    "        agricultural_complexity = (self.results['agricultural_specific_challenges']['crop_stage_variations']['multiple_stages'] +\n",
    "                                 self.results['agricultural_specific_challenges']['disease_detection_difficulty']['very_difficult'])\n",
    "        \n",
    "        training_complexity_score = total_complexity + environmental_complexity + agricultural_complexity\n",
    "        \n",
    "        if training_complexity_score >= 12:\n",
    "            self.results['overall_complexity_assessment']['training_complexity']['very_complex'] += 1\n",
    "        elif training_complexity_score >= 8:\n",
    "            self.results['overall_complexity_assessment']['training_complexity']['complex'] += 1\n",
    "        elif training_complexity_score >= 4:\n",
    "            self.results['overall_complexity_assessment']['training_complexity']['moderate'] += 1\n",
    "        else:\n",
    "            self.results['overall_complexity_assessment']['training_complexity']['simple'] += 1\n",
    "    \n",
    "    def _classify_deployment_feasibility(self):\n",
    "        \"\"\"Classify deployment feasibility\"\"\"\n",
    "        deployment_challenges = (self.results['environmental_challenges']['lighting_variations']['extreme'] +\n",
    "                               self.results['yolo_detection_challenges']['object_density']['very_high'] +\n",
    "                               self.results['agricultural_specific_challenges']['disease_detection_difficulty']['very_difficult'])\n",
    "        \n",
    "        if deployment_challenges >= 3:\n",
    "            self.results['overall_complexity_assessment']['deployment_feasibility']['challenging'] += 1\n",
    "        elif deployment_challenges >= 1:\n",
    "            self.results['overall_complexity_assessment']['deployment_feasibility']['moderate'] += 1\n",
    "        else:\n",
    "            self.results['overall_complexity_assessment']['deployment_feasibility']['feasible'] += 1\n",
    "    \n",
    "    def _calculate_challenge_scores(self) -> Dict:\n",
    "        \"\"\"Calculate comprehensive challenge scores\"\"\"\n",
    "        return {\n",
    "            'cbam_score': self._calculate_cbam_complexity(),\n",
    "            'stn_score': self._calculate_stn_complexity(),\n",
    "            'tps_score': self._calculate_tps_complexity(),\n",
    "            'yolo_score': self._calculate_yolo_complexity()\n",
    "        }\n",
    "\n",
    "def assess_flower_detection_challenges_comprehensive(dataset, dataset_name: str, max_samples: int = 300) -> Dict:\n",
    "    \"\"\"Enhanced challenge assessment with batch processing\"\"\"\n",
    "    if not dataset or len(dataset) == 0:\n",
    "        logger.error(f\"Invalid dataset for challenge assessment: {dataset_name}\")\n",
    "        return {}\n",
    "    \n",
    "    logger.info(f\"Assessing comprehensive challenges in {dataset_name}\")\n",
    "    analyzer = ChallengeAssessmentAnalyzer()\n",
    "    \n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    batch_size = config.max_batch_size\n",
    "    processed_count = 0\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i + batch_size]\n",
    "        batch_data = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                image, targets, path = dataset[idx]\n",
    "                batch_data.append((idx, image, targets, path))\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to load image {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_data:\n",
    "            batch_results = analyzer.process_image_batch(batch_data)\n",
    "            processed_count += len([r for r in batch_results if r.get('processed', False)])\n",
    "        \n",
    "        if (i // batch_size) % 3 == 0:\n",
    "            MemoryManager.clear_memory()\n",
    "    \n",
    "    analyzer.results['metadata'] = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'images_processed': processed_count,\n",
    "        'total_sampled': sample_size,\n",
    "        'processing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Challenge assessment completed for {dataset_name}: {processed_count} images processed\")\n",
    "    return analyzer.results\n",
    "\n",
    "def generate_cbam_stn_tps_yolo_optimization_recommendations(challenge_results: Dict) -> Dict:\n",
    "    \"\"\"Generate specific optimization recommendations for CBAM-STN-TPS-YOLO\"\"\"\n",
    "    recommendations = {'cbam_optimizations': {}, 'stn_optimizations': {}, 'tps_optimizations': {}, 'yolo_optimizations': {}}\n",
    "    \n",
    "    for dataset_name, metrics in challenge_results.items():\n",
    "        # CBAM Optimizations\n",
    "        cbam_data = metrics['cbam_attention_challenges']\n",
    "        cbam_recs = []\n",
    "        \n",
    "        if cbam_data['background_similarity']['severe'] > 0:\n",
    "            cbam_recs.extend([\n",
    "                \"Implement enhanced spatial attention with background suppression\",\n",
    "                \"Use channel attention for color-space feature discrimination\",\n",
    "                \"Add contrastive learning for flower-background separation\"\n",
    "            ])\n",
    "        \n",
    "        if cbam_data['multi_scale_complexity']['extreme'] > 0:\n",
    "            cbam_recs.extend([\n",
    "                \"Deploy multi-scale attention pyramids\",\n",
    "                \"Implement scale-adaptive attention weights\",\n",
    "                \"Use hierarchical attention mechanisms\"\n",
    "            ])\n",
    "        \n",
    "        recommendations['cbam_optimizations'][dataset_name] = cbam_recs\n",
    "        \n",
    "        # STN Optimizations\n",
    "        stn_data = metrics['stn_transformation_challenges']\n",
    "        stn_recs = []\n",
    "        \n",
    "        if stn_data['orientation_variations']['high'] > 0:\n",
    "            stn_recs.extend([\n",
    "                \"Implement rotation-robust localization networks\",\n",
    "                \"Use multi-resolution transformation estimation\",\n",
    "                \"Add orientation-specific data augmentation\"\n",
    "            ])\n",
    "        \n",
    "        if stn_data['perspective_distortions']['severe'] > 0:\n",
    "            stn_recs.extend([\n",
    "                \"Deploy advanced affine transformation models\",\n",
    "                \"Implement perspective-aware grid sampling\",\n",
    "                \"Use camera calibration integration\"\n",
    "            ])\n",
    "        \n",
    "        recommendations['stn_optimizations'][dataset_name] = stn_recs\n",
    "        \n",
    "        # TPS Optimizations\n",
    "        tps_data = metrics['tps_deformation_challenges']\n",
    "        tps_recs = []\n",
    "        \n",
    "        if tps_data['non_rigid_deformations']['complex'] > 0:\n",
    "            tps_recs.extend([\n",
    "                \"Implement adaptive control point selection\",\n",
    "                \"Use deformation-aware regularization\",\n",
    "                \"Deploy multi-level TPS transformations\"\n",
    "            ])\n",
    "        \n",
    "        if tps_data['petal_shape_variations']['extreme'] > 0:\n",
    "            tps_recs.extend([\n",
    "                \"Implement petal-specific shape models\",\n",
    "                \"Use botanical shape priors\",\n",
    "                \"Add temporal shape consistency constraints\"\n",
    "            ])\n",
    "        \n",
    "        recommendations['tps_optimizations'][dataset_name] = tps_recs\n",
    "        \n",
    "        # YOLO Optimizations\n",
    "        yolo_data = metrics['yolo_detection_challenges']\n",
    "        yolo_recs = []\n",
    "        \n",
    "        if yolo_data['object_density']['very_high'] > 0:\n",
    "            yolo_recs.extend([\n",
    "                \"Implement dense prediction with NMS optimization\",\n",
    "                \"Use multi-scale feature pyramid networks\",\n",
    "                \"Deploy crowd-aware loss functions\"\n",
    "            ])\n",
    "        \n",
    "        if yolo_data['size_variation_issues']['extreme'] > 0:\n",
    "            yolo_recs.extend([\n",
    "                \"Implement scale-adaptive anchor generation\",\n",
    "                \"Use feature pyramid network with more scales\",\n",
    "                \"Deploy size-aware confidence scoring\"\n",
    "            ])\n",
    "        \n",
    "        recommendations['yolo_optimizations'][dataset_name] = yolo_recs\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def create_comprehensive_challenge_visualization(challenge_assessment_results: Dict):\n",
    "    \"\"\"Create memory-efficient challenge assessment visualization\"\"\"\n",
    "    if not challenge_assessment_results:\n",
    "        logger.error(\"No challenge assessment results for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
    "        \n",
    "        for idx, (dataset_name, metrics) in enumerate(challenge_assessment_results.items()):\n",
    "            if idx >= 1:\n",
    "                break\n",
    "            create_challenge_plots(fig, gs, dataset_name, metrics)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = notebook_results_dir / 'challenges' / 'comprehensive_challenge_assessment.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', optimize=True)\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        MemoryManager.clear_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Challenge visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "\n",
    "def create_challenge_plots(fig, gs, dataset_name: str, metrics: Dict):\n",
    "    \"\"\"Create individual challenge assessment plots\"\"\"\n",
    "    try:\n",
    "        # CBAM Background Similarity\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        bg_sim = metrics['cbam_attention_challenges']['background_similarity']\n",
    "        if any(bg_sim.values()):\n",
    "            ax1.pie(list(bg_sim.values()), labels=[k.title() for k in bg_sim.keys()], \n",
    "                   autopct='%1.1f%%', colors=['red', 'orange', 'green'])\n",
    "            ax1.set_title('CBAM: Background Similarity', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # STN Orientation Variations\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        orientation_data = metrics['stn_transformation_challenges']['orientation_variations']\n",
    "        if any(orientation_data.values()):\n",
    "            bars = ax2.bar(list(orientation_data.keys()), list(orientation_data.values()), \n",
    "                          color=['red', 'yellow', 'green'], alpha=0.8)\n",
    "            ax2.set_title('STN: Orientation Variations', fontweight='bold', fontsize=11)\n",
    "            ax2.set_ylabel('Count')\n",
    "            \n",
    "            for bar, value in zip(bars, orientation_data.values()):\n",
    "                if value > 0:\n",
    "                    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(orientation_data.values())*0.01,\n",
    "                           f'{value}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # TPS Deformation Complexity\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        deform_data = metrics['tps_deformation_challenges']['non_rigid_deformations']\n",
    "        if any(deform_data.values()):\n",
    "            ax3.bar(range(len(deform_data)), list(deform_data.values()),\n",
    "                   color=sns.color_palette(\"Reds\", len(deform_data)), alpha=0.8)\n",
    "            ax3.set_title('TPS: Non-Rigid Deformations', fontweight='bold', fontsize=11)\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.set_xticks(range(len(deform_data)))\n",
    "            ax3.set_xticklabels([k.title() for k in deform_data.keys()])\n",
    "        \n",
    "        # YOLO Object Density\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        density_data = metrics['yolo_detection_challenges']['object_density']\n",
    "        if any(density_data.values()):\n",
    "            ax4.pie(list(density_data.values()), \n",
    "                   labels=[k.replace('_', '\\n').title() for k in density_data.keys()],\n",
    "                   autopct='%1.1f%%', colors=sns.color_palette(\"Blues\", len(density_data)))\n",
    "            ax4.set_title('YOLO: Object Density', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # Component Stress Analysis\n",
    "        ax5 = fig.add_subplot(gs[1, 0:2])\n",
    "        cbam_stress = bg_sim['severe'] + metrics['cbam_attention_challenges']['multi_scale_complexity']['extreme']\n",
    "        stn_stress = orientation_data['high'] + metrics['stn_transformation_challenges']['perspective_distortions']['severe']\n",
    "        tps_stress = deform_data['complex'] + metrics['tps_deformation_challenges']['petal_shape_variations']['extreme']\n",
    "        yolo_stress = density_data['very_high'] + metrics['yolo_detection_challenges']['size_variation_issues']['extreme']\n",
    "        \n",
    "        component_stress = {'CBAM\\nAttention': cbam_stress, 'STN\\nTransform': stn_stress, \n",
    "                          'TPS\\nDeformation': tps_stress, 'YOLO\\nDetection': yolo_stress}\n",
    "        \n",
    "        bars = ax5.bar(range(len(component_stress)), list(component_stress.values()),\n",
    "                      color=['red', 'blue', 'green', 'orange'], alpha=0.8)\n",
    "        ax5.set_title(f'Component Stress Analysis - {dataset_name}', fontweight='bold', fontsize=12)\n",
    "        ax5.set_ylabel('Stress Level')\n",
    "        ax5.set_xticks(range(len(component_stress)))\n",
    "        ax5.set_xticklabels(list(component_stress.keys()))\n",
    "        \n",
    "        for bar, value in zip(bars, component_stress.values()):\n",
    "            stress_level = 'HIGH' if value > 2 else 'MEDIUM' if value > 1 else 'LOW'\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(component_stress.values())*0.05,\n",
    "                   stress_level, ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Overall Assessment\n",
    "        ax6 = fig.add_subplot(gs[1, 2:4])\n",
    "        difficulty_data = metrics['overall_complexity_assessment']['detection_difficulty']\n",
    "        if any(difficulty_data.values()):\n",
    "            ax6.bar(range(len(difficulty_data)), list(difficulty_data.values()),\n",
    "                   color=['darkred', 'red', 'orange', 'green'], alpha=0.8)\n",
    "            ax6.set_title(f'Detection Difficulty - {dataset_name}', fontweight='bold', fontsize=12)\n",
    "            ax6.set_ylabel('Count')\n",
    "            ax6.set_xticks(range(len(difficulty_data)))\n",
    "            ax6.set_xticklabels([k.replace('_', '\\n').title() for k in difficulty_data.keys()])\n",
    "        \n",
    "        # Comprehensive Summary\n",
    "        ax7 = fig.add_subplot(gs[2, :])\n",
    "        total_severe_challenges = cbam_stress + stn_stress + tps_stress + yolo_stress\n",
    "        performance_expectations = metrics['overall_complexity_assessment']['performance_expectations']\n",
    "        avg_performance = np.mean(performance_expectations) if performance_expectations else 0\n",
    "        \n",
    "        summary_text = f\"\"\"Challenge Assessment Summary - {dataset_name}\n",
    "\n",
    "Component Stress Levels:\n",
    "  CBAM Attention: {'HIGH' if cbam_stress > 2 else 'MEDIUM' if cbam_stress > 0 else 'LOW'}\n",
    "  STN Transform: {'HIGH' if stn_stress > 2 else 'MEDIUM' if stn_stress > 0 else 'LOW'}\n",
    "  TPS Deformation: {'HIGH' if tps_stress > 2 else 'MEDIUM' if tps_stress > 0 else 'LOW'}\n",
    "  YOLO Detection: {'HIGH' if yolo_stress > 2 else 'MEDIUM' if yolo_stress > 0 else 'LOW'}\n",
    "\n",
    "Overall Assessment:\n",
    "  Total Severe Challenges: {total_severe_challenges}\n",
    "  Expected Performance: {avg_performance:.1%}\n",
    "  Detection Difficulty: {max(difficulty_data, key=difficulty_data.get).replace('_', ' ').title()}\n",
    "\n",
    "Critical Issues:\n",
    "{' Background similarity problems' if bg_sim['severe'] > 0 else ''}\n",
    "{' High orientation variations' if orientation_data['high'] > 0 else ''}\n",
    "{' Complex deformations' if deform_data['complex'] > 0 else ''}\n",
    "{' Very high object density' if density_data['very_high'] > 0 else ''}\n",
    "\n",
    "Recommended Focus Areas:\n",
    " {'Enhanced CBAM attention mechanisms' if cbam_stress > 1 else 'Standard CBAM implementation'}\n",
    " {'Advanced STN transformations' if stn_stress > 1 else 'Basic STN preprocessing'}\n",
    " {'Complex TPS deformation modeling' if tps_stress > 1 else 'Simple TPS handling'}\n",
    " {'Multi-scale YOLO optimization' if yolo_stress > 1 else 'Standard YOLO training'}\n",
    "\n",
    "Deployment Readiness: {max(metrics['overall_complexity_assessment']['deployment_feasibility'], key=metrics['overall_complexity_assessment']['deployment_feasibility'].get).title()}\"\"\"\n",
    "        \n",
    "        ax7.text(0.05, 0.95, summary_text, transform=ax7.transAxes,\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "        ax7.set_title('Comprehensive Challenge Summary', fontweight='bold', fontsize=12)\n",
    "        ax7.axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create challenge plots: {e}\")\n",
    "\n",
    "def execute_challenge_assessment():\n",
    "    \"\"\"Execute the complete challenge assessment pipeline\"\"\"\n",
    "    if 'datasets' not in locals() or not datasets:\n",
    "        logger.error(\"No datasets available for challenge assessment\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Starting comprehensive flower detection challenge assessment\")\n",
    "    challenge_assessment_results = {}\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        logger.info(f\"Assessing challenges in {name}\")\n",
    "        \n",
    "        try:\n",
    "            challenge_metrics = safe_operation(\n",
    "                f\"Challenge assessment for {name}\",\n",
    "                assess_flower_detection_challenges_comprehensive,\n",
    "                dataset, name, 300\n",
    "            )\n",
    "            \n",
    "            if challenge_metrics and 'cbam_attention_challenges' in challenge_metrics:\n",
    "                challenge_assessment_results[name] = challenge_metrics\n",
    "                \n",
    "                cbam_data = challenge_metrics['cbam_attention_challenges']\n",
    "                overall_data = challenge_metrics['overall_complexity_assessment']\n",
    "                \n",
    "                logger.info(f\"Challenge assessment complete for {name}:\")\n",
    "                logger.info(f\"  Background similarity - Severe: {cbam_data['background_similarity']['severe']}\")\n",
    "                logger.info(f\"  Most common difficulty: {max(overall_data['detection_difficulty'], key=overall_data['detection_difficulty'].get)}\")\n",
    "                \n",
    "                if overall_data['performance_expectations']:\n",
    "                    logger.info(f\"  Expected performance: {np.mean(overall_data['performance_expectations']):.1%}\")\n",
    "            else:\n",
    "                logger.warning(f\"Challenge assessment failed or returned empty results for {name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to assess challenges in {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if challenge_assessment_results:\n",
    "        safe_operation(\n",
    "            \"Creating challenge assessment visualization\",\n",
    "            create_comprehensive_challenge_visualization,\n",
    "            challenge_assessment_results\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            optimization_recommendations = generate_cbam_stn_tps_yolo_optimization_recommendations(challenge_assessment_results)\n",
    "            save_challenge_assessment_results(challenge_assessment_results, optimization_recommendations)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate optimization recommendations: {e}\")\n",
    "    \n",
    "    return challenge_assessment_results\n",
    "\n",
    "def save_challenge_assessment_results(results: Dict, recommendations: Dict):\n",
    "    \"\"\"Save challenge assessment results with proper serialization\"\"\"\n",
    "    serializable_results = {}\n",
    "    \n",
    "    for dataset_name, metrics in results.items():\n",
    "        serializable_results[dataset_name] = {\n",
    "            'cbam_challenges': {\n",
    "                'background_similarity': metrics['cbam_attention_challenges']['background_similarity'],\n",
    "                'multi_scale_complexity': metrics['cbam_attention_challenges']['multi_scale_complexity'],\n",
    "                'channel_discrimination': metrics['cbam_attention_challenges']['channel_discrimination'],\n",
    "                'avg_attention_requirement': float(np.mean(metrics['cbam_attention_challenges']['spatial_attention_requirements'])) if metrics['cbam_attention_challenges']['spatial_attention_requirements'] else 0\n",
    "            },\n",
    "            'stn_challenges': {\n",
    "                'orientation_variations': metrics['stn_transformation_challenges']['orientation_variations'],\n",
    "                'perspective_distortions': metrics['stn_transformation_challenges']['perspective_distortions'],\n",
    "                'avg_geometric_complexity': float(np.mean(metrics['stn_transformation_challenges']['geometric_normalization_needs'])) if metrics['stn_transformation_challenges']['geometric_normalization_needs'] else 0\n",
    "            },\n",
    "            'tps_challenges': {\n",
    "                'non_rigid_deformations': metrics['tps_deformation_challenges']['non_rigid_deformations'],\n",
    "                'petal_shape_variations': metrics['tps_deformation_challenges']['petal_shape_variations'],\n",
    "                'wind_deformation_effects': metrics['tps_deformation_challenges']['wind_deformation_effects'],\n",
    "                'avg_growth_complexity': float(np.mean(metrics['tps_deformation_challenges']['growth_pattern_complexity'])) if metrics['tps_deformation_challenges']['growth_pattern_complexity'] else 0\n",
    "            },\n",
    "            'yolo_challenges': {\n",
    "                'object_density': metrics['yolo_detection_challenges']['object_density'],\n",
    "                'size_variation_issues': metrics['yolo_detection_challenges']['size_variation_issues'],\n",
    "                'occlusion_problems': metrics['yolo_detection_challenges']['occlusion_problems'],\n",
    "                'avg_boundary_difficulty': float(np.mean(metrics['yolo_detection_challenges']['boundary_definition_difficulty'])) if metrics['yolo_detection_challenges']['boundary_definition_difficulty'] else 0\n",
    "            },\n",
    "            'environmental_challenges': {\n",
    "                'lighting_variations': metrics['environmental_challenges']['lighting_variations'],\n",
    "                'weather_effects': metrics['environmental_challenges']['weather_effects'],\n",
    "                'seasonal_changes': metrics['environmental_challenges']['seasonal_changes']\n",
    "            },\n",
    "            'agricultural_challenges': {\n",
    "                'crop_stage_variations': metrics['agricultural_specific_challenges']['crop_stage_variations'],\n",
    "                'pollination_state_confusion': metrics['agricultural_specific_challenges']['pollination_state_confusion'],\n",
    "                'disease_detection_difficulty': metrics['agricultural_specific_challenges']['disease_detection_difficulty']\n",
    "            },\n",
    "            'overall_assessment': {\n",
    "                'detection_difficulty': metrics['overall_complexity_assessment']['detection_difficulty'],\n",
    "                'training_complexity': metrics['overall_complexity_assessment']['training_complexity'],\n",
    "                'deployment_feasibility': metrics['overall_complexity_assessment']['deployment_feasibility'],\n",
    "                'avg_performance_expectation': float(np.mean(metrics['overall_complexity_assessment']['performance_expectations'])) if metrics['overall_complexity_assessment']['performance_expectations'] else 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    comprehensive_assessment = {\n",
    "        'challenge_metrics': serializable_results,\n",
    "        'optimization_recommendations': recommendations,\n",
    "        'summary_insights': {\n",
    "            'most_challenging_aspects': [],\n",
    "            'recommended_focus_areas': [],\n",
    "            'expected_performance_range': {},\n",
    "            'deployment_readiness': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_path = notebook_results_dir / 'challenges' / 'comprehensive_challenge_assessment.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(comprehensive_assessment, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Comprehensive challenge assessment saved to {output_path}\")\n",
    "    \n",
    "    if recommendations:\n",
    "        logger.info(\"CBAM-STN-TPS-YOLO Optimization Recommendations Summary:\")\n",
    "        for dataset_name in results.keys():\n",
    "            logger.info(f\"  {dataset_name}:\")\n",
    "            \n",
    "            for component in ['cbam_optimizations', 'stn_optimizations', 'tps_optimizations', 'yolo_optimizations']:\n",
    "                if dataset_name in recommendations.get(component, {}) and recommendations[component][dataset_name]:\n",
    "                    component_name = component.split('_')[0].upper()\n",
    "                    logger.info(f\"    {component_name}: {len(recommendations[component][dataset_name])} recommendations\")\n",
    "                    for rec in recommendations[component][dataset_name][:2]:\n",
    "                        logger.info(f\"       {rec}\")\n",
    "\n",
    "# Execute the analysis\n",
    "challenge_assessment_results = execute_challenge_assessment()\n",
    "\n",
    "if not challenge_assessment_results:\n",
    "    logger.error(\"Challenge assessment failed completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6c751",
   "metadata": {},
   "source": [
    "##  Comprehensive MelonFlower Dataset Analysis Summary\n",
    "\n",
    "## Executive Overview\n",
    "\n",
    "This document presents a comprehensive analysis of the MelonFlower dataset with enhanced CBAM-STN-TPS-YOLO insights for agricultural flower detection and health assessment applications.\n",
    "\n",
    "---\n",
    "\n",
    "##  Analysis Metadata\n",
    "\n",
    "- **Timestamp**: Generated with enhanced analysis pipeline\n",
    "- **Analysis Version**: 2.0_enhanced\n",
    "- **Dataset Focus**: MelonFlower Agricultural Detection and Health Assessment\n",
    "- **Domain Specialization**: Agricultural Flower Detection with Pollination Monitoring\n",
    "- **Framework Compatibility**: CBAM-STN-TPS-YOLO Optimized\n",
    "\n",
    "---\n",
    "\n",
    "##  Dataset Overview\n",
    "\n",
    "### Key Statistics\n",
    "- **Total Datasets**: Multiple dataset splits analyzed\n",
    "- **Total Images**: Comprehensive image collection processed\n",
    "- **Dataset Splits**: Training, validation, and test sets\n",
    "- **Primary Domain**: Agricultural flower detection and monitoring\n",
    "- **Application Scope**: Crop pollination assessment and health monitoring\n",
    "\n",
    "### Dataset Characteristics\n",
    "- **Flower Distribution Analysis**: Spatial entropy and clustering coefficient analysis\n",
    "- **Color Analysis**: HSV distribution analysis with dominant color clustering\n",
    "- **Health Assessment**: Multi-stage health indicator evaluation\n",
    "- **Pollination Monitoring**: Reproductive structure analysis and pollination state classification\n",
    "\n",
    "---\n",
    "\n",
    "##  CBAM-STN-TPS-YOLO Architecture Optimization\n",
    "\n",
    "### Attention Mechanism Benefits\n",
    "\n",
    "#### CBAM Spatial Attention\n",
    "- **Flower Background Discrimination**: Critical for similar color backgrounds\n",
    "- **Petal Center Focus**: Essential for pollination state detection\n",
    "- **Multi-Scale Flower Detection**: Required for size variation handling\n",
    "- **Seasonal Adaptation**: Important for temporal color changes\n",
    "\n",
    "#### CBAM Channel Attention\n",
    "- **Spectral Feature Selection**: Optimizes multispectral flower analysis\n",
    "- **Health Indicator Enhancement**: Amplifies disease and stress signatures\n",
    "- **Pollination Cue Detection**: Focuses on reproductive structure features\n",
    "\n",
    "### Spatial Transformer Benefits\n",
    "\n",
    "#### STN Geometric Normalization\n",
    "- **Flower Orientation Invariance**: Handles natural growth variations\n",
    "- **Wind Deformation Compensation**: Corrects for environmental movement\n",
    "- **Viewing Angle Normalization**: Standardizes different camera perspectives\n",
    "\n",
    "#### TPS Deformation Modeling\n",
    "- **Petal Shape Standardization**: Normalizes species and age variations\n",
    "- **Growth Stage Alignment**: Enables temporal comparison across stages\n",
    "- **Stress Deformation Correction**: Compensates for environmental stress effects\n",
    "\n",
    "### YOLO Architecture Benefits\n",
    "- **Multi-Scale Detection**: Handles flower size variations efficiently\n",
    "- **Real-Time Processing**: Enables field deployment and monitoring\n",
    "- **Dense Prediction**: Suitable for high flower density scenarios\n",
    "- **End-to-End Optimization**: Integrates all components seamlessly\n",
    "\n",
    "---\n",
    "\n",
    "##  Training Strategy Recommendations\n",
    "\n",
    "### Data Augmentation Strategies\n",
    "\n",
    "#### Flower-Specific Augmentations\n",
    "- Seasonal color shift simulation\n",
    "- Pollination stage interpolation\n",
    "- Environmental stress simulation\n",
    "- Multi-temporal consistency augmentation\n",
    "- Pollinator interaction scenarios\n",
    "\n",
    "#### Geometric Augmentations\n",
    "- Wind movement simulation\n",
    "- Growth orientation variations\n",
    "- Camera perspective changes\n",
    "- Flower density modifications\n",
    "\n",
    "#### Photometric Augmentations\n",
    "- Diurnal lighting variations\n",
    "- Seasonal illumination changes\n",
    "- Weather condition simulation\n",
    "- Shadow and occlusion effects\n",
    "\n",
    "### Loss Function Design\n",
    "\n",
    "#### Multi-Task Objectives\n",
    "- **Flower Detection Loss**: Primary object detection objective\n",
    "- **Pollination State Classification Loss**: Reproductive stage classification\n",
    "- **Health Assessment Regression Loss**: Continuous health scoring\n",
    "- **Temporal Consistency Loss**: Cross-frame consistency maintenance\n",
    "\n",
    "#### Attention-Guided Losses\n",
    "- CBAM attention supervision\n",
    "- Spatial transformer regularization\n",
    "- Feature discrimination loss\n",
    "\n",
    "### Training Schedule\n",
    "\n",
    "#### Curriculum Learning Progression\n",
    "1. Simple single flower images\n",
    "2. Moderate density scenes\n",
    "3. Complex multi-stage scenarios\n",
    "4. Challenging environmental conditions\n",
    "\n",
    "#### Progressive Complexity\n",
    "1. Basic flower detection\n",
    "2. Pollination state recognition\n",
    "3. Health assessment integration\n",
    "4. Temporal modeling activation\n",
    "\n",
    "---\n",
    "\n",
    "##  Deployment Considerations\n",
    "\n",
    "### Agricultural Deployment Scenarios\n",
    "\n",
    "#### Field Monitoring Systems\n",
    "- **Edge Device Optimization**: Lightweight model variants\n",
    "- **Real-Time Processing**: Optimized inference pipeline\n",
    "- **Environmental Robustness**: Weather-resistant deployment\n",
    "\n",
    "#### Greenhouse Monitoring\n",
    "- **Controlled Environment Advantages**: Consistent lighting and conditions\n",
    "- **High-Frequency Monitoring**: Temporal health tracking\n",
    "- **Precision Agriculture Integration**: Automated decision systems\n",
    "\n",
    "#### Research Applications\n",
    "- **Pollination Efficiency Studies**: Quantitative pollination success metrics\n",
    "- **Crop Breeding Programs**: Flower trait quantification\n",
    "- **Climate Impact Assessment**: Environmental stress monitoring\n",
    "\n",
    "### Technical Requirements\n",
    "\n",
    "#### Hardware Specifications\n",
    "- **Minimum GPU Memory**: 6GB for inference\n",
    "- **Recommended GPU Memory**: 12GB for training\n",
    "- **CPU Fallback Support**: Optimized CPU inference available\n",
    "- **Edge Device Compatibility**: Jetson and mobile deployment ready\n",
    "\n",
    "#### Software Dependencies\n",
    "- **PyTorch Version**: 1.9.0\n",
    "- **OpenCV Version**: 4.5.0\n",
    "- **Additional Libraries**: albumentations, timm, torchvision\n",
    "\n",
    "---\n",
    "\n",
    "##  Research Insights and Future Directions\n",
    "\n",
    "### Novel Contributions\n",
    "\n",
    "#### Architectural Innovations\n",
    "- Flower-specific attention mechanisms\n",
    "- Pollination-aware feature extraction\n",
    "- Temporal health modeling\n",
    "- Multi-spectral integration\n",
    "\n",
    "#### Agricultural Applications\n",
    "- Automated pollination monitoring\n",
    "- Crop health assessment\n",
    "- Yield prediction through flower analysis\n",
    "- Climate adaptation studies\n",
    "\n",
    "### Future Research Directions\n",
    "\n",
    "#### Technological Advances\n",
    "- **Hyperspectral Imaging Integration**: Enhanced spectral analysis capabilities\n",
    "- **Drone-Based Monitoring Systems**: Scalable field monitoring\n",
    "- **IoT Sensor Fusion**: Multi-modal data integration\n",
    "- **Blockchain Agriculture Integration**: Traceability and data integrity\n",
    "\n",
    "#### Biological Modeling\n",
    "- **Pollinator Behavior Prediction**: Ecosystem interaction modeling\n",
    "- **Genetic Trait Expression Analysis**: Phenotype-genotype correlation\n",
    "- **Disease Progression Modeling**: Temporal health prediction\n",
    "- **Climate Resilience Assessment**: Adaptation capacity evaluation\n",
    "\n",
    "### Scientific Impact\n",
    "\n",
    "#### Impact Areas\n",
    "- **Precision Agriculture**: Enables data-driven farming decisions\n",
    "- **Biodiversity Conservation**: Supports pollinator habitat management\n",
    "- **Climate Research**: Provides phenological data for climate studies\n",
    "- **Food Security**: Optimizes crop production and quality\n",
    "\n",
    "---\n",
    "\n",
    "##  Analysis Results Summary\n",
    "\n",
    "### Comprehensive Analysis Components\n",
    "\n",
    "####  Completed Analyses\n",
    "- **Flower Distribution Analysis**: Spatial entropy and clustering analysis\n",
    "- **Color Analysis**: HSV distribution and dominant color extraction\n",
    "- **Health Assessment**: Multi-stage health indicator evaluation\n",
    "- **Pollination Monitoring**: Reproductive structure classification\n",
    "\n",
    "####  Key Findings\n",
    "- **Spatial Entropy**: Quantified flower distribution patterns\n",
    "- **Clustering Coefficient**: Measured spatial organization\n",
    "- **Health Distribution Entropy**: Assessed health state diversity\n",
    "- **Pollination Stage Diversity**: Evaluated reproductive stage variation\n",
    "\n",
    "---\n",
    "\n",
    "##  Business Impact Assessment\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "- **Development Investment**: Moderate initial cost\n",
    "- **Training Requirements**: Specialized but achievable\n",
    "- **Deployment Efficiency**: High scalability potential\n",
    "- **Expected ROI**: Strong positive return anticipated\n",
    "\n",
    "### Implementation Timeline\n",
    "1. **Research Phase (Months 1-3)**:  Completed\n",
    "2. **Development Phase (Months 4-8)**:  In Progress\n",
    "3. **Testing Phase (Months 9-11)**:  Planned\n",
    "4. **Deployment Phase (Month 12+)**:  Target\n",
    "\n",
    "### Risk Assessment\n",
    "- **Technical Risk**: Low-Medium (proven architecture components)\n",
    "- **Market Risk**: Low (strong agricultural demand)\n",
    "- **Operational Risk**: Medium (requires specialized deployment)\n",
    "\n",
    "---\n",
    "\n",
    "##  Conclusion\n",
    "\n",
    "The comprehensive MelonFlower dataset analysis demonstrates strong potential for CBAM-STN-TPS-YOLO architecture implementation in agricultural monitoring applications. The enhanced analysis reveals:\n",
    "\n",
    "### Key Strengths\n",
    "- **Robust Dataset Foundation**: Comprehensive flower distribution and characteristics\n",
    "- **Advanced Architecture Benefits**: Proven attention and transformation mechanisms\n",
    "- **Clear Deployment Pathways**: Multiple agricultural application scenarios\n",
    "- **Strong Research Foundation**: Novel contributions and future research directions\n",
    "\n",
    "### Recommendations\n",
    "1. **Proceed with Model Development**: Strong technical and business case\n",
    "2. **Prioritize Field Testing**: Focus on real-world validation\n",
    "3. **Develop Partnership Strategy**: Engage agricultural stakeholders early\n",
    "4. **Plan Scalable Deployment**: Design for multiple deployment scenarios\n",
    "\n",
    "### Expected Outcomes\n",
    "- **Enhanced Crop Monitoring**: Automated flower health and pollination assessment\n",
    "- **Improved Agricultural Decisions**: Data-driven farming optimization\n",
    "- **Scientific Advancement**: Contribution to agricultural AI research\n",
    "- **Commercial Viability**: Strong market potential and scalability\n",
    "\n",
    "---\n",
    "\n",
    "##  Generated Outputs\n",
    "\n",
    "### File Structure\n",
    "```\n",
    "analysis_results/\n",
    " visualizations/          # Comprehensive visualization suite\n",
    " statistics/             # Statistical analysis results\n",
    " color_analysis/         # Color space analysis outputs\n",
    " health_assessment/      # Health monitoring results\n",
    " comprehensive_summary.json  # Complete analysis summary\n",
    "```\n",
    "\n",
    "### Visualization Suite\n",
    "- **Executive Summary Dashboard**: Key metrics and insights\n",
    "- **Flower Distribution Analysis**: Spatial and density visualizations\n",
    "- **Color Analysis Plots**: HSV distributions and dominant colors\n",
    "- **Health Assessment Charts**: Health indicators and trends\n",
    "- **Architecture Benefits Diagrams**: Component optimization insights\n",
    "\n",
    "---\n",
    "\n",
    "*Analysis completed with CBAM-STN-TPS-YOLO optimization framework*\n",
    "*Generated: Enhanced Analysis Pipeline v2.0*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf89f4",
   "metadata": {},
   "source": [
    "## Summary and Key Flower-Specific Findings\n",
    "\n",
    "This MelonFlower dataset exploration notebook has successfully analyzed:\n",
    "\n",
    "###  **Flower Detection Characteristics**\n",
    "- **Bloom Stage Variations**: Bud, opening, full bloom, and wilting stages\n",
    "- **Color Diversity**: Wide hue, saturation, and brightness variations\n",
    "- **Size Variations**: From tiny buds (0.005) to large mature flowers (0.15+ area)\n",
    "- **Temporal Patterns**: Early, peak, and late season flowering indicators\n",
    "\n",
    "###  **Color and Visual Analysis**\n",
    "- **HSV Color Space Analysis**: Comprehensive hue, saturation, value distributions\n",
    "- **Dominant Color Clustering**: K-means clustering for primary flower colors\n",
    "- **Seasonal Color Indicators**: Spring/summer/autumn color pattern recognition\n",
    "- **Background Similarity Assessment**: Flower-background color discrimination challenges\n",
    "\n",
    "###  **Health and Pollination Assessment**\n",
    "- **Pollination State Detection**: Unpollinated, pollinating, and pollinated flowers\n",
    "- **Health Scoring**: Comprehensive health assessment based on multiple factors\n",
    "- **Petal Condition Analysis**: Intact, partial damage, and significant damage detection\n",
    "- **Maturity Stage Classification**: Automated bloom stage identification\n",
    "\n",
    "###  **Flower-Specific Challenges**\n",
    "- **Color Similarity**: High background-flower color similarity issues\n",
    "- **Scale Variations**: Extreme size differences within and across images\n",
    "- **Environmental Factors**: Weather, lighting, and seasonal impact assessment\n",
    "- **Bloom Stage Confusion**: Risk assessment for stage misclassification\n",
    "\n",
    "###  **CBAM-STN-TPS-YOLO Flower Optimizations**\n",
    "- **CBAM Benefits**: Spatial and channel attention for flower-background discrimination\n",
    "- **STN Advantages**: Geometric transformation for flower pose and orientation variations\n",
    "- **TPS Applications**: Non-rigid deformation modeling for wind and growth effects\n",
    "- **YOLO Efficiency**: Multi-scale detection for varying flower densities\n",
    "\n",
    "###  **Training Recommendations**\n",
    "- Color space augmentation for better flower-background separation\n",
    "- Multi-temporal training for bloom stage progression modeling\n",
    "- Environmental condition simulation through diverse augmentation\n",
    "- Attention mechanisms for petal and flower center discrimination\n",
    "- Pollination state as auxiliary classification task\n",
    "\n",
    "###  **Evaluation Strategy**\n",
    "- Class-wise AP for different bloom stages\n",
    "- Temporal consistency metrics for flower development tracking\n",
    "- Environmental robustness assessment\n",
    "- Color similarity discrimination evaluation\n",
    "- Pollination state classification accuracy\n",
    "\n",
    "**All processed flower analysis data and comprehensive visualizations are saved and ready for CBAM-STN-TPS-YOLO model training and agricultural flower detection optimization.**\n",
    "    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
