{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349394b4",
   "metadata": {},
   "source": [
    "# Data Exploration: Global Wheat Dataset Analysis\n",
    "\n",
    "**CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection - GlobalWheat Focus**\n",
    "\n",
    "**Authors:** Satvik Praveen, Yoonsung Jung  \n",
    "**Institution:** Texas A&M University  \n",
    "**Course:** Computer Vision and Deep Learning  \n",
    "**Date:** April 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive data exploration and analysis of the Global Wheat Dataset for agricultural object detection. We focus on analyzing wheat head characteristics, density variations, field conditions, and spatial distributions to optimize CBAM-STN-TPS-YOLO training for wheat detection tasks.\n",
    "\n",
    "## Key Objectives\n",
    "1. Load and analyze Global Wheat dataset structure and composition\n",
    "2. Examine wheat head distribution patterns and density variations\n",
    "3. Analyze bounding box characteristics specific to wheat heads\n",
    "4. Explore field condition variations and environmental factors\n",
    "5. Assess wheat head clustering and overlapping patterns\n",
    "6. Evaluate dataset quality and identify wheat-specific challenges\n",
    "7. Generate comprehensive visualizations and wheat-focused summary reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c6107",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced setup and imports for CBAM-STN-TPS-YOLO Global Wheat Dataset Analysis\n",
    "Comprehensive environment setup with error handling and device optimization\n",
    "\"\"\"\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Computer vision and ML\n",
    "import cv2\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PyTorch ecosystem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data handling\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Project imports with error handling\n",
    "try:\n",
    "    # Core model components\n",
    "    sys.path.append('..')\n",
    "    from src.data.dataset import GlobalWheatDataset, create_agricultural_dataloader\n",
    "    from src.utils.visualization import Visualizer, plot_training_curves, visualize_predictions\n",
    "    from src.utils.evaluation import ModelEvaluator, calculate_model_complexity\n",
    "    from src.utils.config_validator import load_and_validate_config\n",
    "    \n",
    "    print(\"‚úÖ All project imports successful\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Some project imports failed: {e}\")\n",
    "    print(\"Creating fallback implementations...\")\n",
    "    \n",
    "    # Fallback implementations for missing modules\n",
    "    class DummyVisualizer:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def plot_training_curves(self, *args, **kwargs):\n",
    "            pass\n",
    "    \n",
    "    Visualizer = DummyVisualizer\n",
    "    \n",
    "    # Create dummy dataset class if not available\n",
    "    if 'GlobalWheatDataset' not in locals():\n",
    "        class GlobalWheatDataset:\n",
    "            def __init__(self, data_dir, split='train'):\n",
    "                self.data_dir = data_dir\n",
    "                self.split = split\n",
    "                self.class_names = ['wheat_head']\n",
    "                self._create_dummy_data()\n",
    "            \n",
    "            def _create_dummy_data(self):\n",
    "                # Create realistic wheat dataset simulation\n",
    "                np.random.seed(42)\n",
    "                self.length = 200 if self.split == 'train' else 50\n",
    "            \n",
    "            def __len__(self):\n",
    "                return self.length\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                # Generate realistic wheat field image\n",
    "                image = torch.randn(3, 1024, 1024) * 0.3 + 0.5  # More realistic range\n",
    "                \n",
    "                # Generate wheat heads with field-like distribution\n",
    "                num_heads = np.random.poisson(18)  # Average 18 heads per image\n",
    "                num_heads = max(1, min(num_heads, 40))  # Constrain range\n",
    "                \n",
    "                targets = []\n",
    "                for _ in range(num_heads):\n",
    "                    # Create clusters of wheat heads (more realistic)\n",
    "                    if np.random.random() < 0.7 and targets:  # 70% chance of clustering\n",
    "                        # Place near existing head\n",
    "                        existing_head = targets[np.random.randint(len(targets))]\n",
    "                        x = existing_head[1] + np.random.normal(0, 0.1)\n",
    "                        y = existing_head[2] + np.random.normal(0, 0.1)\n",
    "                    else:\n",
    "                        # Random placement\n",
    "                        x = np.random.uniform(0.1, 0.9)\n",
    "                        y = np.random.uniform(0.1, 0.9)\n",
    "                    \n",
    "                    # Wheat head sizes (small objects)\n",
    "                    w = np.random.uniform(0.015, 0.06)  # Typical wheat head width\n",
    "                    h = np.random.uniform(0.015, 0.06)  # Typical wheat head height\n",
    "                    \n",
    "                    # Ensure within bounds\n",
    "                    x = np.clip(x, w/2, 1-w/2)\n",
    "                    y = np.clip(y, h/2, 1-h/2)\n",
    "                    \n",
    "                    targets.append([0, x, y, w, h])  # class_id, x_center, y_center, width, height\n",
    "                \n",
    "                targets = torch.tensor(targets, dtype=torch.float32)\n",
    "                path = f\"dummy_wheat_{self.split}_{idx:06d}.jpg\"\n",
    "                \n",
    "                return image, targets, path\n",
    "\n",
    "# Setup logging for notebooks\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Enhanced plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.titlesize': 16,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "# Device configuration with automatic detection\n",
    "def setup_device():\n",
    "    \"\"\"Setup optimal device configuration with detailed info\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        gpu_name = torch.cuda.get_device_name()\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"‚úÖ CUDA available: {gpu_name}\")\n",
    "        print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Set memory growth to avoid fragmentation\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"‚úÖ MPS (Apple Silicon) available\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"‚ö†Ô∏è Using CPU - analysis will be slower\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducible results\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"üéØ Random seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Enhanced results directory setup\n",
    "notebook_results_dir = Path('../results/notebooks/globalwheat_exploration')\n",
    "notebook_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for organized results\n",
    "subdirs = ['visualizations', 'data_analysis', 'statistics', 'samples']\n",
    "for subdir in subdirs:\n",
    "    (notebook_results_dir / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"üöÄ Enhanced environment setup complete!\")\n",
    "print(f\"üìÅ Results directory: {notebook_results_dir}\")\n",
    "print(f\"üîß Device: {device}\")\n",
    "print(f\"üìä Plotting backend: {plt.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5759dfe",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced dataset loading with comprehensive validation and error handling\n",
    "\"\"\"\n",
    "\n",
    "def validate_dataset_structure(dataset_path):\n",
    "    \"\"\"Validate Global Wheat dataset structure\"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    required_structure = {\n",
    "        'images': ['train', 'val', 'test'],\n",
    "        'labels': ['train', 'val', 'test']\n",
    "    }\n",
    "    \n",
    "    validation_results = {\n",
    "        'valid': True,\n",
    "        'issues': [],\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    for main_dir, subdirs in required_structure.items():\n",
    "        main_path = dataset_path / main_dir\n",
    "        if not main_path.exists():\n",
    "            validation_results['valid'] = False\n",
    "            validation_results['issues'].append(f\"Missing {main_dir} directory\")\n",
    "            continue\n",
    "            \n",
    "        for subdir in subdirs:\n",
    "            sub_path = main_path / subdir\n",
    "            if sub_path.exists():\n",
    "                files = list(sub_path.glob('*'))\n",
    "                validation_results['statistics'][f'{main_dir}_{subdir}'] = len(files)\n",
    "            else:\n",
    "                validation_results['issues'].append(f\"Missing {main_dir}/{subdir} directory\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Load Global Wheat datasets with enhanced error handling\n",
    "datasets = {}\n",
    "dataset_stats = {}\n",
    "dataset_paths = {\n",
    "    'GlobalWheat': '../data/GlobalWheat',\n",
    "    'GlobalWheat2020': '../data/GlobalWheat2020',  # Alternative path\n",
    "    'wheat_data': '../data/wheat'  # Another common naming\n",
    "}\n",
    "\n",
    "print(\"üåæ Loading Global Wheat datasets...\")\n",
    "\n",
    "dataset_loaded = False\n",
    "for dataset_name, dataset_path in dataset_paths.items():\n",
    "    try:\n",
    "        if Path(dataset_path).exists():\n",
    "            print(f\"üìÅ Found dataset at: {dataset_path}\")\n",
    "            \n",
    "            # Validate dataset structure\n",
    "            validation = validate_dataset_structure(dataset_path)\n",
    "            if not validation['valid']:\n",
    "                print(f\"‚ö†Ô∏è Dataset structure issues: {validation['issues']}\")\n",
    "                continue\n",
    "            \n",
    "            # Load dataset splits\n",
    "            try:\n",
    "                datasets[f'{dataset_name}_train'] = GlobalWheatDataset(dataset_path, split='train')\n",
    "                datasets[f'{dataset_name}_val'] = GlobalWheatDataset(dataset_path, split='val')\n",
    "                \n",
    "                train_size = len(datasets[f'{dataset_name}_train'])\n",
    "                val_size = len(datasets[f'{dataset_name}_val'])\n",
    "                \n",
    "                print(f\"‚úÖ {dataset_name} loaded successfully:\")\n",
    "                print(f\"   Train: {train_size} images\")\n",
    "                print(f\"   Val: {val_size} images\")\n",
    "                \n",
    "                # Test data loading\n",
    "                try:\n",
    "                    sample_image, sample_targets, sample_path = datasets[f'{dataset_name}_train'][0]\n",
    "                    print(f\"   Sample image shape: {sample_image.shape}\")\n",
    "                    print(f\"   Sample targets: {len(sample_targets)} wheat heads\")\n",
    "                    dataset_loaded = True\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Data loading test failed: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load {dataset_name}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking {dataset_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fallback to enhanced dummy dataset if no real data found\n",
    "if not dataset_loaded:\n",
    "    print(\"üìù No real Global Wheat dataset found. Creating enhanced dummy dataset...\")\n",
    "    \n",
    "    class EnhancedDummyWheatDataset:\n",
    "        def __init__(self, split='train'):\n",
    "            self.split = split\n",
    "            self.class_names = ['wheat_head']\n",
    "            self.split_sizes = {'train': 180, 'val': 45, 'test': 30}\n",
    "            self.length = self.split_sizes.get(split, 100)\n",
    "            \n",
    "            # Create realistic wheat distribution parameters\n",
    "            self.wheat_params = {\n",
    "                'avg_heads_per_image': 18,\n",
    "                'head_size_range': (0.015, 0.06),\n",
    "                'clustering_probability': 0.7,\n",
    "                'field_brightness_range': (0.3, 0.8),\n",
    "                'lighting_variations': ['uniform', 'shadows', 'overexposed']\n",
    "            }\n",
    "            \n",
    "            print(f\"üìä Created {split} split with {self.length} synthetic wheat field images\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.length\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            np.random.seed(42 + idx)  # Reproducible but varied\n",
    "            \n",
    "            # Generate realistic field conditions\n",
    "            brightness_base = np.random.uniform(*self.wheat_params['field_brightness_range'])\n",
    "            lighting_condition = np.random.choice(self.wheat_params['lighting_variations'])\n",
    "            \n",
    "            # Create base field image with realistic characteristics\n",
    "            if lighting_condition == 'shadows':\n",
    "                # Create shadow patterns\n",
    "                image = self._create_shadow_field(brightness_base)\n",
    "            elif lighting_condition == 'overexposed':\n",
    "                # Create overexposed areas\n",
    "                image = self._create_bright_field(brightness_base)\n",
    "            else:\n",
    "                # Uniform lighting\n",
    "                image = self._create_uniform_field(brightness_base)\n",
    "            \n",
    "            # Generate wheat heads with realistic clustering\n",
    "            num_heads = max(1, np.random.poisson(self.wheat_params['avg_heads_per_image']))\n",
    "            targets = self._generate_wheat_heads(num_heads)\n",
    "            \n",
    "            path = f\"synthetic_wheat_{self.split}_{idx:06d}_{lighting_condition}.jpg\"\n",
    "            \n",
    "            return image, targets, path\n",
    "        \n",
    "        def _create_uniform_field(self, brightness):\n",
    "            \"\"\"Create uniform lighting field\"\"\"\n",
    "            image = torch.normal(brightness, 0.1, (3, 1024, 1024))\n",
    "            # Add field texture\n",
    "            noise = torch.randn(3, 1024, 1024) * 0.05\n",
    "            return torch.clamp(image + noise, 0, 1)\n",
    "        \n",
    "        def _create_shadow_field(self, brightness):\n",
    "            \"\"\"Create field with shadow patterns\"\"\"\n",
    "            image = torch.full((3, 1024, 1024), brightness)\n",
    "            \n",
    "            # Add diagonal shadow patterns\n",
    "            x, y = torch.meshgrid(torch.linspace(0, 1, 1024), torch.linspace(0, 1, 1024), indexing='ij')\n",
    "            shadow_pattern = torch.sin(x * 10) * torch.cos(y * 8) * 0.2\n",
    "            \n",
    "            for c in range(3):\n",
    "                image[c] = torch.clamp(image[c] + shadow_pattern, 0.2, 1.0)\n",
    "            \n",
    "            return image\n",
    "        \n",
    "        def _create_bright_field(self, brightness):\n",
    "            \"\"\"Create field with bright spots (overexposure)\"\"\"\n",
    "            image = torch.full((3, 1024, 1024), brightness)\n",
    "            \n",
    "            # Add bright circular regions\n",
    "            x, y = torch.meshgrid(torch.linspace(-1, 1, 1024), torch.linspace(-1, 1, 1024), indexing='ij')\n",
    "            bright_spots = torch.exp(-(x**2 + y**2) * 3) * 0.3\n",
    "            \n",
    "            for c in range(3):\n",
    "                image[c] = torch.clamp(image[c] + bright_spots, 0, 1.0)\n",
    "            \n",
    "            return image\n",
    "        \n",
    "        def _generate_wheat_heads(self, num_heads):\n",
    "            \"\"\"Generate realistic wheat head distributions\"\"\"\n",
    "            targets = []\n",
    "            cluster_centers = []\n",
    "            \n",
    "            # Create 2-4 cluster centers\n",
    "            num_clusters = np.random.randint(2, 5)\n",
    "            for _ in range(num_clusters):\n",
    "                center_x = np.random.uniform(0.2, 0.8)\n",
    "                center_y = np.random.uniform(0.2, 0.8)\n",
    "                cluster_centers.append((center_x, center_y))\n",
    "            \n",
    "            for i in range(num_heads):\n",
    "                if np.random.random() < self.wheat_params['clustering_probability'] and cluster_centers:\n",
    "                    # Place near a cluster center\n",
    "                    center_x, center_y = cluster_centers[np.random.randint(len(cluster_centers))]\n",
    "                    x = center_x + np.random.normal(0, 0.1)\n",
    "                    y = center_y + np.random.normal(0, 0.1)\n",
    "                else:\n",
    "                    # Random placement\n",
    "                    x = np.random.uniform(0.1, 0.9)\n",
    "                    y = np.random.uniform(0.1, 0.9)\n",
    "                \n",
    "                # Wheat head size with some correlation to position (perspective effect)\n",
    "                size_factor = 1.0 - (y * 0.3)  # Heads appear smaller towards top\n",
    "                w = np.random.uniform(*self.wheat_params['head_size_range']) * size_factor\n",
    "                h = np.random.uniform(*self.wheat_params['head_size_range']) * size_factor\n",
    "                \n",
    "                # Ensure bounds\n",
    "                x = np.clip(x, w/2, 1-w/2)\n",
    "                y = np.clip(y, h/2, 1-h/2)\n",
    "                \n",
    "                targets.append([0, x, y, w, h])\n",
    "            \n",
    "            return torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    # Create enhanced dummy datasets\n",
    "    datasets['GlobalWheat_train'] = EnhancedDummyWheatDataset('train')\n",
    "    datasets['GlobalWheat_val'] = EnhancedDummyWheatDataset('val')\n",
    "\n",
    "# Calculate comprehensive dataset statistics\n",
    "for name, dataset in datasets.items():\n",
    "    dataset_type = name.split('_')[-1]  # train/val/test\n",
    "    base_name = name.replace(f'_{dataset_type}', '')\n",
    "    \n",
    "    if base_name not in dataset_stats:\n",
    "        dataset_stats[base_name] = {\n",
    "            'splits': {},\n",
    "            'total_images': 0,\n",
    "            'classes': getattr(dataset, 'class_names', ['wheat_head']),\n",
    "            'num_classes': len(getattr(dataset, 'class_names', ['wheat_head']))\n",
    "        }\n",
    "    \n",
    "    dataset_stats[base_name]['splits'][dataset_type] = len(dataset)\n",
    "    dataset_stats[base_name]['total_images'] += len(dataset)\n",
    "\n",
    "# Display enhanced dataset summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üåæ GLOBAL WHEAT DATASET ANALYSIS OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dataset_name, stats in dataset_stats.items():\n",
    "    print(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "    print(f\"   üéØ Domain: Wheat head detection in agricultural fields\")\n",
    "    print(f\"   üìà Total images: {stats['total_images']}\")\n",
    "    print(f\"   üè∑Ô∏è Classes ({stats['num_classes']}): {stats['classes']}\")\n",
    "    \n",
    "    print(f\"   üìÅ Data splits:\")\n",
    "    for split, size in stats['splits'].items():\n",
    "        percentage = (size / stats['total_images']) * 100\n",
    "        print(f\"     {split.capitalize()}: {size} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"   üåæ Expected characteristics:\")\n",
    "    print(f\"     ‚Ä¢ High-density wheat heads (10-30 per image)\")\n",
    "    print(f\"     ‚Ä¢ Small object detection challenge\")\n",
    "    print(f\"     ‚Ä¢ Overlapping and clustering patterns\")\n",
    "    print(f\"     ‚Ä¢ Field condition variations\")\n",
    "    print(f\"     ‚Ä¢ Multiple growth stages and orientations\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loading complete. Ready for wheat-specific analysis!\")\n",
    "print(f\"üìÅ Results will be saved to: {notebook_results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc86d5",
   "metadata": {},
   "source": [
    "## 3. Wheat Head Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced wheat head distribution analysis with advanced metrics and visualizations\n",
    "\"\"\"\n",
    "\n",
    "def analyze_wheat_distribution_advanced(dataset, dataset_name, max_samples=500):\n",
    "    \"\"\"Advanced analysis of wheat head distribution patterns\"\"\"\n",
    "    \n",
    "    wheat_metrics = {\n",
    "        'basic_stats': {\n",
    "            'total_wheat_heads': 0,\n",
    "            'images_analyzed': 0,\n",
    "            'heads_per_image': [],\n",
    "            'valid_images': 0,\n",
    "            'empty_images': 0\n",
    "        },\n",
    "        'density_analysis': {\n",
    "            'density_categories': {'very_low': 0, 'low': 0, 'medium': 0, 'high': 0, 'very_high': 0},\n",
    "            'density_histogram': [],\n",
    "            'density_percentiles': {}\n",
    "        },\n",
    "        'spatial_patterns': {\n",
    "            'x_coordinates': [],\n",
    "            'y_coordinates': [],\n",
    "            'spatial_clusters': [],\n",
    "            'edge_proximity': []\n",
    "        },\n",
    "        'size_analysis': {\n",
    "            'areas': [],\n",
    "            'widths': [],\n",
    "            'heights': [],\n",
    "            'aspect_ratios': [],\n",
    "            'size_categories': {'tiny': 0, 'small': 0, 'medium': 0, 'large': 0}\n",
    "        },\n",
    "        'field_characteristics': {\n",
    "            'row_patterns': [],\n",
    "            'clustering_strength': [],\n",
    "            'uniformity_measures': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Intelligent sampling for diverse representation\n",
    "    total_size = len(dataset)\n",
    "    sample_size = min(total_size, max_samples)\n",
    "    \n",
    "    # Use stratified sampling if possible\n",
    "    if sample_size < total_size:\n",
    "        # Sample evenly across the dataset\n",
    "        step_size = total_size // sample_size\n",
    "        indices = list(range(0, total_size, step_size))[:sample_size]\n",
    "    else:\n",
    "        indices = list(range(total_size))\n",
    "    \n",
    "    print(f\"üåæ Analyzing wheat distribution in {sample_size} images from {dataset_name}...\")\n",
    "    \n",
    "    # Progress tracking\n",
    "    failed_loads = 0\n",
    "    processing_errors = 0\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Processing images\"):\n",
    "        try:\n",
    "            image, targets, path = dataset[idx]\n",
    "            \n",
    "            # Validate image and targets\n",
    "            if image is None or targets is None:\n",
    "                failed_loads += 1\n",
    "                continue\n",
    "                \n",
    "            wheat_metrics['basic_stats']['images_analyzed'] += 1\n",
    "            \n",
    "            # Handle empty images\n",
    "            if targets.numel() == 0 or len(targets) == 0:\n",
    "                wheat_metrics['basic_stats']['heads_per_image'].append(0)\n",
    "                wheat_metrics['basic_stats']['empty_images'] += 1\n",
    "                wheat_metrics['density_analysis']['density_categories']['very_low'] += 1\n",
    "                continue\n",
    "            \n",
    "            wheat_metrics['basic_stats']['valid_images'] += 1\n",
    "            num_heads = len(targets)\n",
    "            wheat_metrics['basic_stats']['total_wheat_heads'] += num_heads\n",
    "            wheat_metrics['basic_stats']['heads_per_image'].append(num_heads)\n",
    "            \n",
    "            # Enhanced density categorization (wheat-specific thresholds)\n",
    "            if num_heads == 0:\n",
    "                wheat_metrics['density_analysis']['density_categories']['very_low'] += 1\n",
    "            elif num_heads <= 5:\n",
    "                wheat_metrics['density_analysis']['density_categories']['low'] += 1\n",
    "            elif num_heads <= 15:\n",
    "                wheat_metrics['density_analysis']['density_categories']['medium'] += 1\n",
    "            elif num_heads <= 30:\n",
    "                wheat_metrics['density_analysis']['density_categories']['high'] += 1\n",
    "            else:\n",
    "                wheat_metrics['density_analysis']['density_categories']['very_high'] += 1\n",
    "            \n",
    "            wheat_metrics['density_analysis']['density_histogram'].append(num_heads)\n",
    "            \n",
    "            # Process individual wheat heads\n",
    "            wheat_positions = []\n",
    "            for target in targets:\n",
    "                try:\n",
    "                    if len(target) >= 5:\n",
    "                        cls, x_center, y_center, width, height = target[:5]\n",
    "                        \n",
    "                        # Spatial analysis\n",
    "                        wheat_metrics['spatial_patterns']['x_coordinates'].append(float(x_center))\n",
    "                        wheat_metrics['spatial_patterns']['y_coordinates'].append(float(y_center))\n",
    "                        wheat_positions.append((float(x_center), float(y_center)))\n",
    "                        \n",
    "                        # Edge proximity analysis\n",
    "                        edge_distance = min(x_center, y_center, 1-x_center, 1-y_center)\n",
    "                        wheat_metrics['spatial_patterns']['edge_proximity'].append(edge_distance)\n",
    "                        \n",
    "                        # Size analysis\n",
    "                        area = float(width * height)\n",
    "                        wheat_metrics['size_analysis']['areas'].append(area)\n",
    "                        wheat_metrics['size_analysis']['widths'].append(float(width))\n",
    "                        wheat_metrics['size_analysis']['heights'].append(float(height))\n",
    "                        \n",
    "                        # Aspect ratio\n",
    "                        if height > 0:\n",
    "                            aspect_ratio = float(width / height)\n",
    "                            wheat_metrics['size_analysis']['aspect_ratios'].append(aspect_ratio)\n",
    "                        \n",
    "                        # Size categorization (refined wheat-specific thresholds)\n",
    "                        if area < 0.0003:  # Very tiny\n",
    "                            wheat_metrics['size_analysis']['size_categories']['tiny'] += 1\n",
    "                        elif area < 0.0015:  # Small\n",
    "                            wheat_metrics['size_analysis']['size_categories']['small'] += 1\n",
    "                        elif area < 0.006:  # Medium\n",
    "                            wheat_metrics['size_analysis']['size_categories']['medium'] += 1\n",
    "                        else:  # Large\n",
    "                            wheat_metrics['size_analysis']['size_categories']['large'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    processing_errors += 1\n",
    "                    continue\n",
    "            \n",
    "            # Analyze spatial clustering for this image\n",
    "            if len(wheat_positions) > 2:\n",
    "                try:\n",
    "                    positions_array = np.array(wheat_positions)\n",
    "                    \n",
    "                    # Calculate clustering strength using DBSCAN\n",
    "                    from sklearn.cluster import DBSCAN\n",
    "                    clustering = DBSCAN(eps=0.08, min_samples=2).fit(positions_array)\n",
    "                    n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n",
    "                    clustering_strength = n_clusters / len(wheat_positions) if len(wheat_positions) > 0 else 0\n",
    "                    \n",
    "                    wheat_metrics['field_characteristics']['clustering_strength'].append(clustering_strength)\n",
    "                    wheat_metrics['spatial_patterns']['spatial_clusters'].append(n_clusters)\n",
    "                    \n",
    "                    # Uniformity measure (coefficient of variation of distances)\n",
    "                    if len(positions_array) > 1:\n",
    "                        distances = pdist(positions_array)\n",
    "                        uniformity = np.std(distances) / np.mean(distances) if np.mean(distances) > 0 else 0\n",
    "                        wheat_metrics['field_characteristics']['uniformity_measures'].append(uniformity)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    processing_errors += 1\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            failed_loads += 1\n",
    "            continue\n",
    "    \n",
    "    # Calculate advanced statistics\n",
    "    if wheat_metrics['basic_stats']['heads_per_image']:\n",
    "        heads_array = np.array(wheat_metrics['basic_stats']['heads_per_image'])\n",
    "        wheat_metrics['density_analysis']['density_percentiles'] = {\n",
    "            'p25': float(np.percentile(heads_array, 25)),\n",
    "            'p50': float(np.percentile(heads_array, 50)),\n",
    "            'p75': float(np.percentile(heads_array, 75)),\n",
    "            'p90': float(np.percentile(heads_array, 90)),\n",
    "            'p95': float(np.percentile(heads_array, 95))\n",
    "        }\n",
    "    \n",
    "    # Report processing statistics\n",
    "    print(f\"   ‚úÖ Successfully processed: {wheat_metrics['basic_stats']['images_analyzed']} images\")\n",
    "    print(f\"   ‚ö†Ô∏è Failed loads: {failed_loads}\")\n",
    "    print(f\"   ‚ö†Ô∏è Processing errors: {processing_errors}\")\n",
    "    \n",
    "    return wheat_metrics\n",
    "\n",
    "# Analyze all datasets with enhanced metrics\n",
    "wheat_distribution_results = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    wheat_metrics = analyze_wheat_distribution_advanced(dataset, name, max_samples=400)\n",
    "    wheat_distribution_results[name] = wheat_metrics\n",
    "    \n",
    "    # Display comprehensive results\n",
    "    basic_stats = wheat_metrics['basic_stats']\n",
    "    density_stats = wheat_metrics['density_analysis']\n",
    "    size_stats = wheat_metrics['size_analysis']\n",
    "    \n",
    "    print(f\"\\nüåæ {name} - COMPREHENSIVE WHEAT ANALYSIS:\")\n",
    "    print(f\"   üìä Basic Statistics:\")\n",
    "    print(f\"     Total wheat heads: {basic_stats['total_wheat_heads']:,}\")\n",
    "    print(f\"     Images analyzed: {basic_stats['images_analyzed']}\")\n",
    "    print(f\"     Valid images: {basic_stats['valid_images']}\")\n",
    "    print(f\"     Empty images: {basic_stats['empty_images']}\")\n",
    "    \n",
    "    if basic_stats['heads_per_image']:\n",
    "        heads_array = np.array(basic_stats['heads_per_image'])\n",
    "        print(f\"   üìà Density Statistics:\")\n",
    "        print(f\"     Mean heads/image: {np.mean(heads_array):.2f} ¬± {np.std(heads_array):.2f}\")\n",
    "        print(f\"     Median heads/image: {np.median(heads_array):.1f}\")\n",
    "        print(f\"     Range: {np.min(heads_array)} - {np.max(heads_array)} heads\")\n",
    "        \n",
    "        print(f\"   üìä Density Distribution:\")\n",
    "        total_images = sum(density_stats['density_categories'].values())\n",
    "        for category, count in density_stats['density_categories'].items():\n",
    "            percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "            print(f\"     {category.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"   üìè Size Analysis:\")\n",
    "        if size_stats['areas']:\n",
    "            areas_array = np.array(size_stats['areas'])\n",
    "            print(f\"     Mean area: {np.mean(areas_array):.6f} ¬± {np.std(areas_array):.6f}\")\n",
    "            print(f\"     Median area: {np.median(areas_array):.6f}\")\n",
    "            \n",
    "            total_heads = sum(size_stats['size_categories'].values())\n",
    "            print(f\"   üéØ Size Categories:\")\n",
    "            for category, count in size_stats['size_categories'].items():\n",
    "                percentage = (count / total_heads * 100) if total_heads > 0 else 0\n",
    "                print(f\"     {category.title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Spatial analysis summary\n",
    "        if wheat_metrics['field_characteristics']['clustering_strength']:\n",
    "            avg_clustering = np.mean(wheat_metrics['field_characteristics']['clustering_strength'])\n",
    "            print(f\"   üîó Spatial Characteristics:\")\n",
    "            print(f\"     Average clustering strength: {avg_clustering:.3f}\")\n",
    "            \n",
    "            if wheat_metrics['field_characteristics']['uniformity_measures']:\n",
    "                avg_uniformity = np.mean(wheat_metrics['field_characteristics']['uniformity_measures'])\n",
    "                print(f\"     Field uniformity measure: {avg_uniformity:.3f}\")\n",
    "\n",
    "# Create enhanced visualization dashboard\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "dataset_colors = plt.cm.Set1(np.linspace(0, 1, len(wheat_distribution_results)))\n",
    "\n",
    "for idx, (dataset_name, metrics) in enumerate(wheat_distribution_results.items()):\n",
    "    color = dataset_colors[idx]\n",
    "    \n",
    "    # 1. Distribution of heads per image (top row, left)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    if metrics['basic_stats']['heads_per_image']:\n",
    "        ax1.hist(metrics['basic_stats']['heads_per_image'], bins=30, alpha=0.7, \n",
    "                label=dataset_name, color=color, density=True)\n",
    "        ax1.axvline(np.mean(metrics['basic_stats']['heads_per_image']), \n",
    "                   color=color, linestyle='--', alpha=0.8)\n",
    "    ax1.set_title('Wheat Heads per Image Distribution', fontweight='bold')\n",
    "    ax1.set_xlabel('Number of Wheat Heads')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Density categories (top row, middle)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    categories = list(metrics['density_analysis']['density_categories'].keys())\n",
    "    values = list(metrics['density_analysis']['density_categories'].values())\n",
    "    bars = ax2.bar([cat.replace('_', '\\n') for cat in categories], values, \n",
    "                   alpha=0.8, color=plt.cm.viridis(np.linspace(0, 1, len(categories))))\n",
    "    ax2.set_title(f'Density Categories - {dataset_name}', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Images')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(values),\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 3. Size distribution (top row, right)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    if metrics['size_analysis']['areas']:\n",
    "        areas = np.array(metrics['size_analysis']['areas'])\n",
    "        ax3.hist(areas, bins=30, alpha=0.7, color=color, edgecolor='black')\n",
    "        ax3.axvline(np.mean(areas), color='red', linestyle='--', alpha=0.8, label='Mean')\n",
    "        ax3.axvline(np.median(areas), color='green', linestyle='--', alpha=0.8, label='Median')\n",
    "    ax3.set_title(f'Wheat Head Area Distribution - {dataset_name}', fontweight='bold')\n",
    "    ax3.set_xlabel('Area (normalized)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Spatial distribution heatmap (second row, spanning two columns)\n",
    "ax4 = fig.add_subplot(gs[1, :2])\n",
    "for idx, (dataset_name, metrics) in enumerate(wheat_distribution_results.items()):\n",
    "    if metrics['spatial_patterns']['x_coordinates']:\n",
    "        x_coords = metrics['spatial_patterns']['x_coordinates']\n",
    "        y_coords = metrics['spatial_patterns']['y_coordinates']\n",
    "        \n",
    "        # Create 2D histogram\n",
    "        hist, xedges, yedges = np.histogram2d(x_coords, y_coords, bins=25, range=[[0, 1], [0, 1]])\n",
    "        \n",
    "        # Overlay heatmaps with different alpha values\n",
    "        im = ax4.imshow(hist.T, origin='lower', extent=[0, 1, 0, 1], \n",
    "                       cmap=plt.cm.Reds, alpha=0.6, interpolation='bilinear')\n",
    "\n",
    "ax4.set_title('Spatial Distribution Heatmap (All Datasets)', fontweight='bold')\n",
    "ax4.set_xlabel('X Coordinate (normalized)')\n",
    "ax4.set_ylabel('Y Coordinate (normalized)')\n",
    "\n",
    "# 5. Clustering analysis (second row, right)\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "clustering_data = []\n",
    "clustering_labels = []\n",
    "for dataset_name, metrics in wheat_distribution_results.items():\n",
    "    if metrics['field_characteristics']['clustering_strength']:\n",
    "        clustering_data.extend(metrics['field_characteristics']['clustering_strength'])\n",
    "        clustering_labels.extend([dataset_name] * len(metrics['field_characteristics']['clustering_strength']))\n",
    "\n",
    "if clustering_data:\n",
    "    df_clustering = pd.DataFrame({'Clustering_Strength': clustering_data, 'Dataset': clustering_labels})\n",
    "    sns.boxplot(data=df_clustering, x='Dataset', y='Clustering_Strength', ax=ax5)\n",
    "    ax5.set_title('Clustering Strength Distribution', fontweight='bold')\n",
    "    ax5.set_ylabel('Clustering Strength')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Size category comparison (third row, left)\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "size_categories = ['tiny', 'small', 'medium', 'large']\n",
    "x_pos = np.arange(len(size_categories))\n",
    "width = 0.8 / len(wheat_distribution_results)\n",
    "\n",
    "for idx, (dataset_name, metrics) in enumerate(wheat_distribution_results.items()):\n",
    "    values = [metrics['size_analysis']['size_categories'][cat] for cat in size_categories]\n",
    "    ax6.bar(x_pos + idx*width, values, width, label=dataset_name, alpha=0.8)\n",
    "\n",
    "ax6.set_title('Size Category Comparison', fontweight='bold')\n",
    "ax6.set_xlabel('Size Category')\n",
    "ax6.set_ylabel('Count')\n",
    "ax6.set_xticks(x_pos + width/2)\n",
    "ax6.set_xticklabels(size_categories)\n",
    "ax6.legend()\n",
    "\n",
    "# 7. Percentile analysis (third row, middle)\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "for idx, (dataset_name, metrics) in enumerate(wheat_distribution_results.items()):\n",
    "    if 'density_percentiles' in metrics['density_analysis']:\n",
    "        percentiles = metrics['density_analysis']['density_percentiles']\n",
    "        if percentiles:\n",
    "            p_labels = list(percentiles.keys())\n",
    "            p_values = list(percentiles.values())\n",
    "            ax7.plot(p_labels, p_values, marker='o', label=dataset_name, linewidth=2, markersize=8)\n",
    "\n",
    "ax7.set_title('Density Percentile Analysis', fontweight='bold')\n",
    "ax7.set_xlabel('Percentile')\n",
    "ax7.set_ylabel('Number of Heads')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Statistical summary (third row, right)\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "ax8.axis('off')\n",
    "\n",
    "summary_text = \"üìä STATISTICAL SUMMARY\\n\\n\"\n",
    "for dataset_name, metrics in wheat_distribution_results.items():\n",
    "    basic_stats = metrics['basic_stats']\n",
    "    if basic_stats['heads_per_image']:\n",
    "        heads_array = np.array(basic_stats['heads_per_image'])\n",
    "        summary_text += f\"üåæ {dataset_name}:\\n\"\n",
    "        summary_text += f\"  Images: {basic_stats['images_analyzed']}\\n\"\n",
    "        summary_text += f\"  Total heads: {basic_stats['total_wheat_heads']:,}\\n\"\n",
    "        summary_text += f\"  Mean/image: {np.mean(heads_array):.1f}\\n\"\n",
    "        summary_text += f\"  Std dev: {np.std(heads_array):.1f}\\n\"\n",
    "        summary_text += f\"  Max density: {np.max(heads_array)}\\n\\n\"\n",
    "\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.suptitle('üåæ COMPREHENSIVE GLOBAL WHEAT DISTRIBUTION ANALYSIS', fontsize=16, fontweight='bold')\n",
    "plt.savefig(notebook_results_dir / 'visualizations' / 'comprehensive_wheat_distribution.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save detailed results\n",
    "analysis_summary = {}\n",
    "for dataset_name, metrics in wheat_distribution_results.items():\n",
    "    analysis_summary[dataset_name] = {\n",
    "        'basic_statistics': {\n",
    "            'total_wheat_heads': metrics['basic_stats']['total_wheat_heads'],\n",
    "            'images_analyzed': metrics['basic_stats']['images_analyzed'],\n",
    "            'valid_images': metrics['basic_stats']['valid_images'],\n",
    "            'empty_images': metrics['basic_stats']['empty_images']\n",
    "        },\n",
    "        'density_statistics': {\n",
    "            'mean_heads_per_image': float(np.mean(metrics['basic_stats']['heads_per_image'])) if metrics['basic_stats']['heads_per_image'] else 0,\n",
    "            'std_heads_per_image': float(np.std(metrics['basic_stats']['heads_per_image'])) if metrics['basic_stats']['heads_per_image'] else 0,\n",
    "            'median_heads_per_image': float(np.median(metrics['basic_stats']['heads_per_image'])) if metrics['basic_stats']['heads_per_image'] else 0,\n",
    "            'percentiles': metrics['density_analysis']['density_percentiles'],\n",
    "            'category_distribution': metrics['density_analysis']['density_categories']\n",
    "        },\n",
    "        'size_statistics': {\n",
    "            'mean_area': float(np.mean(metrics['size_analysis']['areas'])) if metrics['size_analysis']['areas'] else 0,\n",
    "            'std_area': float(np.std(metrics['size_analysis']['areas'])) if metrics['size_analysis']['areas'] else 0,\n",
    "            'size_categories': metrics['size_analysis']['size_categories']\n",
    "        },\n",
    "        'spatial_statistics': {\n",
    "            'clustering_strength': float(np.mean(metrics['field_characteristics']['clustering_strength'])) if metrics['field_characteristics']['clustering_strength'] else 0,\n",
    "            'uniformity_measure': float(np.mean(metrics['field_characteristics']['uniformity_measures'])) if metrics['field_characteristics']['uniformity_measures'] else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(notebook_results_dir / 'data_analysis' / 'comprehensive_wheat_distribution.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Comprehensive wheat distribution analysis saved!\")\n",
    "print(f\"üìÅ Location: {notebook_results_dir / 'data_analysis' / 'comprehensive_wheat_distribution.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0289aaf9",
   "metadata": {},
   "source": [
    "## 4. Wheat Head Clustering and Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea68ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced wheat head clustering and overlap analysis with advanced spatial algorithms\n",
    "\"\"\"\n",
    "\n",
    "def analyze_wheat_clustering_advanced(dataset, dataset_name, max_samples=300):\n",
    "    \"\"\"Advanced clustering analysis with multiple algorithms and metrics\"\"\"\n",
    "    \n",
    "    clustering_metrics = {\n",
    "        'overlap_analysis': {\n",
    "            'iou_scores': [],\n",
    "            'overlap_pairs': [],\n",
    "            'high_overlap_images': 0,\n",
    "            'overlap_statistics': {}\n",
    "        },\n",
    "        'clustering_analysis': {\n",
    "            'dbscan_results': [],\n",
    "            'kmeans_results': [],\n",
    "            'hierarchical_results': [],\n",
    "            'silhouette_scores': []\n",
    "        },\n",
    "        'spatial_metrics': {\n",
    "            'nearest_neighbor_distances': [],\n",
    "            'density_gradients': [],\n",
    "            'spatial_autocorrelation': [],\n",
    "            'boundary_effects': []\n",
    "        },\n",
    "        'pattern_detection': {\n",
    "            'row_patterns': 0,\n",
    "            'circular_patterns': 0,\n",
    "            'random_patterns': 0,\n",
    "            'grid_patterns': 0\n",
    "        },\n",
    "        'density_analysis': {\n",
    "            'local_densities': [],\n",
    "            'density_hotspots': [],\n",
    "            'crowding_indices': [],\n",
    "            'isolation_scores': []\n",
    "        },\n",
    "        'geometric_features': {\n",
    "            'convex_hull_areas': [],\n",
    "            'bounding_box_ratios': [],\n",
    "            'centroid_distances': [],\n",
    "            'orientation_analyses': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    print(f\"üîç Advanced clustering analysis on {sample_size} images from {dataset_name}...\")\n",
    "    \n",
    "    processing_stats = {'success': 0, 'errors': 0, 'empty': 0}\n",
    "    \n",
    "    for i in tqdm(indices, desc=\"Analyzing clustering patterns\"):\n",
    "        try:\n",
    "            image, targets, path = dataset[i]\n",
    "            \n",
    "            if targets.numel() == 0 or len(targets) < 2:\n",
    "                processing_stats['empty'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Extract coordinates and create arrays\n",
    "            coordinates = []\n",
    "            boxes = []\n",
    "            areas = []\n",
    "            \n",
    "            for target in targets:\n",
    "                if len(target) >= 5:\n",
    "                    cls, x_center, y_center, width, height = target[:5]\n",
    "                    coordinates.append([float(x_center), float(y_center)])\n",
    "                    \n",
    "                    # Convert to corner coordinates for IoU calculation\n",
    "                    x1 = float(x_center - width/2)\n",
    "                    y1 = float(y_center - height/2)\n",
    "                    x2 = float(x_center + width/2)\n",
    "                    y2 = float(y_center + height/2)\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    areas.append(float(width * height))\n",
    "            \n",
    "            if len(coordinates) < 2:\n",
    "                processing_stats['empty'] += 1\n",
    "                continue\n",
    "                \n",
    "            coordinates = np.array(coordinates)\n",
    "            boxes = np.array(boxes)\n",
    "            processing_stats['success'] += 1\n",
    "            \n",
    "            # 1. ADVANCED OVERLAP ANALYSIS\n",
    "            iou_scores = []\n",
    "            overlap_count = 0\n",
    "            \n",
    "            for j in range(len(boxes)):\n",
    "                for k in range(j+1, len(boxes)):\n",
    "                    box1, box2 = boxes[j], boxes[k]\n",
    "                    \n",
    "                    # Calculate IoU\n",
    "                    x1_inter = max(box1[0], box2[0])\n",
    "                    y1_inter = max(box1[1], box2[1])\n",
    "                    x2_inter = min(box1[2], box2[2])\n",
    "                    y2_inter = min(box1[3], box2[3])\n",
    "                    \n",
    "                    if x2_inter > x1_inter and y2_inter > y1_inter:\n",
    "                        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "                        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                        union_area = box1_area + box2_area - inter_area\n",
    "                        \n",
    "                        if union_area > 0:\n",
    "                            iou = inter_area / union_area\n",
    "                            iou_scores.append(iou)\n",
    "                            \n",
    "                            if iou > 0.1:  # Significant overlap threshold\n",
    "                                overlap_count += 1\n",
    "            \n",
    "            if iou_scores:\n",
    "                clustering_metrics['overlap_analysis']['iou_scores'].extend(iou_scores)\n",
    "                clustering_metrics['overlap_analysis']['overlap_pairs'].append(overlap_count)\n",
    "                \n",
    "                if max(iou_scores) > 0.3:  # High overlap threshold\n",
    "                    clustering_metrics['overlap_analysis']['high_overlap_images'] += 1\n",
    "            \n",
    "            # 2. MULTI-ALGORITHM CLUSTERING ANALYSIS\n",
    "            if len(coordinates) >= 3:\n",
    "                \n",
    "                # DBSCAN Clustering\n",
    "                try:\n",
    "                    from sklearn.cluster import DBSCAN\n",
    "                    dbscan = DBSCAN(eps=0.08, min_samples=2)\n",
    "                    dbscan_labels = dbscan.fit_predict(coordinates)\n",
    "                    \n",
    "                    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "                    n_noise = list(dbscan_labels).count(-1)\n",
    "                    \n",
    "                    clustering_metrics['clustering_analysis']['dbscan_results'].append({\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'n_noise': n_noise,\n",
    "                        'n_points': len(coordinates),\n",
    "                        'cluster_ratio': n_clusters / len(coordinates) if len(coordinates) > 0 else 0\n",
    "                    })\n",
    "                    \n",
    "                    # Calculate silhouette score if we have clusters\n",
    "                    if n_clusters > 1 and n_noise < len(coordinates):\n",
    "                        try:\n",
    "                            from sklearn.metrics import silhouette_score\n",
    "                            valid_labels = dbscan_labels[dbscan_labels != -1]\n",
    "                            valid_coords = coordinates[dbscan_labels != -1]\n",
    "                            \n",
    "                            if len(set(valid_labels)) > 1 and len(valid_coords) > 1:\n",
    "                                sil_score = silhouette_score(valid_coords, valid_labels)\n",
    "                                clustering_metrics['clustering_analysis']['silhouette_scores'].append(sil_score)\n",
    "                        except:\n",
    "                            pass\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    processing_stats['errors'] += 1\n",
    "                \n",
    "                # K-means clustering (try different k values)\n",
    "                try:\n",
    "                    from sklearn.cluster import KMeans\n",
    "                    optimal_k = min(5, len(coordinates)//2)\n",
    "                    if optimal_k >= 2:\n",
    "                        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "                        kmeans_labels = kmeans.fit_predict(coordinates)\n",
    "                        \n",
    "                        clustering_metrics['clustering_analysis']['kmeans_results'].append({\n",
    "                            'n_clusters': optimal_k,\n",
    "                            'inertia': kmeans.inertia_,\n",
    "                            'n_points': len(coordinates)\n",
    "                        })\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # 3. SPATIAL METRICS ANALYSIS\n",
    "            \n",
    "            # Nearest neighbor distances\n",
    "            if len(coordinates) > 1:\n",
    "                distances = pdist(coordinates)\n",
    "                min_distances = []\n",
    "                \n",
    "                for j in range(len(coordinates)):\n",
    "                    other_coords = np.delete(coordinates, j, axis=0)\n",
    "                    if len(other_coords) > 0:\n",
    "                        dists_to_point = np.sqrt(np.sum((other_coords - coordinates[j])**2, axis=1))\n",
    "                        min_distances.append(np.min(dists_to_point))\n",
    "                \n",
    "                clustering_metrics['spatial_metrics']['nearest_neighbor_distances'].extend(min_distances)\n",
    "            \n",
    "            # Local density analysis\n",
    "            if len(coordinates) >= 3:\n",
    "                local_densities = []\n",
    "                for j, point in enumerate(coordinates):\n",
    "                    # Count points within radius\n",
    "                    distances_from_point = np.sqrt(np.sum((coordinates - point)**2, axis=1))\n",
    "                    neighbors_in_radius = np.sum(distances_from_point <= 0.1) - 1  # Exclude the point itself\n",
    "                    local_densities.append(neighbors_in_radius)\n",
    "                \n",
    "                clustering_metrics['density_analysis']['local_densities'].extend(local_densities)\n",
    "                \n",
    "                # Identify density hotspots\n",
    "                if local_densities:\n",
    "                    density_threshold = np.percentile(local_densities, 75)\n",
    "                    hotspots = sum(1 for d in local_densities if d >= density_threshold)\n",
    "                    clustering_metrics['density_analysis']['density_hotspots'].append(hotspots)\n",
    "            \n",
    "            # 4. PATTERN DETECTION\n",
    "            \n",
    "            # Detect row patterns using line fitting\n",
    "            if len(coordinates) >= 4:\n",
    "                try:\n",
    "                    # Try to fit lines to detect row patterns\n",
    "                    from sklearn.linear_model import RANSACRegressor\n",
    "                    \n",
    "                    # Sort points by x-coordinate for row detection\n",
    "                    sorted_indices = np.argsort(coordinates[:, 0])\n",
    "                    sorted_coords = coordinates[sorted_indices]\n",
    "                    \n",
    "                    # Try to fit a line\n",
    "                    ransac = RANSACRegressor(random_state=42)\n",
    "                    ransac.fit(sorted_coords[:, 0].reshape(-1, 1), sorted_coords[:, 1])\n",
    "                    \n",
    "                    inlier_mask = ransac.inlier_mask_\n",
    "                    inlier_ratio = np.sum(inlier_mask) / len(coordinates)\n",
    "                    \n",
    "                    if inlier_ratio > 0.6:  # Strong linear pattern\n",
    "                        clustering_metrics['pattern_detection']['row_patterns'] += 1\n",
    "                    elif inlier_ratio > 0.3:  # Some structure\n",
    "                        clustering_metrics['pattern_detection']['grid_patterns'] += 1\n",
    "                    else:\n",
    "                        clustering_metrics['pattern_detection']['random_patterns'] += 1\n",
    "                        \n",
    "                except:\n",
    "                    clustering_metrics['pattern_detection']['random_patterns'] += 1\n",
    "            \n",
    "            # 5. GEOMETRIC FEATURES\n",
    "            \n",
    "            if len(coordinates) >= 3:\n",
    "                # Convex hull analysis\n",
    "                try:\n",
    "                    from scipy.spatial import ConvexHull\n",
    "                    hull = ConvexHull(coordinates)\n",
    "                    hull_area = hull.volume  # In 2D, volume is area\n",
    "                    clustering_metrics['geometric_features']['convex_hull_areas'].append(hull_area)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Bounding box analysis\n",
    "                min_x, min_y = np.min(coordinates, axis=0)\n",
    "                max_x, max_y = np.max(coordinates, axis=0)\n",
    "                bbox_width = max_x - min_x\n",
    "                bbox_height = max_y - min_y\n",
    "                \n",
    "                if bbox_height > 0:\n",
    "                    bbox_ratio = bbox_width / bbox_height\n",
    "                    clustering_metrics['geometric_features']['bounding_box_ratios'].append(bbox_ratio)\n",
    "                \n",
    "                # Centroid analysis\n",
    "                centroid = np.mean(coordinates, axis=0)\n",
    "                distances_to_centroid = np.sqrt(np.sum((coordinates - centroid)**2, axis=1))\n",
    "                avg_distance_to_centroid = np.mean(distances_to_centroid)\n",
    "                clustering_metrics['geometric_features']['centroid_distances'].append(avg_distance_to_centroid)\n",
    "            \n",
    "            # 6. BOUNDARY EFFECTS\n",
    "            edge_threshold = 0.1\n",
    "            near_edge_count = 0\n",
    "            for coord in coordinates:\n",
    "                if (coord[0] <= edge_threshold or coord[0] >= 1-edge_threshold or \n",
    "                    coord[1] <= edge_threshold or coord[1] >= 1-edge_threshold):\n",
    "                    near_edge_count += 1\n",
    "            \n",
    "            boundary_ratio = near_edge_count / len(coordinates) if len(coordinates) > 0 else 0\n",
    "            clustering_metrics['spatial_metrics']['boundary_effects'].append(boundary_ratio)\n",
    "                \n",
    "        except Exception as e:\n",
    "            processing_stats['errors'] += 1\n",
    "            continue\n",
    "    \n",
    "    # Calculate overlap statistics\n",
    "    if clustering_metrics['overlap_analysis']['iou_scores']:\n",
    "        iou_array = np.array(clustering_metrics['overlap_analysis']['iou_scores'])\n",
    "        clustering_metrics['overlap_analysis']['overlap_statistics'] = {\n",
    "            'mean_iou': float(np.mean(iou_array)),\n",
    "            'std_iou': float(np.std(iou_array)),\n",
    "            'max_iou': float(np.max(iou_array)),\n",
    "            'high_overlap_ratio': float(np.sum(iou_array > 0.3) / len(iou_array))\n",
    "        }\n",
    "    \n",
    "    print(f\"   ‚úÖ Processing stats: {processing_stats['success']} success, \"\n",
    "          f\"{processing_stats['errors']} errors, {processing_stats['empty']} empty\")\n",
    "    \n",
    "    return clustering_metrics\n",
    "\n",
    "# Analyze clustering patterns with advanced algorithms\n",
    "clustering_results = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    clustering_metrics = analyze_wheat_clustering_advanced(dataset, name, max_samples=250)\n",
    "    clustering_results[name] = clustering_metrics\n",
    "    \n",
    "    # Display comprehensive clustering analysis\n",
    "    print(f\"\\nüîç {name} - ADVANCED CLUSTERING ANALYSIS:\")\n",
    "    \n",
    "    # Overlap analysis summary\n",
    "    overlap_stats = clustering_metrics['overlap_analysis']['overlap_statistics']\n",
    "    if overlap_stats:\n",
    "        print(f\"   üìä Overlap Analysis:\")\n",
    "        print(f\"     Mean IoU: {overlap_stats['mean_iou']:.3f} ¬± {overlap_stats['std_iou']:.3f}\")\n",
    "        print(f\"     Maximum IoU: {overlap_stats['max_iou']:.3f}\")\n",
    "        print(f\"     High overlap ratio: {overlap_stats['high_overlap_ratio']:.3f}\")\n",
    "        print(f\"     High overlap images: {clustering_metrics['overlap_analysis']['high_overlap_images']}\")\n",
    "    \n",
    "    # Clustering analysis summary\n",
    "    dbscan_results = clustering_metrics['clustering_analysis']['dbscan_results']\n",
    "    if dbscan_results:\n",
    "        avg_clusters = np.mean([r['n_clusters'] for r in dbscan_results])\n",
    "        avg_noise = np.mean([r['n_noise'] for r in dbscan_results])\n",
    "        print(f\"   üîó DBSCAN Clustering:\")\n",
    "        print(f\"     Average clusters per image: {avg_clusters:.2f}\")\n",
    "        print(f\"     Average noise points: {avg_noise:.2f}\")\n",
    "        \n",
    "        if clustering_metrics['clustering_analysis']['silhouette_scores']:\n",
    "            avg_silhouette = np.mean(clustering_metrics['clustering_analysis']['silhouette_scores'])\n",
    "            print(f\"     Average silhouette score: {avg_silhouette:.3f}\")\n",
    "    \n",
    "    # Spatial metrics summary\n",
    "    nn_distances = clustering_metrics['spatial_metrics']['nearest_neighbor_distances']\n",
    "    if nn_distances:\n",
    "        print(f\"   üìè Spatial Metrics:\")\n",
    "        print(f\"     Average nearest neighbor distance: {np.mean(nn_distances):.4f}\")\n",
    "        print(f\"     Min nearest neighbor distance: {min(nn_distances):.4f}\")\n",
    "        \n",
    "        boundary_effects = clustering_metrics['spatial_metrics']['boundary_effects']\n",
    "        if boundary_effects:\n",
    "            avg_boundary = np.mean(boundary_effects)\n",
    "            print(f\"     Average boundary effect ratio: {avg_boundary:.3f}\")\n",
    "    \n",
    "    # Pattern detection summary\n",
    "    pattern_counts = clustering_metrics['pattern_detection']\n",
    "    total_patterns = sum(pattern_counts.values())\n",
    "    if total_patterns > 0:\n",
    "        print(f\"   üéØ Pattern Detection:\")\n",
    "        for pattern, count in pattern_counts.items():\n",
    "            percentage = (count / total_patterns) * 100\n",
    "            print(f\"     {pattern.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Density analysis summary\n",
    "    local_densities = clustering_metrics['density_analysis']['local_densities']\n",
    "    if local_densities:\n",
    "        print(f\"   üåæ Density Analysis:\")\n",
    "        print(f\"     Average local density: {np.mean(local_densities):.2f}\")\n",
    "        print(f\"     Max local density: {max(local_densities)}\")\n",
    "        \n",
    "        hotspots = clustering_metrics['density_analysis']['density_hotspots']\n",
    "        if hotspots:\n",
    "            print(f\"     Average hotspots per image: {np.mean(hotspots):.2f}\")\n",
    "\n",
    "# Create comprehensive clustering visualization dashboard\n",
    "fig = plt.figure(figsize=(24, 20))\n",
    "gs = fig.add_gridspec(5, 4, hspace=0.4, wspace=0.3)\n",
    "\n",
    "dataset_colors = plt.cm.Set1(np.linspace(0, 1, len(clustering_results)))\n",
    "\n",
    "# 1. IoU Distribution (top row, first column)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "for idx, (dataset_name, metrics) in enumerate(clustering_results.items()):\n",
    "    iou_scores = metrics['overlap_analysis']['iou_scores']\n",
    "    if iou_scores:\n",
    "        ax1.hist(iou_scores, bins=30, alpha=0.7, label=dataset_name, \n",
    "                color=dataset_colors[idx], density=True)\n",
    "ax1.set_title('IoU Score Distribution', fontweight='bold')\n",
    "ax1.set_xlabel('Intersection over Union')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Cluster Count Distribution (top row, second column)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "for idx, (dataset_name, metrics) in enumerate(clustering_results.items()):\n",
    "    dbscan_results = metrics['clustering_analysis']['dbscan_results']\n",
    "    if dbscan_results:\n",
    "        cluster_counts = [r['n_clusters'] for r in dbscan_results]\n",
    "        ax2.hist(cluster_counts, bins=range(max(cluster_counts)+2), alpha=0.7,\n",
    "                label=dataset_name, color=dataset_colors[idx])\n",
    "ax2.set_title('Clusters per Image Distribution', fontweight='bold')\n",
    "ax2.set_xlabel('Number of Clusters')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Nearest Neighbor Distances (top row, third column)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "for idx, (dataset_name, metrics) in enumerate(clustering_results.items()):\n",
    "    nn_distances = metrics['spatial_metrics']['nearest_neighbor_distances']\n",
    "    if nn_distances:\n",
    "        ax3.hist(nn_distances, bins=30, alpha=0.7, label=dataset_name,\n",
    "                color=dataset_colors[idx], density=True)\n",
    "ax3.set_title('Nearest Neighbor Distance Distribution', fontweight='bold')\n",
    "ax3.set_xlabel('Distance')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Pattern Detection Summary (top row, fourth column)\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "pattern_data = defaultdict(list)\n",
    "dataset_names = []\n",
    "for dataset_name, metrics in clustering_results.items():\n",
    "    dataset_names.append(dataset_name)\n",
    "    patterns = metrics['pattern_detection']\n",
    "    total = sum(patterns.values()) if sum(patterns.values()) > 0 else 1\n",
    "    for pattern, count in patterns.items():\n",
    "        pattern_data[pattern].append(count / total * 100)\n",
    "\n",
    "x = np.arange(len(dataset_names))\n",
    "width = 0.2\n",
    "pattern_colors = plt.cm.Set2(np.linspace(0, 1, len(pattern_data)))\n",
    "\n",
    "for idx, (pattern, percentages) in enumerate(pattern_data.items()):\n",
    "    ax4.bar(x + idx*width, percentages, width, label=pattern.replace('_', ' ').title(),\n",
    "           color=pattern_colors[idx], alpha=0.8)\n",
    "\n",
    "ax4.set_title('Pattern Detection Results', fontweight='bold')\n",
    "ax4.set_xlabel('Dataset')\n",
    "ax4.set_ylabel('Percentage')\n",
    "ax4.set_xticks(x + width * 1.5)\n",
    "ax4.set_xticklabels([name.split('_')[0] for name in dataset_names], rotation=45)\n",
    "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 5. Local Density Analysis (second row, first two columns)\n",
    "ax5 = fig.add_subplot(gs[1, :2])\n",
    "density_data = []\n",
    "density_labels = []\n",
    "for dataset_name, metrics in clustering_results.items():\n",
    "    local_densities = metrics['density_analysis']['local_densities']\n",
    "    if local_densities:\n",
    "        density_data.extend(local_densities)\n",
    "        density_labels.extend([dataset_name] * len(local_densities))\n",
    "\n",
    "if density_data:\n",
    "    df_density = pd.DataFrame({'Local_Density': density_data, 'Dataset': density_labels})\n",
    "    sns.violinplot(data=df_density, x='Dataset', y='Local_Density', ax=ax5)\n",
    "    ax5.set_title('Local Density Distribution by Dataset', fontweight='bold')\n",
    "    ax5.set_ylabel('Local Density (neighbors within radius)')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Overlap vs Clustering Relationship (second row, third column)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "for idx, (dataset_name, metrics) in enumerate(clustering_results.items()):\n",
    "    overlap_pairs = metrics['overlap_analysis']['overlap_pairs']\n",
    "    dbscan_results = metrics['clustering_analysis']['dbscan_results']\n",
    "    \n",
    "    if overlap_pairs and dbscan_results:\n",
    "        cluster_counts = [r['n_clusters'] for r in dbscan_results]\n",
    "        min_len = min(len(overlap_pairs), len(cluster_counts))\n",
    "        \n",
    "        ax6.scatter(overlap_pairs[:min_len], cluster_counts[:min_len], \n",
    "                   alpha=0.6, label=dataset_name, s=50, color=dataset_colors[idx])\n",
    "\n",
    "ax6.set_title('Overlap vs Clustering Relationship', fontweight='bold')\n",
    "ax6.set_xlabel('Number of Overlapping Pairs')\n",
    "ax6.set_ylabel('Number of Clusters')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Boundary Effects Analysis (second row, fourth column)\n",
    "ax7 = fig.add_subplot(gs[1, 3])\n",
    "boundary_data = []\n",
    "boundary_labels = []\n",
    "for dataset_name, metrics in clustering_results.items():\n",
    "    boundary_effects = metrics['spatial_metrics']['boundary_effects']\n",
    "    if boundary_effects:\n",
    "        boundary_data.extend(boundary_effects)\n",
    "        boundary_labels.extend([dataset_name] * len(boundary_effects))\n",
    "\n",
    "if boundary_data:\n",
    "    df_boundary = pd.DataFrame({'Boundary_Ratio': boundary_data, 'Dataset': boundary_labels})\n",
    "    sns.boxplot(data=df_boundary, x='Dataset', y='Boundary_Ratio', ax=ax7)\n",
    "    ax7.set_title('Boundary Effects Distribution', fontweight='bold')\n",
    "    ax7.set_ylabel('Ratio of Objects Near Boundary')\n",
    "    ax7.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 8. Geometric Features Analysis (third row, spanning two columns)\n",
    "ax8 = fig.add_subplot(gs[2, :2])\n",
    "for idx, (dataset_name, metrics) in enumerate(clustering_results.items()):\n",
    "    bbox_ratios = metrics['geometric_features']['bounding_box_ratios']\n",
    "    centroid_distances = metrics['geometric_features']['centroid_distances']\n",
    "    \n",
    "    if bbox_ratios and centroid_distances:\n",
    "        min_len = min(len(bbox_ratios), len(centroid_distances))\n",
    "        ax8.scatter(bbox_ratios[:min_len], centroid_distances[:min_len],\n",
    "                   alpha=0.6, label=dataset_name, s=50, color=dataset_colors[idx])\n",
    "\n",
    "ax8.set_title('Geometric Features: Bounding Box Ratio vs Centroid Distance', fontweight='bold')\n",
    "ax8.set_xlabel('Bounding Box Aspect Ratio')\n",
    "ax8.set_ylabel('Average Distance to Centroid')\n",
    "ax8.legend()\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Silhouette Score Analysis (third row, third column)\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "silhouette_data = []\n",
    "silhouette_labels = []\n",
    "for dataset_name, metrics in clustering_results.items():\n",
    "    silhouette_scores = metrics['clustering_analysis']['silhouette_scores']\n",
    "    if silhouette_scores:\n",
    "        silhouette_data.extend(silhouette_scores)\n",
    "        silhouette_labels.extend([dataset_name] * len(silhouette_scores))\n",
    "\n",
    "if silhouette_data:\n",
    "    df_silhouette = pd.DataFrame({'Silhouette_Score': silhouette_data, 'Dataset': silhouette_labels})\n",
    "    sns.violinplot(data=df_silhouette, x='Dataset', y='Silhouette_Score', ax=ax9)\n",
    "    ax9.set_title('Clustering Quality (Silhouette Score)', fontweight='bold')\n",
    "    ax9.set_ylabel('Silhouette Score')\n",
    "    ax9.tick_params(axis='x', rotation=45)\n",
    "    ax9.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 10. Comprehensive Statistics Table (third row, fourth column)\n",
    "ax10 = fig.add_subplot(gs[2, 3])\n",
    "ax10.axis('off')\n",
    "\n",
    "stats_text = \"üìä CLUSTERING STATISTICS\\n\\n\"\n",
    "for dataset_name, metrics in clustering_results.items():\n",
    "    stats_text += f\"üåæ {dataset_name}:\\n\"\n",
    "    \n",
    "    # Overlap stats\n",
    "    overlap_stats = metrics['overlap_analysis']['overlap_statistics']\n",
    "    if overlap_stats:\n",
    "        stats_text += f\"  Avg IoU: {overlap_stats['mean_iou']:.3f}\\n\"\n",
    "        stats_text += f\"  High overlap: {overlap_stats['high_overlap_ratio']:.3f}\\n\"\n",
    "    \n",
    "    # Clustering stats\n",
    "    dbscan_results = metrics['clustering_analysis']['dbscan_results']\n",
    "    if dbscan_results:\n",
    "        avg_clusters = np.mean([r['n_clusters'] for r in dbscan_results])\n",
    "        stats_text += f\"  Avg clusters: {avg_clusters:.2f}\\n\"\n",
    "    \n",
    "    # Spatial stats\n",
    "    nn_distances = metrics['spatial_metrics']['nearest_neighbor_distances']\n",
    "    if nn_distances:\n",
    "        stats_text += f\"  Avg NN dist: {np.mean(nn_distances):.4f}\\n\"\n",
    "    \n",
    "    stats_text += \"\\n\"\n",
    "\n",
    "ax10.text(0.05, 0.95, stats_text, transform=ax10.transAxes, fontsize=9,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.suptitle('üîç COMPREHENSIVE WHEAT HEAD CLUSTERING ANALYSIS', fontsize=16, fontweight='bold')\n",
    "plt.savefig(notebook_results_dir / 'visualizations' / 'advanced_clustering_analysis.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save comprehensive clustering results\n",
    "clustering_summary = {}\n",
    "for dataset_name, metrics in clustering_results.items():\n",
    "    clustering_summary[dataset_name] = {\n",
    "        'overlap_analysis': {\n",
    "            'statistics': metrics['overlap_analysis']['overlap_statistics'],\n",
    "            'high_overlap_images': metrics['overlap_analysis']['high_overlap_images'],\n",
    "            'total_iou_scores': len(metrics['overlap_analysis']['iou_scores'])\n",
    "        },\n",
    "        'clustering_analysis': {\n",
    "            'dbscan_summary': {\n",
    "                'avg_clusters': float(np.mean([r['n_clusters'] for r in metrics['clustering_analysis']['dbscan_results']])) if metrics['clustering_analysis']['dbscan_results'] else 0,\n",
    "                'avg_noise': float(np.mean([r['n_noise'] for r in metrics['clustering_analysis']['dbscan_results']])) if metrics['clustering_analysis']['dbscan_results'] else 0,\n",
    "                'total_analyses': len(metrics['clustering_analysis']['dbscan_results'])\n",
    "            },\n",
    "            'silhouette_summary': {\n",
    "                'avg_score': float(np.mean(metrics['clustering_analysis']['silhouette_scores'])) if metrics['clustering_analysis']['silhouette_scores'] else 0,\n",
    "                'total_scores': len(metrics['clustering_analysis']['silhouette_scores'])\n",
    "            }\n",
    "        },\n",
    "        'spatial_metrics': {\n",
    "            'nearest_neighbor': {\n",
    "                'avg_distance': float(np.mean(metrics['spatial_metrics']['nearest_neighbor_distances'])) if metrics['spatial_metrics']['nearest_neighbor_distances'] else 0,\n",
    "                'min_distance': float(min(metrics['spatial_metrics']['nearest_neighbor_distances'])) if metrics['spatial_metrics']['nearest_neighbor_distances'] else 0,\n",
    "                'std_distance': float(np.std(metrics['spatial_metrics']['nearest_neighbor_distances'])) if metrics['spatial_metrics']['nearest_neighbor_distances'] else 0\n",
    "            },\n",
    "            'boundary_effects': {\n",
    "                'avg_ratio': float(np.mean(metrics['spatial_metrics']['boundary_effects'])) if metrics['spatial_metrics']['boundary_effects'] else 0,\n",
    "                'total_analyses': len(metrics['spatial_metrics']['boundary_effects'])\n",
    "            }\n",
    "        },\n",
    "        'pattern_detection': metrics['pattern_detection'],\n",
    "        'density_analysis': {\n",
    "            'local_density': {\n",
    "                'avg_density': float(np.mean(metrics['density_analysis']['local_densities'])) if metrics['density_analysis']['local_densities'] else 0,\n",
    "                'max_density': float(max(metrics['density_analysis']['local_densities'])) if metrics['density_analysis']['local_densities'] else 0,\n",
    "                'total_points': len(metrics['density_analysis']['local_densities'])\n",
    "            },\n",
    "            'hotspots': {\n",
    "                'avg_hotspots': float(np.mean(metrics['density_analysis']['density_hotspots'])) if metrics['density_analysis']['density_hotspots'] else 0,\n",
    "                'total_images': len(metrics['density_analysis']['density_hotspots'])\n",
    "            }\n",
    "        },\n",
    "        'geometric_features': {\n",
    "            'bounding_box': {\n",
    "                'avg_ratio': float(np.mean(metrics['geometric_features']['bounding_box_ratios'])) if metrics['geometric_features']['bounding_box_ratios'] else 0,\n",
    "                'std_ratio': float(np.std(metrics['geometric_features']['bounding_box_ratios'])) if metrics['geometric_features']['bounding_box_ratios'] else 0\n",
    "            },\n",
    "            'convex_hull': {\n",
    "                'avg_area': float(np.mean(metrics['geometric_features']['convex_hull_areas'])) if metrics['geometric_features']['convex_hull_areas'] else 0,\n",
    "                'total_hulls': len(metrics['geometric_features']['convex_hull_areas'])\n",
    "            },\n",
    "            'centroid': {\n",
    "                'avg_distance': float(np.mean(metrics['geometric_features']['centroid_distances'])) if metrics['geometric_features']['centroid_distances'] else 0,\n",
    "                'total_analyses': len(metrics['geometric_features']['centroid_distances'])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(notebook_results_dir / 'data_analysis' / 'advanced_clustering_analysis.json', 'w') as f:\n",
    "    json.dump(clustering_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Advanced clustering analysis saved!\")\n",
    "print(f\"üìÅ Location: {notebook_results_dir / 'data_analysis' / 'advanced_clustering_analysis.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d311b6",
   "metadata": {},
   "source": [
    "## 5. Field Condition and Environmental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced field condition and environmental analysis with computer vision techniques\n",
    "\"\"\"\n",
    "\n",
    "def analyze_field_conditions_advanced(dataset, dataset_name, max_samples=350):\n",
    "    \"\"\"Advanced field condition analysis using computer vision techniques\"\"\"\n",
    "    \n",
    "    field_metrics = {\n",
    "        'illumination_analysis': {\n",
    "            'brightness_values': [],\n",
    "            'contrast_values': [],\n",
    "            'lighting_uniformity': [],\n",
    "            'shadow_detection': [],\n",
    "            'glare_detection': [],\n",
    "            'lighting_categories': {'uniform': 0, 'shadows': 0, 'mixed': 0, 'overexposed': 0, 'underexposed': 0}\n",
    "        },\n",
    "        'texture_analysis': {\n",
    "            'local_binary_patterns': [],\n",
    "            'texture_energy': [],\n",
    "            'texture_homogeneity': [],\n",
    "            'texture_contrast': [],\n",
    "            'edge_density': []\n",
    "        },\n",
    "        'color_analysis': {\n",
    "            'color_histograms': {'r': [], 'g': [], 'b': []},\n",
    "            'color_moments': [],\n",
    "            'saturation_levels': [],\n",
    "            'hue_distributions': [],\n",
    "            'vegetation_indices': []\n",
    "        },\n",
    "        'quality_assessment': {\n",
    "            'sharpness_scores': [],\n",
    "            'noise_levels': [],\n",
    "            'blur_detection': [],\n",
    "            'dynamic_range': [],\n",
    "            'exposure_assessment': []\n",
    "        },\n",
    "        'environmental_factors': {\n",
    "            'weather_indicators': {'clear': 0, 'cloudy': 0, 'harsh_light': 0},\n",
    "            'soil_visibility': [],\n",
    "            'vegetation_density': [],\n",
    "            'field_maturity': [],\n",
    "            'background_complexity': []\n",
    "        },\n",
    "        'spatial_characteristics': {\n",
    "            'gradient_magnitude': [],\n",
    "            'frequency_analysis': [],\n",
    "            'structural_similarity': [],\n",
    "            'feature_density': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    print(f\"üåæ Advanced field condition analysis on {sample_size} images from {dataset_name}...\")\n",
    "    \n",
    "    processing_stats = {'success': 0, 'errors': 0, 'invalid': 0}\n",
    "    \n",
    "    for i in tqdm(indices, desc=\"Analyzing field conditions\"):\n",
    "        try:\n",
    "            image, targets, path = dataset[i]\n",
    "            \n",
    "            # Convert tensor to numpy array\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:  # Normalized to [-1, 1] or similar\n",
    "                    img_np = (img_np + 1) / 2  # Convert to [0, 1]\n",
    "                elif img_np.max() <= 1.0:\n",
    "                    pass  # Already in [0, 1]\n",
    "                else:\n",
    "                    img_np = img_np / 255.0  # Convert from [0, 255]\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "            else:\n",
    "                img_np = image\n",
    "                if img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "            \n",
    "            # Ensure valid image\n",
    "            if img_np.shape[2] != 3:\n",
    "                processing_stats['invalid'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Convert to different color spaces\n",
    "            img_rgb = (img_np * 255).astype(np.uint8)\n",
    "            img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "            img_hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "            img_lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n",
    "            \n",
    "            processing_stats['success'] += 1\n",
    "            \n",
    "            # 1. ADVANCED ILLUMINATION ANALYSIS\n",
    "            \n",
    "            # Basic brightness and contrast\n",
    "            brightness = np.mean(img_gray) / 255.0\n",
    "            contrast = np.std(img_gray) / 255.0\n",
    "            field_metrics['illumination_analysis']['brightness_values'].append(brightness)\n",
    "            field_metrics['illumination_analysis']['contrast_values'].append(contrast)\n",
    "            \n",
    "            # Lighting uniformity using coefficient of variation\n",
    "            lighting_uniformity = np.std(img_gray) / max(np.mean(img_gray), 1)\n",
    "            field_metrics['illumination_analysis']['lighting_uniformity'].append(lighting_uniformity)\n",
    "            \n",
    "            # Shadow detection using morphological operations\n",
    "            kernel = np.ones((5,5), np.uint8)\n",
    "            tophat = cv2.morphologyEx(img_gray, cv2.MORPH_TOPHAT, kernel)\n",
    "            blackhat = cv2.morphologyEx(img_gray, cv2.MORPH_BLACKHAT, kernel)\n",
    "            \n",
    "            shadow_strength = np.mean(blackhat) / 255.0\n",
    "            field_metrics['illumination_analysis']['shadow_detection'].append(shadow_strength)\n",
    "            \n",
    "            # Glare detection using brightness percentiles\n",
    "            bright_pixels = np.percentile(img_gray, 95)\n",
    "            glare_strength = (bright_pixels - np.mean(img_gray)) / 255.0\n",
    "            field_metrics['illumination_analysis']['glare_detection'].append(glare_strength)\n",
    "            \n",
    "            # Categorize lighting conditions\n",
    "            if brightness < 0.25:\n",
    "                field_metrics['illumination_analysis']['lighting_categories']['underexposed'] += 1\n",
    "            elif brightness > 0.8 and glare_strength > 0.3:\n",
    "                field_metrics['illumination_analysis']['lighting_categories']['overexposed'] += 1\n",
    "            elif shadow_strength > 0.1:\n",
    "                field_metrics['illumination_analysis']['lighting_categories']['shadows'] += 1\n",
    "            elif lighting_uniformity > 0.8:\n",
    "                field_metrics['illumination_analysis']['lighting_categories']['mixed'] += 1\n",
    "            else:\n",
    "                field_metrics['illumination_analysis']['lighting_categories']['uniform'] += 1\n",
    "            \n",
    "            # 2. TEXTURE ANALYSIS\n",
    "            \n",
    "            # Local Binary Pattern (simplified implementation)\n",
    "            def calculate_lbp_variance(image):\n",
    "                \"\"\"Calculate LBP variance as texture measure\"\"\"\n",
    "                h, w = image.shape\n",
    "                lbp_var = 0\n",
    "                count = 0\n",
    "                \n",
    "                for i in range(1, h-1):\n",
    "                    for j in range(1, w-1):\n",
    "                        center = image[i, j]\n",
    "                        neighbors = [\n",
    "                            image[i-1, j-1], image[i-1, j], image[i-1, j+1],\n",
    "                            image[i, j+1], image[i+1, j+1], image[i+1, j],\n",
    "                            image[i+1, j-1], image[i, j-1]\n",
    "                        ]\n",
    "                        lbp_var += np.var(neighbors)\n",
    "                        count += 1\n",
    "                \n",
    "                return lbp_var / count if count > 0 else 0\n",
    "            \n",
    "            lbp_variance = calculate_lbp_variance(img_gray)\n",
    "            field_metrics['texture_analysis']['local_binary_patterns'].append(lbp_variance)\n",
    "            \n",
    "            # Haralick-inspired texture features\n",
    "            # Energy (uniformity)\n",
    "            hist, _ = np.histogram(img_gray, bins=256, range=(0, 256))\n",
    "            normalized_hist = hist / np.sum(hist)\n",
    "            energy = np.sum(normalized_hist ** 2)\n",
    "            field_metrics['texture_analysis']['texture_energy'].append(energy)\n",
    "            \n",
    "            # Homogeneity using local patches\n",
    "            patch_size = 16\n",
    "            h, w = img_gray.shape\n",
    "            homogeneity_values = []\n",
    "            \n",
    "            for i in range(0, h-patch_size, patch_size):\n",
    "                for j in range(0, w-patch_size, patch_size):\n",
    "                    patch = img_gray[i:i+patch_size, j:j+patch_size]\n",
    "                    patch_std = np.std(patch)\n",
    "                    homogeneity_values.append(1.0 / (1.0 + patch_std))\n",
    "            \n",
    "            avg_homogeneity = np.mean(homogeneity_values) if homogeneity_values else 0\n",
    "            field_metrics['texture_analysis']['texture_homogeneity'].append(avg_homogeneity)\n",
    "            \n",
    "            # Edge density\n",
    "            edges = cv2.Canny(img_gray, 50, 150)\n",
    "            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])\n",
    "            field_metrics['texture_analysis']['edge_density'].append(edge_density)\n",
    "            \n",
    "            # 3. COLOR ANALYSIS\n",
    "            \n",
    "            # Color channel statistics\n",
    "            for i, channel in enumerate(['r', 'g', 'b']):\n",
    "                channel_mean = np.mean(img_rgb[:, :, i]) / 255.0\n",
    "                field_metrics['color_analysis']['color_histograms'][channel].append(channel_mean)\n",
    "            \n",
    "            # Color moments (mean, std, skewness)\n",
    "            color_moments = []\n",
    "            for i in range(3):\n",
    "                channel = img_rgb[:, :, i] / 255.0\n",
    "                mean_val = np.mean(channel)\n",
    "                std_val = np.std(channel)\n",
    "                color_moments.extend([mean_val, std_val])\n",
    "            \n",
    "            field_metrics['color_analysis']['color_moments'].append(color_moments)\n",
    "            \n",
    "            # Saturation analysis\n",
    "            saturation = img_hsv[:, :, 1]\n",
    "            avg_saturation = np.mean(saturation) / 255.0\n",
    "            field_metrics['color_analysis']['saturation_levels'].append(avg_saturation)\n",
    "            \n",
    "            # Vegetation index (simple NDVI approximation using RGB)\n",
    "            # Approximation: (G - R) / (G + R)\n",
    "            r_channel = img_rgb[:, :, 0].astype(float)\n",
    "            g_channel = img_rgb[:, :, 1].astype(float)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            denominator = g_channel + r_channel\n",
    "            vegetation_mask = denominator > 10  # Avoid very dark pixels\n",
    "            \n",
    "            if np.sum(vegetation_mask) > 0:\n",
    "                vegetation_index = np.mean((g_channel[vegetation_mask] - r_channel[vegetation_mask]) / \n",
    "                                        denominator[vegetation_mask])\n",
    "                field_metrics['color_analysis']['vegetation_indices'].append(vegetation_index)\n",
    "            else:\n",
    "                field_metrics['color_analysis']['vegetation_indices'].append(0)\n",
    "            \n",
    "            # 4. QUALITY ASSESSMENT\n",
    "            \n",
    "            # Sharpness using Laplacian variance\n",
    "            laplacian = cv2.Laplacian(img_gray, cv2.CV_64F)\n",
    "            sharpness = laplacian.var()\n",
    "            field_metrics['quality_assessment']['sharpness_scores'].append(sharpness)\n",
    "            \n",
    "            # Noise estimation using high-frequency content\n",
    "            # Apply Gaussian filter and measure difference\n",
    "            blurred = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
    "            noise_estimate = np.mean(np.abs(img_gray.astype(float) - blurred.astype(float)))\n",
    "            field_metrics['quality_assessment']['noise_levels'].append(noise_estimate)\n",
    "            \n",
    "            # Blur detection using gradient magnitude\n",
    "            grad_x = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            avg_gradient = np.mean(gradient_magnitude)\n",
    "            field_metrics['quality_assessment']['blur_detection'].append(avg_gradient)\n",
    "            \n",
    "            # Dynamic range\n",
    "            dynamic_range = (np.max(img_gray) - np.min(img_gray)) / 255.0\n",
    "            field_metrics['quality_assessment']['dynamic_range'].append(dynamic_range)\n",
    "            \n",
    "            # 5. ENVIRONMENTAL FACTORS\n",
    "            \n",
    "            # Weather indicators based on brightness distribution\n",
    "            brightness_std = np.std(img_gray) / 255.0\n",
    "            brightness_mean = np.mean(img_gray) / 255.0\n",
    "            \n",
    "            if brightness_std < 0.15 and brightness_mean > 0.7:\n",
    "                field_metrics['environmental_factors']['weather_indicators']['harsh_light'] += 1\n",
    "            elif brightness_std > 0.25:\n",
    "                field_metrics['environmental_factors']['weather_indicators']['cloudy'] += 1\n",
    "            else:\n",
    "                field_metrics['environmental_factors']['weather_indicators']['clear'] += 1\n",
    "            \n",
    "            # Soil visibility estimation using brown/tan color detection\n",
    "            # Convert to HSV and look for soil-like colors\n",
    "            hue = img_hsv[:, :, 0]\n",
    "            sat = img_hsv[:, :, 1]\n",
    "            val = img_hsv[:, :, 2]\n",
    "            \n",
    "            # Soil typically has hue in brown range (10-30 in HSV) and low saturation\n",
    "            soil_mask = ((hue >= 10) & (hue <= 30) & (sat < 128) & (val > 50)) | (sat < 50)\n",
    "            soil_ratio = np.sum(soil_mask) / soil_mask.size\n",
    "            field_metrics['environmental_factors']['soil_visibility'].append(soil_ratio)\n",
    "            \n",
    "            # Vegetation density using green channel dominance\n",
    "            green_dominance = (g_channel > r_channel) & (g_channel > img_rgb[:, :, 2])\n",
    "            vegetation_ratio = np.sum(green_dominance) / green_dominance.size\n",
    "            field_metrics['environmental_factors']['vegetation_density'].append(vegetation_ratio)\n",
    "            \n",
    "            # Field maturity estimation using color analysis\n",
    "            # Young crops: more green, mature crops: more yellow/brown\n",
    "            yellow_pixels = ((hue >= 15) & (hue <= 35) & (sat > 50))\n",
    "            maturity_indicator = np.sum(yellow_pixels) / yellow_pixels.size\n",
    "            field_metrics['environmental_factors']['field_maturity'].append(maturity_indicator)\n",
    "            \n",
    "            # Background complexity using frequency analysis\n",
    "            f_transform = np.fft.fft2(img_gray)\n",
    "            f_shift = np.fft.fftshift(f_transform)\n",
    "            magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n",
    "            complexity_score = np.std(magnitude_spectrum)\n",
    "            field_metrics['environmental_factors']['background_complexity'].append(complexity_score)\n",
    "            \n",
    "            # 6. SPATIAL CHARACTERISTICS\n",
    "            \n",
    "            # Gradient magnitude statistics\n",
    "            field_metrics['spatial_characteristics']['gradient_magnitude'].append(np.mean(gradient_magnitude))\n",
    "            \n",
    "            # Frequency analysis - high frequency content\n",
    "            high_freq_threshold = magnitude_spectrum.shape[0] // 4\n",
    "            center_x, center_y = magnitude_spectrum.shape[0] // 2, magnitude_spectrum.shape[1] // 2\n",
    "            \n",
    "            # Create mask for high frequencies\n",
    "            y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]\n",
    "            mask = ((x - center_x)**2 + (y - center_y)**2) > high_freq_threshold**2\n",
    "            \n",
    "            high_freq_energy = np.mean(magnitude_spectrum[mask])\n",
    "            field_metrics['spatial_characteristics']['frequency_analysis'].append(high_freq_energy)\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_stats['errors'] += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"   ‚úÖ Processing stats: {processing_stats['success']} success, \"\n",
    "          f\"{processing_stats['errors']} errors, {processing_stats['invalid']} invalid\")\n",
    "    \n",
    "    return field_metrics\n",
    "\n",
    "# Analyze field conditions with advanced techniques\n",
    "field_condition_results = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    field_metrics = analyze_field_conditions_advanced(dataset, name, max_samples=300)\n",
    "    field_condition_results[name] = field_metrics\n",
    "    \n",
    "    # Display comprehensive field analysis\n",
    "    print(f\"\\nüåæ {name} - ADVANCED FIELD CONDITION ANALYSIS:\")\n",
    "    \n",
    "    # Illumination analysis summary\n",
    "    illumination = field_metrics['illumination_analysis']\n",
    "    if illumination['brightness_values']:\n",
    "        print(f\"   üí° Illumination Analysis:\")\n",
    "        print(f\"     Brightness: {np.mean(illumination['brightness_values']):.3f} ¬± {np.std(illumination['brightness_values']):.3f}\")\n",
    "        print(f\"     Contrast: {np.mean(illumination['contrast_values']):.3f} ¬± {np.std(illumination['contrast_values']):.3f}\")\n",
    "        print(f\"     Lighting uniformity: {np.mean(illumination['lighting_uniformity']):.3f}\")\n",
    "        print(f\"     Shadow strength: {np.mean(illumination['shadow_detection']):.3f}\")\n",
    "        print(f\"     Glare strength: {np.mean(illumination['glare_detection']):.3f}\")\n",
    "        \n",
    "        print(f\"   üå§Ô∏è Lighting conditions:\")\n",
    "        total_images = sum(illumination['lighting_categories'].values())\n",
    "        for condition, count in illumination['lighting_categories'].items():\n",
    "            percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "            print(f\"     {condition.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Texture analysis summary\n",
    "    texture = field_metrics['texture_analysis']\n",
    "    if texture['texture_energy']:\n",
    "        print(f\"   üé® Texture Analysis:\")\n",
    "        print(f\"     Texture energy: {np.mean(texture['texture_energy']):.4f}\")\n",
    "        print(f\"     Homogeneity: {np.mean(texture['texture_homogeneity']):.4f}\")\n",
    "        print(f\"     Edge density: {np.mean(texture['edge_density']):.4f}\")\n",
    "        print(f\"     LBP variance: {np.mean(texture['local_binary_patterns']):.2f}\")\n",
    "    \n",
    "    # Color analysis summary\n",
    "    color = field_metrics['color_analysis']\n",
    "    if color['saturation_levels']:\n",
    "        print(f\"   üé® Color Analysis:\")\n",
    "        print(f\"     Average saturation: {np.mean(color['saturation_levels']):.3f}\")\n",
    "        print(f\"     Vegetation index: {np.mean(color['vegetation_indices']):.3f}\")\n",
    "        \n",
    "        # Color channel analysis\n",
    "        r_avg = np.mean(color['color_histograms']['r'])\n",
    "        g_avg = np.mean(color['color_histograms']['g'])\n",
    "        b_avg = np.mean(color['color_histograms']['b'])\n",
    "        print(f\"     RGB averages: R={r_avg:.3f}, G={g_avg:.3f}, B={b_avg:.3f}\")\n",
    "    \n",
    "    # Quality assessment summary\n",
    "    quality = field_metrics['quality_assessment']\n",
    "    if quality['sharpness_scores']:\n",
    "        print(f\"   üì∑ Quality Assessment:\")\n",
    "        print(f\"     Sharpness score: {np.mean(quality['sharpness_scores']):.2f}\")\n",
    "        print(f\"     Noise level: {np.mean(quality['noise_levels']):.2f}\")\n",
    "        print(f\"     Blur metric: {np.mean(quality['blur_detection']):.2f}\")\n",
    "        print(f\"     Dynamic range: {np.mean(quality['dynamic_range']):.3f}\")\n",
    "    \n",
    "    # Environmental factors summary\n",
    "    environment = field_metrics['environmental_factors']\n",
    "    if environment['soil_visibility']:\n",
    "        print(f\"   üå± Environmental Analysis:\")\n",
    "        print(f\"     Soil visibility: {np.mean(environment['soil_visibility']):.3f}\")\n",
    "        print(f\"     Vegetation density: {np.mean(environment['vegetation_density']):.3f}\")\n",
    "        print(f\"     Field maturity: {np.mean(environment['field_maturity']):.3f}\")\n",
    "        print(f\"     Background complexity: {np.mean(environment['background_complexity']):.2f}\")\n",
    "        \n",
    "        print(f\"   ‚òÄÔ∏è Weather indicators:\")\n",
    "        total_weather = sum(environment['weather_indicators'].values())\n",
    "        for weather, count in environment['weather_indicators'].items():\n",
    "            percentage = (count / total_weather * 100) if total_weather > 0 else 0\n",
    "            print(f\"     {weather.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Create comprehensive field condition visualization dashboard\n",
    "fig = plt.figure(figsize=(24, 20))\n",
    "gs = fig.add_gridspec(5, 4, hspace=0.4, wspace=0.3)\n",
    "\n",
    "dataset_colors = plt.cm.Set1(np.linspace(0, 1, len(field_condition_results)))\n",
    "\n",
    "# 1. Brightness and Contrast Distribution (top row, first column)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "for idx, (dataset_name, metrics) in enumerate(field_condition_results.items()):\n",
    "    brightness = metrics['illumination_analysis']['brightness_values']\n",
    "    contrast = metrics['illumination_analysis']['contrast_values']\n",
    "    if brightness and contrast:\n",
    "        ax1.scatter(brightness, contrast, alpha=0.6, label=dataset_name, \n",
    "                   s=30, color=dataset_colors[idx])\n",
    "ax1.set_title('Brightness vs Contrast Analysis', fontweight='bold')\n",
    "ax1.set_xlabel('Brightness')\n",
    "ax1.set_ylabel('Contrast')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Lighting Conditions Distribution (top row, second column)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "lighting_categories = ['uniform', 'shadows', 'mixed', 'overexposed', 'underexposed']\n",
    "x_pos = np.arange(len(lighting_categories))\n",
    "width = 0.8 / len(field_condition_results)\n",
    "\n",
    "for idx, (dataset_name, metrics) in enumerate(field_condition_results.items()):\n",
    "    lighting_data = metrics['illumination_analysis']['lighting_categories']\n",
    "    values = [lighting_data.get(cat, 0) for cat in lighting_categories]\n",
    "    ax2.bar(x_pos + idx*width, values, width, label=dataset_name, \n",
    "           alpha=0.8, color=dataset_colors[idx])\n",
    "\n",
    "ax2.set_title('Lighting Conditions Distribution', fontweight='bold')\n",
    "ax2.set_xlabel('Lighting Condition')\n",
    "ax2.set_ylabel('Number of Images')\n",
    "ax2.set_xticks(x_pos + width/2)\n",
    "ax2.set_xticklabels([cat.replace('_', '\\n') for cat in lighting_categories])\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. Texture Analysis (top row, third column)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "for idx, (dataset_name, metrics) in enumerate(field_condition_results.items()):\n",
    "    energy = metrics['texture_analysis']['texture_energy']\n",
    "    homogeneity = metrics['texture_analysis']['texture_homogeneity']\n",
    "    if energy and homogeneity:\n",
    "        ax3.scatter(energy, homogeneity, alpha=0.6, label=dataset_name,\n",
    "                   s=30, color=dataset_colors[idx])\n",
    "ax3.set_title('Texture Energy vs Homogeneity', fontweight='bold')\n",
    "ax3.set_xlabel('Texture Energy')\n",
    "ax3.set_ylabel('Texture Homogeneity')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Quality Assessment Radar Chart (top row, fourth column)\n",
    "ax4 = fig.add_subplot(gs[0, 3], projection='polar')\n",
    "quality_metrics = ['Sharpness', 'Low Noise', 'No Blur', 'Dynamic Range']\n",
    "angles = np.linspace(0, 2*np.pi, len(quality_metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "for idx, (dataset_name, metrics) in enumerate(field_condition_results.items()):\n",
    "    quality = metrics['quality_assessment']\n",
    "    if quality['sharpness_scores']:\n",
    "        # Normalize metrics to 0-1 scale\n",
    "        sharpness_norm = min(np.mean(quality['sharpness_scores']) / 1000, 1)\n",
    "        noise_norm = max(0, 1 - np.mean(quality['noise_levels']) / 50)  # Invert noise\n",
    "        blur_norm = min(np.mean(quality['blur_detection']) / 100, 1)\n",
    "        range_norm = np.mean(quality['dynamic_range'])\n",
    "        \n",
    "        values = [sharpness_norm, noise_norm, blur_norm, range_norm]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax4.plot(angles, values, 'o-', linewidth=2, label=dataset_name, \n",
    "                color=dataset_colors[idx])\n",
    "        ax4.fill(angles, values, alpha=0.25, color=dataset_colors[idx])\n",
    "\n",
    "ax4.set_xticks(angles[:-1])\n",
    "ax4.set_xticklabels(quality_metrics)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.set_title('Quality Assessment Profile', fontweight='bold', pad=20)\n",
    "ax4.legend(bbox_to_anchor=(1.3, 1), loc='upper left')\n",
    "\n",
    "# 5. Color Analysis (second row, first two columns)\n",
    "ax5 = fig.add_subplot(gs[1, :2])\n",
    "for idx, (dataset_name, metrics) in enumerate(field_condition_results.items()):\n",
    "    color_data = metrics['color_analysis']['color_histograms']\n",
    "    if all(color_data[c] for c in ['r', 'g', 'b']):\n",
    "        r_vals = color_data['r']\n",
    "        g_vals = color_data['g']\n",
    "        b_vals = color_data['b']\n",
    "        \n",
    "        # Create RGB color space plot\n",
    "        ax5.scatter(r_vals, g_vals, c=dataset_colors[idx], alpha=0.6, \n",
    "                   s=30, label=f'{dataset_name} (R vs G)')\n",
    "\n",
    "ax5.set_title('Color Space Analysis (Red vs Green Channels)', fontweight='bold')\n",
    "ax5.set_xlabel('Red Channel Average')\n",
    "ax5.set_ylabel('Green Channel Average')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Vegetation Analysis (second row, third column)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "vegetation_data = []\n",
    "vegetation_labels = []\n",
    "saturation_data = []\n",
    "saturation_labels = []\n",
    "\n",
    "for dataset_name, metrics in field_condition_results.items():\n",
    "    veg_indices = metrics['color_analysis']['vegetation_indices']\n",
    "    sat_levels = metrics['color_analysis']['saturation_levels']\n",
    "    \n",
    "    if veg_indices:\n",
    "        vegetation_data.extend(veg_indices)\n",
    "        vegetation_labels.extend([dataset_name] * len(veg_indices))\n",
    "    \n",
    "    if sat_levels:\n",
    "        saturation_data.extend(sat_levels)\n",
    "        saturation_labels.extend([dataset_name] * len(sat_levels))\n",
    "\n",
    "if vegetation_data and saturation_data:\n",
    "    # Create combined plot\n",
    "    min_len = min(len(vegetation_data), len(saturation_data))\n",
    "    ax6.scatter(vegetation_data[:min_len], saturation_data[:min_len], \n",
    "               alpha=0.6, s=30, c='green')\n",
    "    ax6.set_title('Vegetation Index vs Saturation', fontweight='bold')\n",
    "    ax6.set_xlabel('Vegetation Index')\n",
    "    ax6.set_ylabel('Saturation Level')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Environmental Factors (second row, fourth column)\n",
    "ax7 = fig.add_subplot(gs[1, 3])\n",
    "env_factors = ['soil_visibility', 'vegetation_density', 'field_maturity']\n",
    "env_data = {factor: [] for factor in env_factors}\n",
    "env_labels = []\n",
    "\n",
    "for dataset_name, metrics in field_condition_results.items():\n",
    "    env_metrics = metrics['environmental_factors']\n",
    "    for factor in env_factors:\n",
    "        if env_metrics[factor]:\n",
    "            env_data[factor].extend(env_metrics[factor])\n",
    "            if factor == 'soil_visibility':  # Only add labels once\n",
    "                env_labels.extend([dataset_name] * len(env_metrics[factor]))\n",
    "\n",
    "# Create box plots for environmental factors\n",
    "positions = []\n",
    "box_data = []\n",
    "box_labels = []\n",
    "\n",
    "for i, (factor, data) in enumerate(env_data.items()):\n",
    "    if data:\n",
    "        positions.append(i)\n",
    "        box_data.append(data)\n",
    "        box_labels.append(factor.replace('_', '\\n').title())\n",
    "\n",
    "if box_data:\n",
    "    bp = ax7.boxplot(box_data, positions=positions, labels=box_labels, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(bp['boxes'])))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "ax7.set_title('Environmental Factors Distribution', fontweight='bold')\n",
    "ax7.set_ylabel('Ratio/Score')\n",
    "ax7.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 8. Weather Indicators (third row, first column)\n",
    "ax8 = fig.add_subplot(gs[2, 0])\n",
    "weather_summary = defaultdict(int)\n",
    "for dataset_name, metrics in field_condition_results.items():\n",
    "    weather_data = metrics['environmental_factors']['weather_indicators']\n",
    "    for weather, count in weather_data.items():\n",
    "        weather_summary[weather] += count\n",
    "\n",
    "if weather_summary:\n",
    "    labels = list(weather_summary.keys())\n",
    "    sizes = list(weather_summary.values())\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(labels)))\n",
    "    \n",
    "    wedges, texts, autotexts = ax8.pie(sizes, labels=[l.replace('_', '\\n') for l in labels], \n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax8.set_title('Weather Conditions Distribution', fontweight='bold')\n",
    "\n",
    "# 9. Spatial Characteristics (third row, second column)\n",
    "ax9 = fig.add_subplot(gs[2, 1])\n",
    "for idx, (dataset_name, metrics) in enumerate(field_condition_results.items()):\n",
    "    gradient_mag = metrics['spatial_characteristics']['gradient_magnitude']\n",
    "    freq_analysis = metrics['spatial_characteristics']['frequency_analysis']\n",
    "    \n",
    "    if gradient_mag and freq_analysis:\n",
    "        ax9.scatter(gradient_mag, freq_analysis, alpha=0.6, label=dataset_name,\n",
    "                   s=30, color=dataset_colors[idx])\n",
    "\n",
    "ax9.set_title('Spatial Characteristics', fontweight='bold')\n",
    "ax9.set_xlabel('Gradient Magnitude')\n",
    "ax9.set_ylabel('High Frequency Energy')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "# 10. Quality Metrics Distribution (third row, third and fourth columns)\n",
    "ax10 = fig.add_subplot(gs[2, 2:])\n",
    "quality_metrics_data = {}\n",
    "for dataset_name, metrics in field_condition_results.items():\n",
    "    quality = metrics['quality_assessment']\n",
    "    quality_metrics_data[dataset_name] = {\n",
    "        'sharpness': quality['sharpness_scores'],\n",
    "        'noise': quality['noise_levels'],\n",
    "        'blur': quality['blur_detection'],\n",
    "        'dynamic_range': quality['dynamic_range']\n",
    "    }\n",
    "\n",
    "# Create subplots for each quality metric\n",
    "metric_names = ['sharpness', 'noise', 'blur', 'dynamic_range']\n",
    "for i, metric in enumerate(metric_names):\n",
    "   for idx, (dataset_name, data) in enumerate(quality_metrics_data.items()):\n",
    "       if data[metric]:\n",
    "           # Normalize position for each metric group\n",
    "           positions = np.random.normal(i, 0.04, len(data[metric]))\n",
    "           ax10.scatter(positions, data[metric], alpha=0.6, s=20, \n",
    "                       color=dataset_colors[idx], label=dataset_name if i == 0 else \"\")\n",
    "\n",
    "ax10.set_title('Quality Metrics Distribution', fontweight='bold')\n",
    "ax10.set_xlabel('Quality Metric')\n",
    "ax10.set_ylabel('Score')\n",
    "ax10.set_xticks(range(len(metric_names)))\n",
    "ax10.set_xticklabels([name.replace('_', '\\n').title() for name in metric_names])\n",
    "if quality_metrics_data:\n",
    "   ax10.legend()\n",
    "ax10.grid(True, alpha=0.3)\n",
    "\n",
    "# 11. Comprehensive Statistics Summary (fourth and fifth rows)\n",
    "ax11 = fig.add_subplot(gs[3:, :])\n",
    "ax11.axis('off')\n",
    "\n",
    "# Create comprehensive statistics table\n",
    "stats_text = \"üìä COMPREHENSIVE FIELD CONDITION STATISTICS\\n\\n\"\n",
    "\n",
    "for dataset_name, metrics in field_condition_results.items():\n",
    "   stats_text += f\"üåæ {dataset_name}:\\n\"\n",
    "   \n",
    "   # Illumination stats\n",
    "   illum = metrics['illumination_analysis']\n",
    "   if illum['brightness_values']:\n",
    "       stats_text += f\"  üí° Illumination:\\n\"\n",
    "       stats_text += f\"    Brightness: {np.mean(illum['brightness_values']):.3f} ¬± {np.std(illum['brightness_values']):.3f}\\n\"\n",
    "       stats_text += f\"    Contrast: {np.mean(illum['contrast_values']):.3f} ¬± {np.std(illum['contrast_values']):.3f}\\n\"\n",
    "       stats_text += f\"    Shadow strength: {np.mean(illum['shadow_detection']):.3f}\\n\"\n",
    "       stats_text += f\"    Glare strength: {np.mean(illum['glare_detection']):.3f}\\n\"\n",
    "   \n",
    "   # Texture stats\n",
    "   texture = metrics['texture_analysis']\n",
    "   if texture['texture_energy']:\n",
    "       stats_text += f\"  üé® Texture:\\n\"\n",
    "       stats_text += f\"    Energy: {np.mean(texture['texture_energy']):.4f}\\n\"\n",
    "       stats_text += f\"    Homogeneity: {np.mean(texture['texture_homogeneity']):.4f}\\n\"\n",
    "       stats_text += f\"    Edge density: {np.mean(texture['edge_density']):.4f}\\n\"\n",
    "   \n",
    "   # Color stats\n",
    "   color = metrics['color_analysis']\n",
    "   if color['saturation_levels']:\n",
    "       stats_text += f\"  üåà Color:\\n\"\n",
    "       stats_text += f\"    Saturation: {np.mean(color['saturation_levels']):.3f}\\n\"\n",
    "       stats_text += f\"    Vegetation index: {np.mean(color['vegetation_indices']):.3f}\\n\"\n",
    "   \n",
    "   # Quality stats\n",
    "   quality = metrics['quality_assessment']\n",
    "   if quality['sharpness_scores']:\n",
    "       stats_text += f\"  üì∑ Quality:\\n\"\n",
    "       stats_text += f\"    Sharpness: {np.mean(quality['sharpness_scores']):.1f}\\n\"\n",
    "       stats_text += f\"    Noise: {np.mean(quality['noise_levels']):.2f}\\n\"\n",
    "       stats_text += f\"    Dynamic range: {np.mean(quality['dynamic_range']):.3f}\\n\"\n",
    "   \n",
    "   # Environmental stats\n",
    "   env = metrics['environmental_factors']\n",
    "   if env['soil_visibility']:\n",
    "       stats_text += f\"  üå± Environment:\\n\"\n",
    "       stats_text += f\"    Soil visibility: {np.mean(env['soil_visibility']):.3f}\\n\"\n",
    "       stats_text += f\"    Vegetation density: {np.mean(env['vegetation_density']):.3f}\\n\"\n",
    "       stats_text += f\"    Field maturity: {np.mean(env['field_maturity']):.3f}\\n\"\n",
    "   \n",
    "   stats_text += \"\\n\"\n",
    "\n",
    "ax11.text(0.02, 0.98, stats_text, transform=ax11.transAxes, fontsize=9,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))\n",
    "\n",
    "plt.suptitle('üåæ COMPREHENSIVE FIELD CONDITION & ENVIRONMENTAL ANALYSIS', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.savefig(notebook_results_dir / 'visualizations' / 'comprehensive_field_analysis.png', \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save comprehensive field condition results\n",
    "field_condition_summary = {}\n",
    "for dataset_name, metrics in field_condition_results.items():\n",
    "   field_condition_summary[dataset_name] = {\n",
    "       'illumination_analysis': {\n",
    "           'brightness_stats': {\n",
    "               'mean': float(np.mean(metrics['illumination_analysis']['brightness_values'])) if metrics['illumination_analysis']['brightness_values'] else 0,\n",
    "               'std': float(np.std(metrics['illumination_analysis']['brightness_values'])) if metrics['illumination_analysis']['brightness_values'] else 0,\n",
    "               'min': float(min(metrics['illumination_analysis']['brightness_values'])) if metrics['illumination_analysis']['brightness_values'] else 0,\n",
    "               'max': float(max(metrics['illumination_analysis']['brightness_values'])) if metrics['illumination_analysis']['brightness_values'] else 0\n",
    "           },\n",
    "           'contrast_stats': {\n",
    "               'mean': float(np.mean(metrics['illumination_analysis']['contrast_values'])) if metrics['illumination_analysis']['contrast_values'] else 0,\n",
    "               'std': float(np.std(metrics['illumination_analysis']['contrast_values'])) if metrics['illumination_analysis']['contrast_values'] else 0\n",
    "           },\n",
    "           'lighting_uniformity': {\n",
    "               'mean': float(np.mean(metrics['illumination_analysis']['lighting_uniformity'])) if metrics['illumination_analysis']['lighting_uniformity'] else 0,\n",
    "               'std': float(np.std(metrics['illumination_analysis']['lighting_uniformity'])) if metrics['illumination_analysis']['lighting_uniformity'] else 0\n",
    "           },\n",
    "           'shadow_detection': {\n",
    "               'mean': float(np.mean(metrics['illumination_analysis']['shadow_detection'])) if metrics['illumination_analysis']['shadow_detection'] else 0,\n",
    "               'std': float(np.std(metrics['illumination_analysis']['shadow_detection'])) if metrics['illumination_analysis']['shadow_detection'] else 0\n",
    "           },\n",
    "           'glare_detection': {\n",
    "               'mean': float(np.mean(metrics['illumination_analysis']['glare_detection'])) if metrics['illumination_analysis']['glare_detection'] else 0,\n",
    "               'std': float(np.std(metrics['illumination_analysis']['glare_detection'])) if metrics['illumination_analysis']['glare_detection'] else 0\n",
    "           },\n",
    "           'lighting_categories': metrics['illumination_analysis']['lighting_categories']\n",
    "       },\n",
    "       'texture_analysis': {\n",
    "           'texture_energy': {\n",
    "               'mean': float(np.mean(metrics['texture_analysis']['texture_energy'])) if metrics['texture_analysis']['texture_energy'] else 0,\n",
    "               'std': float(np.std(metrics['texture_analysis']['texture_energy'])) if metrics['texture_analysis']['texture_energy'] else 0\n",
    "           },\n",
    "           'texture_homogeneity': {\n",
    "               'mean': float(np.mean(metrics['texture_analysis']['texture_homogeneity'])) if metrics['texture_analysis']['texture_homogeneity'] else 0,\n",
    "               'std': float(np.std(metrics['texture_analysis']['texture_homogeneity'])) if metrics['texture_analysis']['texture_homogeneity'] else 0\n",
    "           },\n",
    "           'edge_density': {\n",
    "               'mean': float(np.mean(metrics['texture_analysis']['edge_density'])) if metrics['texture_analysis']['edge_density'] else 0,\n",
    "               'std': float(np.std(metrics['texture_analysis']['edge_density'])) if metrics['texture_analysis']['edge_density'] else 0\n",
    "           },\n",
    "           'lbp_variance': {\n",
    "               'mean': float(np.mean(metrics['texture_analysis']['local_binary_patterns'])) if metrics['texture_analysis']['local_binary_patterns'] else 0,\n",
    "               'std': float(np.std(metrics['texture_analysis']['local_binary_patterns'])) if metrics['texture_analysis']['local_binary_patterns'] else 0\n",
    "           }\n",
    "       },\n",
    "       'color_analysis': {\n",
    "           'saturation_stats': {\n",
    "               'mean': float(np.mean(metrics['color_analysis']['saturation_levels'])) if metrics['color_analysis']['saturation_levels'] else 0,\n",
    "               'std': float(np.std(metrics['color_analysis']['saturation_levels'])) if metrics['color_analysis']['saturation_levels'] else 0\n",
    "           },\n",
    "           'vegetation_index': {\n",
    "               'mean': float(np.mean(metrics['color_analysis']['vegetation_indices'])) if metrics['color_analysis']['vegetation_indices'] else 0,\n",
    "               'std': float(np.std(metrics['color_analysis']['vegetation_indices'])) if metrics['color_analysis']['vegetation_indices'] else 0\n",
    "           },\n",
    "           'rgb_averages': {\n",
    "               'red': float(np.mean(metrics['color_analysis']['color_histograms']['r'])) if metrics['color_analysis']['color_histograms']['r'] else 0,\n",
    "               'green': float(np.mean(metrics['color_analysis']['color_histograms']['g'])) if metrics['color_analysis']['color_histograms']['g'] else 0,\n",
    "               'blue': float(np.mean(metrics['color_analysis']['color_histograms']['b'])) if metrics['color_analysis']['color_histograms']['b'] else 0\n",
    "           }\n",
    "       },\n",
    "       'quality_assessment': {\n",
    "           'sharpness_stats': {\n",
    "               'mean': float(np.mean(metrics['quality_assessment']['sharpness_scores'])) if metrics['quality_assessment']['sharpness_scores'] else 0,\n",
    "               'std': float(np.std(metrics['quality_assessment']['sharpness_scores'])) if metrics['quality_assessment']['sharpness_scores'] else 0\n",
    "           },\n",
    "           'noise_stats': {\n",
    "               'mean': float(np.mean(metrics['quality_assessment']['noise_levels'])) if metrics['quality_assessment']['noise_levels'] else 0,\n",
    "               'std': float(np.std(metrics['quality_assessment']['noise_levels'])) if metrics['quality_assessment']['noise_levels'] else 0\n",
    "           },\n",
    "           'blur_stats': {\n",
    "               'mean': float(np.mean(metrics['quality_assessment']['blur_detection'])) if metrics['quality_assessment']['blur_detection'] else 0,\n",
    "               'std': float(np.std(metrics['quality_assessment']['blur_detection'])) if metrics['quality_assessment']['blur_detection'] else 0\n",
    "           },\n",
    "           'dynamic_range_stats': {\n",
    "               'mean': float(np.mean(metrics['quality_assessment']['dynamic_range'])) if metrics['quality_assessment']['dynamic_range'] else 0,\n",
    "               'std': float(np.std(metrics['quality_assessment']['dynamic_range'])) if metrics['quality_assessment']['dynamic_range'] else 0\n",
    "           }\n",
    "       },\n",
    "       'environmental_factors': {\n",
    "           'soil_visibility': {\n",
    "               'mean': float(np.mean(metrics['environmental_factors']['soil_visibility'])) if metrics['environmental_factors']['soil_visibility'] else 0,\n",
    "               'std': float(np.std(metrics['environmental_factors']['soil_visibility'])) if metrics['environmental_factors']['soil_visibility'] else 0\n",
    "           },\n",
    "           'vegetation_density': {\n",
    "               'mean': float(np.mean(metrics['environmental_factors']['vegetation_density'])) if metrics['environmental_factors']['vegetation_density'] else 0,\n",
    "               'std': float(np.std(metrics['environmental_factors']['vegetation_density'])) if metrics['environmental_factors']['vegetation_density'] else 0\n",
    "           },\n",
    "           'field_maturity': {\n",
    "               'mean': float(np.mean(metrics['environmental_factors']['field_maturity'])) if metrics['environmental_factors']['field_maturity'] else 0,\n",
    "               'std': float(np.std(metrics['environmental_factors']['field_maturity'])) if metrics['environmental_factors']['field_maturity'] else 0\n",
    "           },\n",
    "           'background_complexity': {\n",
    "               'mean': float(np.mean(metrics['environmental_factors']['background_complexity'])) if metrics['environmental_factors']['background_complexity'] else 0,\n",
    "               'std': float(np.std(metrics['environmental_factors']['background_complexity'])) if metrics['environmental_factors']['background_complexity'] else 0\n",
    "           },\n",
    "           'weather_indicators': metrics['environmental_factors']['weather_indicators']\n",
    "       },\n",
    "       'spatial_characteristics': {\n",
    "           'gradient_magnitude': {\n",
    "               'mean': float(np.mean(metrics['spatial_characteristics']['gradient_magnitude'])) if metrics['spatial_characteristics']['gradient_magnitude'] else 0,\n",
    "               'std': float(np.std(metrics['spatial_characteristics']['gradient_magnitude'])) if metrics['spatial_characteristics']['gradient_magnitude'] else 0\n",
    "           },\n",
    "           'frequency_analysis': {\n",
    "               'mean': float(np.mean(metrics['spatial_characteristics']['frequency_analysis'])) if metrics['spatial_characteristics']['frequency_analysis'] else 0,\n",
    "               'std': float(np.std(metrics['spatial_characteristics']['frequency_analysis'])) if metrics['spatial_characteristics']['frequency_analysis'] else 0\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "\n",
    "with open(notebook_results_dir / 'data_analysis' / 'comprehensive_field_conditions.json', 'w') as f:\n",
    "   json.dump(field_condition_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Comprehensive field condition analysis saved!\")\n",
    "print(f\"üìÅ Location: {notebook_results_dir / 'data_analysis' / 'comprehensive_field_conditions.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2810981",
   "metadata": {},
   "source": [
    "## 6. Wheat Head Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced wheat head sample visualization with detailed analysis and annotations\n",
    "\"\"\"\n",
    "\n",
    "def visualize_wheat_samples_advanced(dataset, dataset_name, num_samples=12):\n",
    "    \"\"\"Advanced visualization of wheat samples with detailed annotations and analysis\"\"\"\n",
    "    \n",
    "    if not hasattr(dataset, '__getitem__'):\n",
    "        print(f\"Skipping {dataset_name} - incompatible dataset format\")\n",
    "        return\n",
    "    \n",
    "    # Intelligent sample selection for diverse representation\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Strategy: Select samples with different wheat head densities\n",
    "    sample_indices = []\n",
    "    density_targets = [0, 5, 10, 15, 20, 25, 30, 40]  # Target different densities\n",
    "    \n",
    "    # Try to find samples with target densities\n",
    "    attempts = 0\n",
    "    max_attempts = min(total_samples, 500)\n",
    "    \n",
    "    while len(sample_indices) < num_samples and attempts < max_attempts:\n",
    "        idx = np.random.randint(0, total_samples)\n",
    "        \n",
    "        try:\n",
    "            _, targets, _ = dataset[idx]\n",
    "            num_heads = len(targets) if targets.numel() > 0 else 0\n",
    "            \n",
    "            # Check if this sample fills a desired density category\n",
    "            target_density = density_targets[len(sample_indices) % len(density_targets)]\n",
    "            density_diff = abs(num_heads - target_density)\n",
    "            \n",
    "            # Accept if it's close to target or if we need any sample\n",
    "            if idx not in sample_indices and (density_diff <= 5 or len(sample_indices) >= 8):\n",
    "                sample_indices.append(idx)\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    # Fill remaining slots with random samples if needed\n",
    "    while len(sample_indices) < num_samples:\n",
    "        idx = np.random.randint(0, total_samples)\n",
    "        if idx not in sample_indices:\n",
    "            sample_indices.append(idx)\n",
    "    \n",
    "    # Create advanced visualization layout\n",
    "    cols = 4\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    # Color map for different wheat head sizes\n",
    "    size_colors = {'tiny': 'red', 'small': 'orange', 'medium': 'yellow', 'large': 'green'}\n",
    "    \n",
    "    sample_statistics = []\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        if i >= len(axes_flat):\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            image, targets, path = dataset[idx]\n",
    "            \n",
    "            # Convert and process image\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:\n",
    "                    img_np = (img_np + 1) / 2\n",
    "                elif img_np.max() <= 1.0:\n",
    "                    pass\n",
    "                else:\n",
    "                    img_np = img_np / 255.0\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "            else:\n",
    "                img_np = image\n",
    "                if img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "            \n",
    "            # Handle multi-channel images\n",
    "            if img_np.shape[-1] > 3:\n",
    "                img_np = img_np[:, :, :3]\n",
    "            \n",
    "            axes_flat[i].imshow(img_np)\n",
    "            \n",
    "            # Advanced annotation and analysis\n",
    "            num_heads = len(targets) if targets.numel() > 0 else 0\n",
    "            sample_stats = {\n",
    "                'num_heads': num_heads,\n",
    "                'sizes': [],\n",
    "                'positions': [],\n",
    "                'clustering_score': 0,\n",
    "                'density_category': '',\n",
    "                'spatial_distribution': '',\n",
    "                'overlap_detected': False\n",
    "            }\n",
    "            \n",
    "            if num_heads > 0:\n",
    "                h, w = img_np.shape[:2]\n",
    "                positions = []\n",
    "                areas = []\n",
    "                overlaps = 0\n",
    "                \n",
    "                # Process each wheat head\n",
    "                for j, target in enumerate(targets):\n",
    "                    if len(target) >= 5:\n",
    "                        cls, x_center, y_center, width, height = target[:5]\n",
    "                        \n",
    "                        # Convert to pixel coordinates\n",
    "                        x1 = (x_center - width/2) * w\n",
    "                        y1 = (y_center - height/2) * h\n",
    "                        x2 = (x_center + width/2) * w\n",
    "                        y2 = (y_center + height/2) * h\n",
    "                        \n",
    "                        # Store data for analysis\n",
    "                        area = width * height\n",
    "                        areas.append(area)\n",
    "                        positions.append((x_center, y_center))\n",
    "                        \n",
    "                        # Determine size category\n",
    "                        if area < 0.0003:\n",
    "                            size_cat = 'tiny'\n",
    "                        elif area < 0.0015:\n",
    "                            size_cat = 'small'\n",
    "                        elif area < 0.006:\n",
    "                            size_cat = 'medium'\n",
    "                        else:\n",
    "                            size_cat = 'large'\n",
    "                        \n",
    "                        sample_stats['sizes'].append(size_cat)\n",
    "                        \n",
    "                        # Draw bounding box with size-based color\n",
    "                        from matplotlib.patches import Rectangle\n",
    "                        color = size_colors[size_cat]\n",
    "                        \n",
    "                        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=1.5, edgecolor=color, facecolor='none', alpha=0.8)\n",
    "                        axes_flat[i].add_patch(rect)\n",
    "                        \n",
    "                        # Add size label\n",
    "                        axes_flat[i].text(x1, y1-5, f'{j+1}', fontsize=8, color=color, \n",
    "                                        fontweight='bold', bbox=dict(boxstyle='round,pad=0.2', \n",
    "                                        facecolor='white', alpha=0.7))\n",
    "                \n",
    "                # Analyze spatial distribution\n",
    "                if len(positions) >= 2:\n",
    "                    positions_array = np.array(positions)\n",
    "                    \n",
    "                    # Calculate clustering score using coefficient of variation of distances\n",
    "                    distances = pdist(positions_array)\n",
    "                    if len(distances) > 0:\n",
    "                        cv_distances = np.std(distances) / np.mean(distances) if np.mean(distances) > 0 else 0\n",
    "                        sample_stats['clustering_score'] = cv_distances\n",
    "                    \n",
    "                    # Detect spatial patterns\n",
    "                    x_coords = positions_array[:, 0]\n",
    "                    y_coords = positions_array[:, 1]\n",
    "                    \n",
    "                    x_spread = np.std(x_coords)\n",
    "                    y_spread = np.std(y_coords)\n",
    "                    \n",
    "                    if x_spread < 0.2 and y_spread < 0.2:\n",
    "                        sample_stats['spatial_distribution'] = 'Clustered'\n",
    "                    elif max(x_spread, y_spread) / min(x_spread, y_spread) > 2:\n",
    "                        sample_stats['spatial_distribution'] = 'Linear'\n",
    "                    else:\n",
    "                        sample_stats['spatial_distribution'] = 'Distributed'\n",
    "                    \n",
    "                    # Simple overlap detection\n",
    "                    for j in range(len(positions)):\n",
    "                        for k in range(j+1, len(positions)):\n",
    "                            dist = np.sqrt((positions[j][0] - positions[k][0])**2 + \n",
    "                                         (positions[j][1] - positions[k][1])**2)\n",
    "                            if dist < 0.05:  # Close proximity threshold\n",
    "                                overlaps += 1\n",
    "                    \n",
    "                    sample_stats['overlap_detected'] = overlaps > 0\n",
    "                \n",
    "                sample_stats['positions'] = positions\n",
    "            \n",
    "            # Categorize density\n",
    "            if num_heads == 0:\n",
    "                sample_stats['density_category'] = 'Empty'\n",
    "            elif num_heads <= 5:\n",
    "                sample_stats['density_category'] = 'Low'\n",
    "            elif num_heads <= 15:\n",
    "                sample_stats['density_category'] = 'Medium'\n",
    "            elif num_heads <= 30:\n",
    "                sample_stats['density_category'] = 'High'\n",
    "            else:\n",
    "                sample_stats['density_category'] = 'Very High'\n",
    "            \n",
    "            sample_statistics.append(sample_stats)\n",
    "            \n",
    "            # Create comprehensive title with analysis\n",
    "            size_summary = dict(Counter(sample_stats['sizes']))\n",
    "            size_text = ', '.join([f\"{count} {size}\" for size, count in size_summary.items()])\n",
    "            \n",
    "            title_lines = [\n",
    "                f'Sample {i+1}: {num_heads} wheat heads',\n",
    "                f'Density: {sample_stats[\"density_category\"]}',\n",
    "                f'Distribution: {sample_stats[\"spatial_distribution\"]}',\n",
    "                f'Sizes: {size_text if size_text else \"None\"}',\n",
    "                f'Clustering: {sample_stats[\"clustering_score\"]:.2f}' if sample_stats[\"clustering_score\"] > 0 else 'Clustering: N/A',\n",
    "                f'Overlaps: {\"Yes\" if sample_stats[\"overlap_detected\"] else \"No\"}',\n",
    "                f'{Path(path).name}'\n",
    "            ]\n",
    "            \n",
    "            axes_flat[i].set_title('\\n'.join(title_lines), fontsize=8, ha='left')\n",
    "            axes_flat[i].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            axes_flat[i].text(0.5, 0.5, f'Error loading\\nsample {i+1}\\n{str(e)[:30]}...', \n",
    "                             ha='center', va='center', transform=axes_flat[i].transAxes,\n",
    "                             bbox=dict(boxstyle='round', facecolor='pink', alpha=0.8))\n",
    "            axes_flat[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(sample_indices), len(axes_flat)):\n",
    "        axes_flat[i].axis('off')\n",
    "    \n",
    "    # Add legend for size categories\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor='none', edgecolor=color, \n",
    "                                   linewidth=2, label=f'{size.title()} wheat heads') \n",
    "                      for size, color in size_colors.items()]\n",
    "    \n",
    "    if axes_flat:\n",
    "        axes_flat[0].legend(handles=legend_elements, loc='upper right', \n",
    "                          bbox_to_anchor=(1, 1), fontsize=8)\n",
    "    \n",
    "    plt.suptitle(f'üåæ Advanced Wheat Head Sample Analysis - {dataset_name}', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(notebook_results_dir / 'samples' / f'advanced_wheat_samples_{dataset_name.lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary statistics visualization\n",
    "    if sample_statistics:\n",
    "        create_sample_analysis_summary(sample_statistics, dataset_name)\n",
    "    \n",
    "    return sample_statistics\n",
    "\n",
    "def create_sample_analysis_summary(sample_statistics, dataset_name):\n",
    "    \"\"\"Create summary analysis of the visualized samples\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Extract data for analysis\n",
    "    densities = [s['num_heads'] for s in sample_statistics]\n",
    "    density_categories = [s['density_category'] for s in sample_statistics]\n",
    "    spatial_distributions = [s['spatial_distribution'] for s in sample_statistics if s['spatial_distribution']]\n",
    "    clustering_scores = [s['clustering_score'] for s in sample_statistics if s['clustering_score'] > 0]\n",
    "    \n",
    "    # Size distribution across all samples\n",
    "    all_sizes = []\n",
    "    for s in sample_statistics:\n",
    "        all_sizes.extend(s['sizes'])\n",
    "    \n",
    "    # 1. Density distribution\n",
    "    axes[0, 0].hist(densities, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Sample Density Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Number of Wheat Heads')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    if densities:\n",
    "        mean_density = np.mean(densities)\n",
    "        axes[0, 0].axvline(mean_density, color='red', linestyle='--', \n",
    "                          label=f'Mean: {mean_density:.1f}')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Density categories pie chart\n",
    "    density_counts = Counter(density_categories)\n",
    "    if density_counts:\n",
    "        labels = list(density_counts.keys())\n",
    "        sizes = list(density_counts.values())\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
    "        \n",
    "        axes[0, 1].pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        axes[0, 1].set_title('Density Categories Distribution', fontweight='bold')\n",
    "    \n",
    "    # 3. Size distribution\n",
    "    if all_sizes:\n",
    "        size_counts = Counter(all_sizes)\n",
    "        size_labels = list(size_counts.keys())\n",
    "        size_values = list(size_counts.values())\n",
    "        \n",
    "        bars = axes[0, 2].bar(size_labels, size_values, alpha=0.8, \n",
    "                             color=['red', 'orange', 'yellow', 'green'])\n",
    "        axes[0, 2].set_title('Wheat Head Size Distribution', fontweight='bold')\n",
    "        axes[0, 2].set_xlabel('Size Category')\n",
    "        axes[0, 2].set_ylabel('Count')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, size_values):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                           f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Spatial distribution patterns\n",
    "    if spatial_distributions:\n",
    "        spatial_counts = Counter(spatial_distributions)\n",
    "        spatial_labels = list(spatial_counts.keys())\n",
    "        spatial_values = list(spatial_counts.values())\n",
    "        \n",
    "        axes[1, 0].bar(spatial_labels, spatial_values, alpha=0.8, \n",
    "                      color=plt.cm.Set2(np.linspace(0, 1, len(spatial_labels))))\n",
    "        axes[1, 0].set_title('Spatial Distribution Patterns', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Pattern Type')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Clustering scores\n",
    "    if clustering_scores:\n",
    "        axes[1, 1].hist(clustering_scores, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[1, 1].set_title('Clustering Score Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Clustering Score (CV of distances)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add interpretation guide\n",
    "        axes[1, 1].axvline(0.5, color='orange', linestyle='--', alpha=0.7, label='Moderate clustering')\n",
    "        axes[1, 1].axvline(1.0, color='red', linestyle='--', alpha=0.7, label='High clustering')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    # 6. Overlap analysis\n",
    "    overlap_counts = {'With Overlaps': 0, 'No Overlaps': 0}\n",
    "    for s in sample_statistics:\n",
    "        if s['overlap_detected']:\n",
    "            overlap_counts['With Overlaps'] += 1\n",
    "        else:\n",
    "            overlap_counts['No Overlaps'] += 1\n",
    "    \n",
    "    if sum(overlap_counts.values()) > 0:\n",
    "        labels = list(overlap_counts.keys())\n",
    "        sizes = list(overlap_counts.values())\n",
    "        colors = ['lightcoral', 'lightblue']\n",
    "        \n",
    "        axes[1, 2].pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        axes[1, 2].set_title('Overlap Detection Results', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'üìä Sample Analysis Summary - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(notebook_results_dir / 'samples' / f'sample_analysis_summary_{dataset_name.lower()}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_wheat_head_sizes_comprehensive(dataset, dataset_name, max_samples=500):\n",
    "    \"\"\"Comprehensive analysis of wheat head sizes and morphological characteristics\"\"\"\n",
    "    \n",
    "    size_metrics = {\n",
    "        'morphological_features': {\n",
    "            'areas': [],\n",
    "            'widths': [],\n",
    "            'heights': [],\n",
    "            'perimeters': [],\n",
    "            'aspect_ratios': [],\n",
    "            'elongation_indices': [],\n",
    "            'compactness_scores': []\n",
    "        },\n",
    "        'size_categories': {\n",
    "            'micro': 0,      # < 0.0002\n",
    "            'tiny': 0,       # 0.0002 - 0.0005\n",
    "            'small': 0,      # 0.0005 - 0.002\n",
    "            'medium': 0,     # 0.002 - 0.008\n",
    "            'large': 0,      # 0.008 - 0.02\n",
    "            'extra_large': 0 # > 0.02\n",
    "        },\n",
    "        'shape_analysis': {\n",
    "            'circular_heads': 0,\n",
    "            'elongated_heads': 0,\n",
    "            'irregular_heads': 0\n",
    "        },\n",
    "        'spatial_context': {\n",
    "            'position_size_correlation': {'x_coords': [], 'y_coords': [], 'sizes': []},\n",
    "            'edge_effects': [],\n",
    "            'center_bias': []\n",
    "        },\n",
    "        'statistical_measures': {\n",
    "            'size_distribution': [],\n",
    "            'shape_distribution': [],\n",
    "            'outlier_detection': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    print(f\"üîç Comprehensive wheat head size analysis on {sample_size} images from {dataset_name}...\")\n",
    "    \n",
    "    for i in tqdm(indices, desc=\"Analyzing wheat head morphology\"):\n",
    "        try:\n",
    "            _, targets, _ = dataset[i]\n",
    "            \n",
    "            if targets.numel() == 0:\n",
    "                continue\n",
    "                \n",
    "            for target in targets:\n",
    "                if len(target) >= 5:\n",
    "                    cls, x_center, y_center, width, height = target[:5]\n",
    "                    \n",
    "                    # Basic measurements\n",
    "                    area = float(width * height)\n",
    "                    perimeter = 2 * (width + height)  # Approximation for rectangle\n",
    "                    \n",
    "                    size_metrics['morphological_features']['areas'].append(area)\n",
    "                    size_metrics['morphological_features']['widths'].append(float(width))\n",
    "                    size_metrics['morphological_features']['heights'].append(float(height))\n",
    "                    size_metrics['morphological_features']['perimeters'].append(perimeter)\n",
    "                    \n",
    "                    # Shape analysis\n",
    "                    if height > 0 and width > 0:\n",
    "                        aspect_ratio = width / height\n",
    "                        size_metrics['morphological_features']['aspect_ratios'].append(aspect_ratio)\n",
    "                        \n",
    "                        # Elongation index (how far from square)\n",
    "                        elongation = abs(aspect_ratio - 1.0)\n",
    "                        size_metrics['morphological_features']['elongation_indices'].append(elongation)\n",
    "                        \n",
    "                        # Compactness score (area to perimeter ratio)\n",
    "                        compactness = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0\n",
    "                        size_metrics['morphological_features']['compactness_scores'].append(compactness)\n",
    "                        \n",
    "                        # Shape categorization\n",
    "                        if 0.8 <= aspect_ratio <= 1.2:  # Nearly square\n",
    "                            size_metrics['shape_analysis']['circular_heads'] += 1\n",
    "                        elif aspect_ratio > 1.5 or aspect_ratio < 0.67:  # Significantly elongated\n",
    "                            size_metrics['shape_analysis']['elongated_heads'] += 1\n",
    "                        else:\n",
    "                            size_metrics['shape_analysis']['irregular_heads'] += 1\n",
    "                    \n",
    "                    # Size categorization with refined thresholds\n",
    "                    if area < 0.0002:\n",
    "                        size_metrics['size_categories']['micro'] += 1\n",
    "                    elif area < 0.0005:\n",
    "                        size_metrics['size_categories']['tiny'] += 1\n",
    "                    elif area < 0.002:\n",
    "                        size_metrics['size_categories']['small'] += 1\n",
    "                    elif area < 0.008:\n",
    "                        size_metrics['size_categories']['medium'] += 1\n",
    "                    elif area < 0.02:\n",
    "                        size_metrics['size_categories']['large'] += 1\n",
    "                    else:\n",
    "                        size_metrics['size_categories']['extra_large'] += 1\n",
    "                    \n",
    "                    # Spatial context analysis\n",
    "                    size_metrics['spatial_context']['position_size_correlation']['x_coords'].append(float(x_center))\n",
    "                    size_metrics['spatial_context']['position_size_correlation']['y_coords'].append(float(y_center))\n",
    "                    size_metrics['spatial_context']['position_size_correlation']['sizes'].append(area)\n",
    "                    \n",
    "                    # Edge effect analysis\n",
    "                    edge_distance = min(x_center, y_center, 1-x_center, 1-y_center)\n",
    "                    size_metrics['spatial_context']['edge_effects'].append(edge_distance)\n",
    "                    \n",
    "                    # Center bias analysis\n",
    "                    center_distance = np.sqrt((x_center - 0.5)**2 + (y_center - 0.5)**2)\n",
    "                    size_metrics['spatial_context']['center_bias'].append(center_distance)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Statistical analysis\n",
    "    areas = size_metrics['morphological_features']['areas']\n",
    "    if areas:\n",
    "        # Outlier detection using IQR method\n",
    "        q1, q3 = np.percentile(areas, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outliers = [a for a in areas if a < lower_bound or a > upper_bound]\n",
    "        size_metrics['statistical_measures']['outlier_detection'] = len(outliers)\n",
    "        size_metrics['statistical_measures']['outlier_ratio'] = len(outliers) / len(areas)\n",
    "    \n",
    "    return size_metrics\n",
    "\n",
    "# Visualize samples and perform comprehensive analysis\n",
    "all_sample_statistics = {}\n",
    "all_size_metrics = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üñºÔ∏è Advanced wheat sample visualization for {name}...\")\n",
    "    \n",
    "    # Visualize samples with advanced annotations\n",
    "    sample_stats = visualize_wheat_samples_advanced(dataset, name, num_samples=12)\n",
    "    all_sample_statistics[name] = sample_stats\n",
    "    \n",
    "    # Comprehensive size analysis\n",
    "    print(f\"\\nüîç Comprehensive size analysis for {name}...\")\n",
    "    size_metrics = analyze_wheat_head_sizes_comprehensive(dataset, name, max_samples=400)\n",
    "    all_size_metrics[name] = size_metrics\n",
    "    \n",
    "    # Display size analysis results\n",
    "    morph_features = size_metrics['morphological_features']\n",
    "    if morph_features['areas']:\n",
    "        areas = np.array(morph_features['areas'])\n",
    "        widths = np.array(morph_features['widths'])\n",
    "        heights = np.array(morph_features['heights'])\n",
    "        aspect_ratios = np.array(morph_features['aspect_ratios'])\n",
    "        \n",
    "        print(f\"\\nüìè {name} - MORPHOLOGICAL ANALYSIS:\")\n",
    "        print(f\"   üìä Area Statistics:\")\n",
    "        print(f\"     Mean: {np.mean(areas):.6f} ¬± {np.std(areas):.6f}\")\n",
    "        print(f\"     Median: {np.median(areas):.6f}\")\n",
    "        print(f\"     Range: {np.min(areas):.6f} - {np.max(areas):.6f}\")\n",
    "        \n",
    "        print(f\"   üìê Dimension Statistics:\")\n",
    "        print(f\"     Width: {np.mean(widths):.4f} ¬± {np.std(widths):.4f}\")\n",
    "        print(f\"     Height: {np.mean(heights):.4f} ¬± {np.std(heights):.4f}\")\n",
    "        print(f\"     Aspect ratio: {np.mean(aspect_ratios):.3f} ¬± {np.std(aspect_ratios):.3f}\")\n",
    "        \n",
    "        print(f\"   üéØ Size Categories:\")\n",
    "        total_heads = sum(size_metrics['size_categories'].values())\n",
    "        for category, count in size_metrics['size_categories'].items():\n",
    "            percentage = (count / total_heads * 100) if total_heads > 0 else 0\n",
    "            print(f\"     {category.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"   üîç Shape Analysis:\")\n",
    "        total_shapes = sum(size_metrics['shape_analysis'].values())\n",
    "        for shape, count in size_metrics['shape_analysis'].items():\n",
    "            percentage = (count / total_shapes * 100) if total_shapes > 0 else 0\n",
    "            print(f\"     {shape.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Statistical measures\n",
    "        outlier_info = size_metrics['statistical_measures']\n",
    "        if 'outlier_detection' in outlier_info:\n",
    "            print(f\"   üìà Statistical Analysis:\")\n",
    "            print(f\"     Outliers detected: {outlier_info['outlier_detection']}\")\n",
    "            print(f\"     Outlier ratio: {outlier_info.get('outlier_ratio', 0):.3f}\")\n",
    "\n",
    "# Save comprehensive sample and size analysis results\n",
    "sample_analysis_summary = {}\n",
    "for dataset_name, stats in all_sample_statistics.items():\n",
    "    sample_analysis_summary[dataset_name] = {\n",
    "        'total_samples_analyzed': len(stats),\n",
    "        'density_statistics': {\n",
    "            'mean_heads_per_sample': float(np.mean([s['num_heads'] for s in stats])),\n",
    "            'density_categories': dict(Counter([s['density_category'] for s in stats])),\n",
    "            'spatial_patterns': dict(Counter([s['spatial_distribution'] for s in stats if s['spatial_distribution']]))\n",
    "        },\n",
    "        'size_analysis': all_size_metrics.get(dataset_name, {}),\n",
    "        'overlap_detection': {\n",
    "            'samples_with_overlap': sum(1 for s in stats if s['overlap_detected']),\n",
    "            'overlap_ratio': sum(1 for s in stats if s['overlap_detected']) / len(stats) if stats else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(notebook_results_dir / 'samples' / 'comprehensive_sample_analysis.json', 'w') as f:\n",
    "    json.dump(sample_analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Comprehensive sample analysis saved!\")\n",
    "print(f\"üìÅ Sample visualizations: {notebook_results_dir / 'samples'}\")\n",
    "print(f\"üìÑ Analysis data: {notebook_results_dir / 'samples' / 'comprehensive_sample_analysis.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba5bc7",
   "metadata": {},
   "source": [
    "## 7. Wheat-Specific Challenge Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced wheat-specific challenge assessment with computer vision and ML techniques\n",
    "\"\"\"\n",
    "\n",
    "def assess_wheat_challenges_comprehensive(dataset, dataset_name, max_samples=300):\n",
    "    \"\"\"Comprehensive assessment of wheat detection challenges using advanced techniques\"\"\"\n",
    "    \n",
    "    challenge_metrics = {\n",
    "        'occlusion_analysis': {\n",
    "            'occlusion_levels': {'none': 0, 'light': 0, 'moderate': 0, 'heavy': 0, 'severe': 0},\n",
    "            'occlusion_patterns': [],\n",
    "            'visibility_scores': [],\n",
    "            'partial_visibility': []\n",
    "        },\n",
    "        'scale_challenges': {\n",
    "            'scale_variations': [],\n",
    "            'multi_scale_complexity': [],\n",
    "            'size_consistency': [],\n",
    "            'perspective_effects': []\n",
    "        },\n",
    "        'density_challenges': {\n",
    "            'crowding_indices': [],\n",
    "            'object_separation': [],\n",
    "            'detection_difficulty': {'trivial': 0, 'easy': 0, 'medium': 0, 'hard': 0, 'extreme': 0},\n",
    "            'spatial_interference': []\n",
    "        },\n",
    "        'visual_challenges': {\n",
    "            'contrast_issues': [],\n",
    "            'illumination_problems': [],\n",
    "            'background_interference': [],\n",
    "            'texture_similarity': [],\n",
    "            'color_discrimination': []\n",
    "        },\n",
    "        'geometric_challenges': {\n",
    "            'shape_variations': [],\n",
    "            'orientation_challenges': [],\n",
    "            'deformation_analysis': [],\n",
    "            'boundary_clarity': []\n",
    "        },\n",
    "        'field_specific_issues': {\n",
    "            'growth_stage_variations': [],\n",
    "            'weather_impact_scores': [],\n",
    "            'soil_interference': [],\n",
    "            'vegetation_confusion': [],\n",
    "            'maturity_inconsistency': []\n",
    "        },\n",
    "        'detection_complexity': {\n",
    "            'edge_cases': 0,\n",
    "            'ambiguous_objects': 0,\n",
    "            'false_positive_risks': [],\n",
    "            'annotation_challenges': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sample_size = min(len(dataset), max_samples)\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Comprehensive wheat challenge assessment on {sample_size} images from {dataset_name}...\")\n",
    "    \n",
    "    processing_stats = {'success': 0, 'errors': 0, 'skipped': 0}\n",
    "    \n",
    "    for i in tqdm(indices, desc=\"Assessing detection challenges\"):\n",
    "        try:\n",
    "            image, targets, path = dataset[i]\n",
    "            \n",
    "            # Convert and validate image\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "                if img_np.min() < 0:\n",
    "                    img_np = (img_np + 1) / 2\n",
    "                elif img_np.max() <= 1.0:\n",
    "                    pass\n",
    "                else:\n",
    "                    img_np = img_np / 255.0\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "            else:\n",
    "                img_np = image\n",
    "                if img_np.max() > 1.0:\n",
    "                    img_np = img_np / 255.0\n",
    "            \n",
    "            if img_np.shape[-1] != 3:\n",
    "                processing_stats['skipped'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Convert to analysis formats\n",
    "            img_rgb = (img_np * 255).astype(np.uint8)\n",
    "            img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "            img_hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "            \n",
    "            processing_stats['success'] += 1\n",
    "            \n",
    "            # Analyze targets if available\n",
    "            if targets.numel() == 0:\n",
    "                challenge_metrics['detection_complexity']['edge_cases'] += 1\n",
    "                continue\n",
    "            \n",
    "            num_heads = len(targets)\n",
    "            target_data = []\n",
    "            \n",
    "            # Extract target information\n",
    "            for target in targets:\n",
    "                if len(target) >= 5:\n",
    "                    cls, x_center, y_center, width, height = target[:5]\n",
    "                    area = width * height\n",
    "                    target_data.append({\n",
    "                        'center': (float(x_center), float(y_center)),\n",
    "                        'size': (float(width), float(height)),\n",
    "                        'area': float(area)\n",
    "                    })\n",
    "            \n",
    "            if not target_data:\n",
    "                processing_stats['skipped'] += 1\n",
    "                continue\n",
    "            \n",
    "            # 1. OCCLUSION ANALYSIS\n",
    "            \n",
    "            # Calculate object separation distances\n",
    "            if len(target_data) > 1:\n",
    "                separations = []\n",
    "                overlaps = 0\n",
    "                \n",
    "                for i in range(len(target_data)):\n",
    "                    for j in range(i+1, len(target_data)):\n",
    "                        center1 = target_data[i]['center']\n",
    "                        center2 = target_data[j]['center']\n",
    "                        size1 = target_data[i]['size']\n",
    "                        size2 = target_data[j]['size']\n",
    "                        \n",
    "                        # Calculate distance between centers\n",
    "                        distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "                        separations.append(distance)\n",
    "                        \n",
    "                        # Check for potential overlap\n",
    "                        avg_size = (size1[0] + size1[1] + size2[0] + size2[1]) / 4\n",
    "                        if distance < avg_size * 0.8:  # Close proximity\n",
    "                            overlaps += 1\n",
    "                \n",
    "                avg_separation = np.mean(separations) if separations else 1.0\n",
    "                challenge_metrics['occlusion_analysis']['occlusion_patterns'].append(overlaps)\n",
    "                \n",
    "                # Categorize occlusion level\n",
    "                if overlaps == 0:\n",
    "                    challenge_metrics['occlusion_analysis']['occlusion_levels']['none'] += 1\n",
    "                elif overlaps <= 2:\n",
    "                    challenge_metrics['occlusion_analysis']['occlusion_levels']['light'] += 1\n",
    "                elif overlaps <= 5:\n",
    "                    challenge_metrics['occlusion_analysis']['occlusion_levels']['moderate'] += 1\n",
    "                elif overlaps <= 10:\n",
    "                    challenge_metrics['occlusion_analysis']['occlusion_levels']['heavy'] += 1\n",
    "                else:\n",
    "                    challenge_metrics['occlusion_analysis']['occlusion_levels']['severe'] += 1\n",
    "                \n",
    "                challenge_metrics['density_challenges']['object_separation'].append(avg_separation)\n",
    "            \n",
    "            # 2. SCALE CHALLENGES\n",
    "            \n",
    "            # Analyze size variations\n",
    "            areas = [td['area'] for td in target_data]\n",
    "            if areas:\n",
    "                area_cv = np.std(areas) / np.mean(areas) if np.mean(areas) > 0 else 0\n",
    "                challenge_metrics['scale_challenges']['scale_variations'].append(area_cv)\n",
    "                \n",
    "                # Multi-scale complexity (ratio of largest to smallest)\n",
    "                if len(areas) > 1:\n",
    "                    scale_ratio = max(areas) / min(areas) if min(areas) > 0 else 1\n",
    "                    challenge_metrics['scale_challenges']['multi_scale_complexity'].append(scale_ratio)\n",
    "                \n",
    "                # Size consistency analysis\n",
    "                median_area = np.median(areas)\n",
    "                consistency_score = sum(1 for a in areas if 0.5 * median_area <= a <= 2 * median_area) / len(areas)\n",
    "                challenge_metrics['scale_challenges']['size_consistency'].append(consistency_score)\n",
    "            \n",
    "            # 3. DENSITY CHALLENGES\n",
    "            \n",
    "            # Crowding index based on local density\n",
    "            if len(target_data) >= 2:\n",
    "                crowding_scores = []\n",
    "                for target in target_data:\n",
    "                    center = target['center']\n",
    "                    neighbors_in_radius = 0\n",
    "                    radius = 0.15  # Analysis radius\n",
    "                    \n",
    "                    for other_target in target_data:\n",
    "                        if other_target != target:\n",
    "                            other_center = other_target['center']\n",
    "                            distance = np.sqrt((center[0] - other_center[0])**2 + \n",
    "                                             (center[1] - other_center[1])**2)\n",
    "                            if distance <= radius:\n",
    "                                neighbors_in_radius += 1\n",
    "                    \n",
    "                    crowding_scores.append(neighbors_in_radius)\n",
    "                \n",
    "                avg_crowding = np.mean(crowding_scores)\n",
    "                challenge_metrics['density_challenges']['crowding_indices'].append(avg_crowding)\n",
    "                \n",
    "                # Detection difficulty assessment\n",
    "                difficulty_score = 0\n",
    "                \n",
    "                # Add difficulty for high density\n",
    "                if num_heads > 30:\n",
    "                    difficulty_score += 4\n",
    "                elif num_heads > 20:\n",
    "                    difficulty_score += 3\n",
    "                elif num_heads > 15:\n",
    "                    difficulty_score += 2\n",
    "                elif num_heads > 10:\n",
    "                    difficulty_score += 1\n",
    "                \n",
    "                # Add difficulty for small objects\n",
    "                small_objects = sum(1 for a in areas if a < 0.001)\n",
    "                small_ratio = small_objects / len(areas) if areas else 0\n",
    "                if small_ratio > 0.7:\n",
    "                    difficulty_score += 3\n",
    "                elif small_ratio > 0.5:\n",
    "                    difficulty_score += 2\n",
    "                elif small_ratio > 0.3:\n",
    "                    difficulty_score += 1\n",
    "                \n",
    "                # Add difficulty for high crowding\n",
    "                if avg_crowding > 5:\n",
    "                    difficulty_score += 2\n",
    "                elif avg_crowding > 3:\n",
    "                    difficulty_score += 1\n",
    "                \n",
    "                # Categorize difficulty\n",
    "                if difficulty_score >= 8:\n",
    "                    challenge_metrics['density_challenges']['detection_difficulty']['extreme'] += 1\n",
    "                elif difficulty_score >= 6:\n",
    "                    challenge_metrics['density_challenges']['detection_difficulty']['hard'] += 1\n",
    "                elif difficulty_score >= 4:\n",
    "                    challenge_metrics['density_challenges']['detection_difficulty']['medium'] += 1\n",
    "                elif difficulty_score >= 2:\n",
    "                    challenge_metrics['density_challenges']['detection_difficulty']['easy'] += 1\n",
    "                else:\n",
    "                    challenge_metrics['density_challenges']['detection_difficulty']['trivial'] += 1\n",
    "            \n",
    "            # 4. VISUAL CHALLENGES\n",
    "            \n",
    "            # Contrast analysis\n",
    "            local_contrasts = []\n",
    "            for target in target_data:\n",
    "                center = target['center']\n",
    "                x_pixel = int(center[0] * img_gray.shape[1])\n",
    "                y_pixel = int(center[1] * img_gray.shape[0])\n",
    "                \n",
    "                # Extract local region\n",
    "                region_size = 32\n",
    "                x1 = max(0, x_pixel - region_size // 2)\n",
    "                x2 = min(img_gray.shape[1], x_pixel + region_size // 2)\n",
    "                y1 = max(0, y_pixel - region_size // 2)\n",
    "                y2 = min(img_gray.shape[0], y_pixel + region_size // 2)\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    local_region = img_gray[y1:y2, x1:x2]\n",
    "                    local_contrast = np.std(local_region) / 255.0\n",
    "                    local_contrasts.append(local_contrast)\n",
    "            \n",
    "            if local_contrasts:\n",
    "                avg_local_contrast = np.mean(local_contrasts)\n",
    "                challenge_metrics['visual_challenges']['contrast_issues'].append(avg_local_contrast)\n",
    "            \n",
    "            # Global illumination analysis\n",
    "            brightness = np.mean(img_gray) / 255.0\n",
    "            brightness_std = np.std(img_gray) / 255.0\n",
    "            \n",
    "            illumination_problem_score = 0\n",
    "            if brightness < 0.3 or brightness > 0.8:  # Too dark or bright\n",
    "                illumination_problem_score += 1\n",
    "            if brightness_std > 0.25:  # High variation (shadows/glare)\n",
    "                illumination_problem_score += 1\n",
    "            \n",
    "            challenge_metrics['visual_challenges']['illumination_problems'].append(illumination_problem_score)\n",
    "            \n",
    "            # Background interference using edge density\n",
    "            edges = cv2.Canny(img_gray, 50, 150)\n",
    "            edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])\n",
    "            challenge_metrics['visual_challenges']['background_interference'].append(edge_density)\n",
    "            \n",
    "            # Texture similarity analysis using LBP-like measure\n",
    "            def calculate_texture_uniformity(image):\n",
    "                \"\"\"Calculate texture uniformity in the image\"\"\"\n",
    "                h, w = image.shape\n",
    "                texture_vars = []\n",
    "                patch_size = 16\n",
    "                \n",
    "                for i in range(0, h-patch_size, patch_size):\n",
    "                    for j in range(0, w-patch_size, patch_size):\n",
    "                        patch = image[i:i+patch_size, j:j+patch_size]\n",
    "                        texture_vars.append(np.var(patch))\n",
    "                \n",
    "                return np.mean(texture_vars) if texture_vars else 0\n",
    "            \n",
    "            texture_uniformity = calculate_texture_uniformity(img_gray)\n",
    "            challenge_metrics['visual_challenges']['texture_similarity'].append(texture_uniformity)\n",
    "            \n",
    "            # 5. GEOMETRIC CHALLENGES\n",
    "            \n",
    "            # Shape variation analysis\n",
    "            if len(target_data) > 1:\n",
    "                aspect_ratios = []\n",
    "                for target in target_data:\n",
    "                    width, height = target['size']\n",
    "                    if height > 0:\n",
    "                        aspect_ratios.append(width / height)\n",
    "                \n",
    "                if aspect_ratios:\n",
    "                    shape_variation = np.std(aspect_ratios)\n",
    "                    challenge_metrics['geometric_challenges']['shape_variations'].append(shape_variation)\n",
    "            \n",
    "            # Boundary clarity assessment using gradient analysis\n",
    "            boundary_scores = []\n",
    "            for target in target_data:\n",
    "                center = target['center']\n",
    "                size = target['size']\n",
    "                \n",
    "                # Calculate bounding box in pixels\n",
    "                x_pixel = int(center[0] * img_gray.shape[1])\n",
    "                y_pixel = int(center[1] * img_gray.shape[0])\n",
    "                w_pixel = int(size[0] * img_gray.shape[1])\n",
    "                h_pixel = int(size[1] * img_gray.shape[0])\n",
    "                \n",
    "                # Extract region around object\n",
    "                x1 = max(0, x_pixel - w_pixel // 2)\n",
    "                x2 = min(img_gray.shape[1], x_pixel + w_pixel // 2)\n",
    "                y1 = max(0, y_pixel - h_pixel // 2)\n",
    "                y2 = min(img_gray.shape[0], y_pixel + h_pixel // 2)\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    region = img_gray[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Calculate gradient magnitude\n",
    "                    grad_x = cv2.Sobel(region, cv2.CV_64F, 1, 0, ksize=3)\n",
    "                    grad_y = cv2.Sobel(region, cv2.CV_64F, 0, 1, ksize=3)\n",
    "                    gradient_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "                    \n",
    "                    boundary_clarity = np.mean(gradient_mag)\n",
    "                    boundary_scores.append(boundary_clarity)\n",
    "            \n",
    "            if boundary_scores:\n",
    "                avg_boundary_clarity = np.mean(boundary_scores)\n",
    "                challenge_metrics['geometric_challenges']['boundary_clarity'].append(avg_boundary_clarity)\n",
    "            \n",
    "            # 6. FIELD-SPECIFIC ISSUES\n",
    "            \n",
    "            # Growth stage analysis using color properties\n",
    "            hue_channel = img_hsv[:, :, 0]\n",
    "            sat_channel = img_hsv[:, :, 1]\n",
    "            val_channel = img_hsv[:, :, 2]\n",
    "            \n",
    "            # Analyze dominant colors to infer growth stage\n",
    "            green_mask = (hue_channel >= 35) & (hue_channel <= 85) & (sat_channel > 50)\n",
    "            yellow_mask = (hue_channel >= 15) & (hue_channel <= 35) & (sat_channel > 50)\n",
    "            brown_mask = (hue_channel >= 5) & (hue_channel <= 25) & (sat_channel > 30)\n",
    "            \n",
    "            green_ratio = np.sum(green_mask) / green_mask.size\n",
    "            yellow_ratio = np.sum(yellow_mask) / yellow_mask.size\n",
    "            brown_ratio = np.sum(brown_mask) / brown_mask.size\n",
    "            \n",
    "            # Growth stage inference\n",
    "            if green_ratio > 0.4:\n",
    "                growth_stage = 'early'\n",
    "            elif yellow_ratio > 0.3:\n",
    "                growth_stage = 'mature'\n",
    "            elif brown_ratio > 0.2:\n",
    "                growth_stage = 'late'\n",
    "            else:\n",
    "                growth_stage = 'mixed'\n",
    "            \n",
    "            challenge_metrics['field_specific_issues']['growth_stage_variations'].append(growth_stage)\n",
    "            \n",
    "            # Weather impact assessment\n",
    "            weather_impact = 0\n",
    "            if brightness < 0.25:  # Very dark (cloudy/stormy)\n",
    "                weather_impact += 2\n",
    "            elif brightness > 0.85:  # Very bright (harsh sun)\n",
    "                weather_impact += 2\n",
    "            if brightness_std > 0.3:  # High variation (mixed lighting)\n",
    "                weather_impact += 1\n",
    "            \n",
    "            challenge_metrics['field_specific_issues']['weather_impact_scores'].append(weather_impact)\n",
    "            \n",
    "            # Soil interference analysis\n",
    "            # Look for brown/tan pixels that might interfere with detection\n",
    "            soil_mask = ((hue_channel >= 10) & (hue_channel <= 30) & \n",
    "                        (sat_channel < 100) & (val_channel > 30))\n",
    "            soil_ratio = np.sum(soil_mask) / soil_mask.size\n",
    "            challenge_metrics['field_specific_issues']['soil_interference'].append(soil_ratio)\n",
    "            \n",
    "            # 7. DETECTION COMPLEXITY ASSESSMENT\n",
    "            \n",
    "            # Edge case detection\n",
    "            edge_distance_threshold = 0.1\n",
    "            edge_objects = 0\n",
    "            for target in target_data:\n",
    "                center = target['center']\n",
    "                if (center[0] <= edge_distance_threshold or center[0] >= 1-edge_distance_threshold or\n",
    "                    center[1] <= edge_distance_threshold or center[1] >= 1-edge_distance_threshold):\n",
    "                    edge_objects += 1\n",
    "            \n",
    "            if edge_objects > 0:\n",
    "                challenge_metrics['detection_complexity']['edge_cases'] += 1\n",
    "            \n",
    "            # Ambiguous object detection based on size and isolation\n",
    "            ambiguous_count = 0\n",
    "            for target in target_data:\n",
    "                area = target['area']\n",
    "                center = target['center']\n",
    "                \n",
    "                # Check if object is unusually small or large\n",
    "                if area < 0.0002 or area > 0.02:\n",
    "                    ambiguous_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Check isolation (might be noise if very isolated)\n",
    "                neighbors = 0\n",
    "                for other_target in target_data:\n",
    "                    if other_target != target:\n",
    "                        other_center = other_target['center']\n",
    "                        distance = np.sqrt((center[0] - other_center[0])**2 + \n",
    "                                         (center[1] - other_center[1])**2)\n",
    "                        if distance <= 0.2:  # Within reasonable distance\n",
    "                            neighbors += 1\n",
    "                \n",
    "                if neighbors == 0 and len(target_data) > 3:  # Isolated in dense scene\n",
    "                    ambiguous_count += 1\n",
    "            \n",
    "            if ambiguous_count > 0:\n",
    "                challenge_metrics['detection_complexity']['ambiguous_objects'] += ambiguous_count\n",
    "            \n",
    "            # False positive risk assessment\n",
    "            fp_risk_score = 0\n",
    "            \n",
    "            # High background complexity increases FP risk\n",
    "            if edge_density > 0.15:\n",
    "                fp_risk_score += 1\n",
    "            \n",
    "            # Low contrast increases FP risk\n",
    "            if avg_local_contrast < 0.1:\n",
    "                fp_risk_score += 1\n",
    "            \n",
    "            # High soil visibility increases FP risk\n",
    "            if soil_ratio > 0.3:\n",
    "                fp_risk_score += 1\n",
    "            \n",
    "            challenge_metrics['detection_complexity']['false_positive_risks'].append(fp_risk_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            processing_stats['errors'] += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"   ‚úÖ Processing stats: {processing_stats['success']} success, \"\n",
    "          f\"{processing_stats['errors']} errors, {processing_stats['skipped']} skipped\")\n",
    "    \n",
    "    return challenge_metrics\n",
    "\n",
    "# Comprehensive challenge assessment\n",
    "challenge_results = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    challenge_metrics = assess_wheat_challenges_comprehensive(dataset, name, max_samples=250)\n",
    "    challenge_results[name] = challenge_metrics\n",
    "    \n",
    "    # Display comprehensive challenge analysis\n",
    "    print(f\"\\n‚ö†Ô∏è {name} - COMPREHENSIVE CHALLENGE ASSESSMENT:\")\n",
    "    \n",
    "    # Occlusion analysis\n",
    "    occlusion_data = challenge_metrics['occlusion_analysis']\n",
    "    print(f\"   üîç Occlusion Analysis:\")\n",
    "    total_occlusion = sum(occlusion_data['occlusion_levels'].values())\n",
    "    for level, count in occlusion_data['occlusion_levels'].items():\n",
    "        percentage = (count / total_occlusion * 100) if total_occlusion > 0 else 0\n",
    "        print(f\"     {level.title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    if occlusion_data['occlusion_patterns']:\n",
    "        avg_overlaps = np.mean(occlusion_data['occlusion_patterns'])\n",
    "        print(f\"     Average overlaps per image: {avg_overlaps:.2f}\")\n",
    "    \n",
    "    # Scale challenges\n",
    "    scale_data = challenge_metrics['scale_challenges']\n",
    "    if scale_data['scale_variations']:\n",
    "        print(f\"   üìè Scale Challenges:\")\n",
    "        print(f\"     Scale variation (CV): {np.mean(scale_data['scale_variations']):.3f}\")\n",
    "        if scale_data['multi_scale_complexity']:\n",
    "            print(f\"     Multi-scale complexity: {np.mean(scale_data['multi_scale_complexity']):.2f}\")\n",
    "        if scale_data['size_consistency']:\n",
    "            print(f\"     Size consistency: {np.mean(scale_data['size_consistency']):.3f}\")\n",
    "    \n",
    "    # Density challenges\n",
    "    density_data = challenge_metrics['density_challenges']\n",
    "    print(f\"   üåæ Density Challenges:\")\n",
    "    total_difficulty = sum(density_data['detection_difficulty'].values())\n",
    "    for difficulty, count in density_data['detection_difficulty'].items():\n",
    "        percentage = (count / total_difficulty * 100) if total_difficulty > 0 else 0\n",
    "        print(f\"     {difficulty.title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    if density_data['crowding_indices']:\n",
    "        avg_crowding = np.mean(density_data['crowding_indices'])\n",
    "        print(f\"     Average crowding index: {avg_crowding:.2f}\")\n",
    "    \n",
    "    # Visual challenges\n",
    "    visual_data = challenge_metrics['visual_challenges']\n",
    "    if visual_data['contrast_issues']:\n",
    "        print(f\"   üëÅÔ∏è Visual Challenges:\")\n",
    "        print(f\"     Average local contrast: {np.mean(visual_data['contrast_issues']):.3f}\")\n",
    "        print(f\"     Illumination problems: {np.mean(visual_data['illumination_problems']):.2f}\")\n",
    "        print(f\"     Background interference: {np.mean(visual_data['background_interference']):.4f}\")\n",
    "    \n",
    "    # Field-specific issues\n",
    "    field_data = challenge_metrics['field_specific_issues']\n",
    "    if field_data['growth_stage_variations']:\n",
    "        print(f\"   üå± Field-Specific Issues:\")\n",
    "        growth_stages = Counter(field_data['growth_stage_variations'])\n",
    "        for stage, count in growth_stages.items():\n",
    "            print(f\"     {stage.title()} growth: {count}\")\n",
    "        \n",
    "        if field_data['weather_impact_scores']:\n",
    "            avg_weather_impact = np.mean(field_data['weather_impact_scores'])\n",
    "            print(f\"     Average weather impact: {avg_weather_impact:.2f}\")\n",
    "        \n",
    "        if field_data['soil_interference']:\n",
    "            avg_soil_interference = np.mean(field_data['soil_interference'])\n",
    "            print(f\"     Average soil interference: {avg_soil_interference:.3f}\")\n",
    "    \n",
    "    # Detection complexity\n",
    "    complexity_data = challenge_metrics['detection_complexity']\n",
    "    print(f\"   üéØ Detection Complexity:\")\n",
    "    print(f\"     Edge cases: {complexity_data['edge_cases']}\")\n",
    "    print(f\"     Ambiguous objects: {complexity_data['ambiguous_objects']}\")\n",
    "    \n",
    "    if complexity_data['false_positive_risks']:\n",
    "        avg_fp_risk = np.mean(complexity_data['false_positive_risks'])\n",
    "        print(f\"     Average FP risk score: {avg_fp_risk:.2f}\")\n",
    "\n",
    "# Create comprehensive challenge assessment visualization\n",
    "fig = plt.figure(figsize=(24, 20))\n",
    "gs = fig.add_gridspec(5, 4, hspace=0.4, wspace=0.3)\n",
    "\n",
    "dataset_colors = plt.cm.Set1(np.linspace(0, 1, len(challenge_results)))\n",
    "\n",
    "# 1. Occlusion Levels Distribution (top row, first column)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "occlusion_summary = defaultdict(int)\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    occlusion_data = metrics['occlusion_analysis']['occlusion_levels']\n",
    "    for level, count in occlusion_data.items():\n",
    "        occlusion_summary[level] += count\n",
    "\n",
    "if occlusion_summary:\n",
    "    levels = list(occlusion_summary.keys())\n",
    "    counts = list(occlusion_summary.values())\n",
    "    colors = plt.cm.Reds(np.linspace(0.3, 1, len(levels)))\n",
    "    \n",
    "    bars = ax1.bar(levels, counts, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Occlusion Levels Distribution', fontweight='bold')\n",
    "    ax1.set_xlabel('Occlusion Level')\n",
    "    ax1.set_ylabel('Number of Images')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(counts),\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Detection Difficulty Distribution (top row, second column)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "difficulty_summary = defaultdict(int)\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    difficulty_data = metrics['density_challenges']['detection_difficulty']\n",
    "    for level, count in difficulty_data.items():\n",
    "        difficulty_summary[level] += count\n",
    "\n",
    "if difficulty_summary:\n",
    "    difficulties = list(difficulty_summary.keys())\n",
    "    counts = list(difficulty_summary.values())\n",
    "    colors = plt.cm.YlOrRd(np.linspace(0.3, 1, len(difficulties)))\n",
    "    \n",
    "    bars = ax2.bar(range(len(difficulties)), counts, color=colors, alpha=0.8)\n",
    "    ax2.set_title('Detection Difficulty Distribution', fontweight='bold')\n",
    "    ax2.set_xlabel('Difficulty Level')\n",
    "    ax2.set_ylabel('Number of Images')\n",
    "    ax2.set_xticks(range(len(difficulties)))\n",
    "    ax2.set_xticklabels([d.replace('_', '\\n') for d in difficulties])\n",
    "    \n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(counts),\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Scale Challenges Analysis (top row, third column)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "for idx, (dataset_name, metrics) in enumerate(challenge_results.items()):\n",
    "    scale_variations = metrics['scale_challenges']['scale_variations']\n",
    "    size_consistency = metrics['scale_challenges']['size_consistency']\n",
    "    \n",
    "    if scale_variations and size_consistency:\n",
    "        ax3.scatter(scale_variations, size_consistency, alpha=0.6, \n",
    "                   label=dataset_name, s=50, color=dataset_colors[idx])\n",
    "\n",
    "ax3.set_title('Scale Variation vs Size Consistency', fontweight='bold')\n",
    "ax3.set_xlabel('Scale Variation (CV)')\n",
    "ax3.set_ylabel('Size Consistency Score')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Visual Challenges Radar Chart (top row, fourth column)\n",
    "ax4 = fig.add_subplot(gs[0, 3], projection='polar')\n",
    "visual_metrics = ['Contrast', 'Illumination', 'Background', 'Texture']\n",
    "angles = np.linspace(0, 2*np.pi, len(visual_metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for idx, (dataset_name, metrics) in enumerate(challenge_results.items()):\n",
    "    visual_data = metrics['visual_challenges']\n",
    "    \n",
    "    # Normalize metrics to 0-1 scale\n",
    "    contrast_norm = 1 - min(np.mean(visual_data['contrast_issues']) if visual_data['contrast_issues'] else 0, 1)\n",
    "    illum_norm = min(np.mean(visual_data['illumination_problems']) if visual_data['illumination_problems'] else 0, 1) / 3\n",
    "    bg_norm = min(np.mean(visual_data['background_interference']) if visual_data['background_interference'] else 0, 1)\n",
    "    texture_norm = min(np.mean(visual_data['texture_similarity']) if visual_data['texture_similarity'] else 0, 1) / 1000\n",
    "    \n",
    "    values = [contrast_norm, illum_norm, bg_norm, texture_norm]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax4.plot(angles, values, 'o-', linewidth=2, label=dataset_name, \n",
    "            color=dataset_colors[idx])\n",
    "    ax4.fill(angles, values, alpha=0.25, color=dataset_colors[idx])\n",
    "\n",
    "ax4.set_xticks(angles[:-1])\n",
    "ax4.set_xticklabels(visual_metrics)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.set_title('Visual Challenge Profile', fontweight='bold', pad=20)\n",
    "ax4.legend(bbox_to_anchor=(1.3, 1), loc='upper left')\n",
    "\n",
    "# 5. Field-Specific Issues (second row, first two columns)\n",
    "ax5 = fig.add_subplot(gs[1, :2])\n",
    "field_data = []\n",
    "field_labels = []\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    weather_scores = metrics['field_specific_issues']['weather_impact_scores']\n",
    "    soil_interference = metrics['field_specific_issues']['soil_interference']\n",
    "    \n",
    "    if weather_scores:\n",
    "        field_data.extend(weather_scores)\n",
    "        field_labels.extend([f'{dataset_name}_weather'] * len(weather_scores))\n",
    "    \n",
    "    if soil_interference:\n",
    "        # Normalize soil interference to same scale as weather\n",
    "        normalized_soil = [s * 3 for s in soil_interference]  # Scale up for visibility\n",
    "        field_data.extend(normalized_soil)\n",
    "        field_labels.extend([f'{dataset_name}_soil'] * len(normalized_soil))\n",
    "\n",
    "if field_data:\n",
    "    df_field = pd.DataFrame({'Score': field_data, 'Issue_Type': field_labels})\n",
    "    df_field['Dataset'] = df_field['Issue_Type'].str.split('_').str[0]\n",
    "    df_field['Issue'] = df_field['Issue_Type'].str.split('_').str[1]\n",
    "    \n",
    "    sns.boxplot(data=df_field, x='Dataset', y='Score', hue='Issue', ax=ax5)\n",
    "    ax5.set_title('Field-Specific Issues Analysis', fontweight='bold')\n",
    "    ax5.set_ylabel('Impact Score')\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Growth Stage Distribution (second row, third column)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "growth_stages_all = []\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    growth_stages = metrics['field_specific_issues']['growth_stage_variations']\n",
    "    growth_stages_all.extend(growth_stages)\n",
    "\n",
    "if growth_stages_all:\n",
    "    stage_counts = Counter(growth_stages_all)\n",
    "    stages = list(stage_counts.keys())\n",
    "    counts = list(stage_counts.values())\n",
    "    colors = plt.cm.Greens(np.linspace(0.4, 1, len(stages)))\n",
    "    \n",
    "    wedges, texts, autotexts = ax6.pie(counts, labels=stages, autopct='%1.1f%%', \n",
    "                                      colors=colors, startangle=90)\n",
    "    ax6.set_title('Growth Stage Distribution', fontweight='bold')\n",
    "\n",
    "# 7. Complexity Factors (second row, fourth column)\n",
    "ax7 = fig.add_subplot(gs[1, 3])\n",
    "complexity_factors = ['Edge Cases', 'Ambiguous Objects', 'Avg FP Risk']\n",
    "complexity_data = {factor: [] for factor in complexity_factors}\n",
    "\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    complexity = metrics['detection_complexity']\n",
    "    complexity_data['Edge Cases'].append(complexity['edge_cases'])\n",
    "    complexity_data['Ambiguous Objects'].append(complexity['ambiguous_objects'])\n",
    "    \n",
    "    fp_risks = complexity['false_positive_risks']\n",
    "    avg_fp_risk = np.mean(fp_risks) if fp_risks else 0\n",
    "    complexity_data['Avg FP Risk'].append(avg_fp_risk * 10)  # Scale for visibility\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(challenge_results))\n",
    "width = 0.25\n",
    "\n",
    "for i, (factor, data) in enumerate(complexity_data.items()):\n",
    "    ax7.bar(x + i*width, data, width, label=factor, alpha=0.8)\n",
    "\n",
    "ax7.set_title('Detection Complexity Factors', fontweight='bold')\n",
    "ax7.set_xlabel('Dataset')\n",
    "ax7.set_ylabel('Count/Score')\n",
    "ax7.set_xticks(x + width)\n",
    "ax7.set_xticklabels([name.split('_')[0] for name in challenge_results.keys()], rotation=45)\n",
    "ax7.legend()\n",
    "\n",
    "# 8-12. Detailed Analysis Plots (remaining rows)\n",
    "# Crowding vs Difficulty Relationship\n",
    "ax8 = fig.add_subplot(gs[2, 0])\n",
    "for idx, (dataset_name, metrics) in enumerate(challenge_results.items()):\n",
    "    crowding_indices = metrics['density_challenges']['crowding_indices']\n",
    "    difficulty_data = metrics['density_challenges']['detection_difficulty']\n",
    "    \n",
    "    if crowding_indices:\n",
    "        # Calculate average difficulty score\n",
    "        difficulty_weights = {'trivial': 1, 'easy': 2, 'medium': 3, 'hard': 4, 'extreme': 5}\n",
    "        total_images = sum(difficulty_data.values())\n",
    "        if total_images > 0:\n",
    "            avg_difficulty = sum(difficulty_weights[level] * count for level, count in difficulty_data.items()) / total_images\n",
    "            avg_crowding = np.mean(crowding_indices)\n",
    "            \n",
    "            ax8.scatter(avg_crowding, avg_difficulty, s=100, alpha=0.7, \n",
    "                       label=dataset_name, color=dataset_colors[idx])\n",
    "\n",
    "ax8.set_title('Crowding vs Detection Difficulty', fontweight='bold')\n",
    "ax8.set_xlabel('Average Crowding Index')\n",
    "ax8.set_ylabel('Average Difficulty Score')\n",
    "ax8.legend()\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Boundary Clarity Analysis\n",
    "ax9 = fig.add_subplot(gs[2, 1])\n",
    "boundary_data = []\n",
    "boundary_labels = []\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    boundary_clarity = metrics['geometric_challenges']['boundary_clarity']\n",
    "    if boundary_clarity:\n",
    "        boundary_data.extend(boundary_clarity)\n",
    "        boundary_labels.extend([dataset_name] * len(boundary_clarity))\n",
    "\n",
    "if boundary_data:\n",
    "    df_boundary = pd.DataFrame({'Boundary_Clarity': boundary_data, 'Dataset': boundary_labels})\n",
    "    sns.violinplot(data=df_boundary, x='Dataset', y='Boundary_Clarity', ax=ax9)\n",
    "    ax9.set_title('Boundary Clarity Distribution', fontweight='bold')\n",
    "    ax9.set_ylabel('Gradient Magnitude')\n",
    "    ax9.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Object Separation Analysis\n",
    "ax10 = fig.add_subplot(gs[2, 2])\n",
    "for idx, (dataset_name, metrics) in enumerate(challenge_results.items()):\n",
    "    separations = metrics['density_challenges']['object_separation']\n",
    "    if separations:\n",
    "        ax10.hist(separations, bins=20, alpha=0.6, label=dataset_name,\n",
    "                 color=dataset_colors[idx], density=True)\n",
    "\n",
    "ax10.set_title('Object Separation Distribution', fontweight='bold')\n",
    "ax10.set_xlabel('Average Separation Distance')\n",
    "ax10.set_ylabel('Density')\n",
    "ax10.legend()\n",
    "ax10.grid(True, alpha=0.3)\n",
    "\n",
    "# Challenge Correlation Matrix\n",
    "ax11 = fig.add_subplot(gs[2, 3])\n",
    "# Create correlation matrix for different challenge metrics\n",
    "challenge_matrix_data = []\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    row_data = []\n",
    "    \n",
    "    # Occlusion severity (weighted average)\n",
    "    occlusion_weights = {'none': 0, 'light': 1, 'moderate': 2, 'heavy': 3, 'severe': 4}\n",
    "    occlusion_data = metrics['occlusion_analysis']['occlusion_levels']\n",
    "    total_occ = sum(occlusion_data.values())\n",
    "    if total_occ > 0:\n",
    "        avg_occlusion = sum(occlusion_weights[level] * count for level, count in occlusion_data.items()) / total_occ\n",
    "    else:\n",
    "        avg_occlusion = 0\n",
    "    row_data.append(avg_occlusion)\n",
    "    \n",
    "    # Scale complexity\n",
    "    scale_vars = metrics['scale_challenges']['scale_variations']\n",
    "    avg_scale_var = np.mean(scale_vars) if scale_vars else 0\n",
    "    row_data.append(avg_scale_var)\n",
    "    \n",
    "    # Visual challenge score\n",
    "    visual = metrics['visual_challenges']\n",
    "    visual_score = (\n",
    "        np.mean(visual['illumination_problems']) if visual['illumination_problems'] else 0\n",
    "    ) + (\n",
    "        (1 - np.mean(visual['contrast_issues'])) if visual['contrast_issues'] else 0\n",
    "    )\n",
    "    row_data.append(visual_score)\n",
    "    \n",
    "    # Density challenge\n",
    "    crowding = metrics['density_challenges']['crowding_indices']\n",
    "    avg_crowding = np.mean(crowding) if crowding else 0\n",
    "    row_data.append(avg_crowding)\n",
    "    \n",
    "    challenge_matrix_data.append(row_data)\n",
    "\n",
    "if challenge_matrix_data:\n",
    "    challenge_matrix = np.array(challenge_matrix_data)\n",
    "    correlation_matrix = np.corrcoef(challenge_matrix.T)\n",
    "    \n",
    "    labels = ['Occlusion', 'Scale Var', 'Visual', 'Density']\n",
    "    im = ax11.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    ax11.set_title('Challenge Correlation Matrix', fontweight='bold')\n",
    "    ax11.set_xticks(range(len(labels)))\n",
    "    ax11.set_yticks(range(len(labels)))\n",
    "    ax11.set_xticklabels(labels, rotation=45)\n",
    "    ax11.set_yticklabels(labels)\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax11.text(j, i, f'{correlation_matrix[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    plt.colorbar(im, ax=ax11)\n",
    "\n",
    "# 13. Comprehensive Statistics Summary (bottom rows)\n",
    "ax12 = fig.add_subplot(gs[3:, :])\n",
    "ax12.axis('off')\n",
    "\n",
    "summary_text = \"‚ö†Ô∏è COMPREHENSIVE WHEAT DETECTION CHALLENGE ASSESSMENT\\n\\n\"\n",
    "\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    summary_text += f\"üåæ {dataset_name}:\\n\"\n",
    "    \n",
    "    # Occlusion summary\n",
    "    occlusion_data = metrics['occlusion_analysis']['occlusion_levels']\n",
    "    total_occ = sum(occlusion_data.values())\n",
    "    if total_occ > 0:\n",
    "        severe_ratio = (occlusion_data.get('heavy', 0) + occlusion_data.get('severe', 0)) / total_occ\n",
    "        summary_text += f\"  üîç Occlusion: {severe_ratio*100:.1f}% severe cases\\n\"\n",
    "    \n",
    "    # Difficulty summary\n",
    "    difficulty_data = metrics['density_challenges']['detection_difficulty']\n",
    "    total_diff = sum(difficulty_data.values())\n",
    "    if total_diff > 0:\n",
    "        hard_ratio = (difficulty_data.get('hard', 0) + difficulty_data.get('extreme', 0)) / total_diff\n",
    "        summary_text += f\"  üéØ Difficulty: {hard_ratio*100:.1f}% hard/extreme cases\\n\"\n",
    "    \n",
    "    # Scale challenges\n",
    "    scale_data = metrics['scale_challenges']\n",
    "    if scale_data['scale_variations']:\n",
    "        avg_scale_var = np.mean(scale_data['scale_variations'])\n",
    "        summary_text += f\"  üìè Scale variation: {avg_scale_var:.3f}\\n\"\n",
    "    \n",
    "    # Visual challenges\n",
    "    visual_data = metrics['visual_challenges']\n",
    "    if visual_data['contrast_issues']:\n",
    "        avg_contrast = np.mean(visual_data['contrast_issues'])\n",
    "        summary_text += f\"  üëÅÔ∏è Average contrast: {avg_contrast:.3f}\\n\"\n",
    "    \n",
    "    # Complexity factors\n",
    "    complexity_data = metrics['detection_complexity']\n",
    "    summary_text += f\"  üé≤ Edge cases: {complexity_data['edge_cases']}\\n\"\n",
    "    summary_text += f\"  ‚ùì Ambiguous objects: {complexity_data['ambiguous_objects']}\\n\"\n",
    "    \n",
    "    # Field-specific issues\n",
    "    field_data = metrics['field_specific_issues']\n",
    "    if field_data['weather_impact_scores']:\n",
    "        avg_weather = np.mean(field_data['weather_impact_scores'])\n",
    "        summary_text += f\"  üå§Ô∏è Weather impact: {avg_weather:.2f}\\n\"\n",
    "    \n",
    "    summary_text += \"\\n\"\n",
    "\n",
    "ax12.text(0.02, 0.98, summary_text, transform=ax12.transAxes, fontsize=9,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.suptitle('‚ö†Ô∏è COMPREHENSIVE WHEAT DETECTION CHALLENGE ASSESSMENT', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.savefig(notebook_results_dir / 'visualizations' / 'comprehensive_challenge_assessment.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save comprehensive challenge assessment results\n",
    "challenge_summary = {}\n",
    "for dataset_name, metrics in challenge_results.items():\n",
    "    challenge_summary[dataset_name] = {\n",
    "        'occlusion_analysis': {\n",
    "            'levels_distribution': metrics['occlusion_analysis']['occlusion_levels'],\n",
    "            'average_overlaps': float(np.mean(metrics['occlusion_analysis']['occlusion_patterns'])) if metrics['occlusion_analysis']['occlusion_patterns'] else 0,\n",
    "            'severe_cases_ratio': float((metrics['occlusion_analysis']['occlusion_levels'].get('heavy', 0) + \n",
    "                                       metrics['occlusion_analysis']['occlusion_levels'].get('severe', 0)) / \n",
    "                                      max(sum(metrics['occlusion_analysis']['occlusion_levels'].values()), 1))\n",
    "        },\n",
    "        'scale_challenges': {\n",
    "            'scale_variation': float(np.mean(metrics['scale_challenges']['scale_variations'])) if metrics['scale_challenges']['scale_variations'] else 0,\n",
    "            'multi_scale_complexity': float(np.mean(metrics['scale_challenges']['multi_scale_complexity'])) if metrics['scale_challenges']['multi_scale_complexity'] else 0,\n",
    "            'size_consistency': float(np.mean(metrics['scale_challenges']['size_consistency'])) if metrics['scale_challenges']['size_consistency'] else 0\n",
    "        },\n",
    "        'density_challenges': {\n",
    "            'difficulty_distribution': metrics['density_challenges']['detection_difficulty'],\n",
    "            'average_crowding': float(np.mean(metrics['density_challenges']['crowding_indices'])) if metrics['density_challenges']['crowding_indices'] else 0,\n",
    "            'hard_cases_ratio': float((metrics['density_challenges']['detection_difficulty'].get('hard', 0) + \n",
    "                                     metrics['density_challenges']['detection_difficulty'].get('extreme', 0)) / \n",
    "                                    max(sum(metrics['density_challenges']['detection_difficulty'].values()), 1))\n",
    "        },\n",
    "        'visual_challenges': {\n",
    "            'contrast_issues': float(np.mean(metrics['visual_challenges']['contrast_issues'])) if metrics['visual_challenges']['contrast_issues'] else 0,\n",
    "            'illumination_problems': float(np.mean(metrics['visual_challenges']['illumination_problems'])) if metrics['visual_challenges']['illumination_problems'] else 0,\n",
    "            'background_interference': float(np.mean(metrics['visual_challenges']['background_interference'])) if metrics['visual_challenges']['background_interference'] else 0,\n",
    "            'texture_similarity': float(np.mean(metrics['visual_challenges']['texture_similarity'])) if metrics['visual_challenges']['texture_similarity'] else 0\n",
    "        },\n",
    "        'geometric_challenges': {\n",
    "            'shape_variations': float(np.mean(metrics['geometric_challenges']['shape_variations'])) if metrics['geometric_challenges']['shape_variations'] else 0,\n",
    "            'boundary_clarity': float(np.mean(metrics['geometric_challenges']['boundary_clarity'])) if metrics['geometric_challenges']['boundary_clarity'] else 0\n",
    "        },\n",
    "        'field_specific_issues': {\n",
    "            'growth_stage_distribution': dict(Counter(metrics['field_specific_issues']['growth_stage_variations'])),\n",
    "            'weather_impact': float(np.mean(metrics['field_specific_issues']['weather_impact_scores'])) if metrics['field_specific_issues']['weather_impact_scores'] else 0,\n",
    "            'soil_interference': float(np.mean(metrics['field_specific_issues']['soil_interference'])) if metrics['field_specific_issues']['soil_interference'] else 0\n",
    "        },\n",
    "        'detection_complexity': {\n",
    "            'edge_cases': metrics['detection_complexity']['edge_cases'],\n",
    "            'ambiguous_objects': metrics['detection_complexity']['ambiguous_objects'],\n",
    "            'false_positive_risk': float(np.mean(metrics['detection_complexity']['false_positive_risks'])) if metrics['detection_complexity']['false_positive_risks'] else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(notebook_results_dir / 'data_analysis' / 'comprehensive_challenge_assessment.json', 'w') as f:\n",
    "    json.dump(challenge_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Comprehensive challenge assessment saved!\")\n",
    "print(f\"üìÅ Location: {notebook_results_dir / 'data_analysis' / 'comprehensive_challenge_assessment.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6159d",
   "metadata": {},
   "source": [
    "## üåæ Global Wheat Head Detection - Comprehensive Analysis Summary\n",
    "## Enhanced CBAM-STN-TPS-YOLO Dataset Exploration\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Overview\n",
    "\n",
    "This comprehensive analysis presents the complete evaluation of the Global Wheat Head Detection dataset, focusing on dense object detection challenges and optimization strategies for the CBAM-STN-TPS-YOLO architecture. The analysis encompasses distribution patterns, clustering characteristics, field conditions, and detection challenges specific to agricultural wheat head identification.\n",
    "\n",
    "### Analysis Metadata\n",
    "- **Analysis Version**: 2.0_enhanced\n",
    "- **Dataset Focus**: Global Wheat Head Detection - Comprehensive Analysis\n",
    "- **Domain Characteristics**: Agricultural Computer Vision - Dense Object Detection\n",
    "- **Analysis Scope**: Multi-dimensional wheat detection challenge assessment\n",
    "- **Detection Paradigm**: Dense small object detection in agricultural environments\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview and Characteristics\n",
    "\n",
    "### Core Statistics\n",
    "- **Total Images Processed**: Comprehensive wheat field image collection\n",
    "- **Domain Specialization**: Wheat head detection in natural field environments\n",
    "- **Detection Paradigm**: Dense small object detection with high overlap scenarios\n",
    "- **Complexity Level**: High-density small objects with significant clustering\n",
    "\n",
    "### Primary Detection Challenges\n",
    "\n",
    "#### Density and Clustering Challenges\n",
    "- **High-Density Clustering**: Dense wheat head arrangements requiring sophisticated detection\n",
    "- **Small Object Detection**: Wheat heads typically occupy 0.001-0.008 normalized image area\n",
    "- **Overlapping Wheat Heads**: Significant occlusion patterns in mature wheat fields\n",
    "- **Multi-Scale Detection**: Varying wheat head sizes within single field images\n",
    "\n",
    "#### Environmental and Field Conditions\n",
    "- **Field Condition Variations**: Diverse agricultural environments and growing conditions\n",
    "- **Illumination Challenges**: Variable lighting from shadows to overexposure\n",
    "- **Background Interference**: Complex soil, vegetation, and field texture variations\n",
    "- **Growth Stage Adaptations**: Different wheat maturity levels affecting appearance\n",
    "\n",
    "#### Technical Detection Complexities\n",
    "- **Occlusion Management**: Heavy, partial, and minimal occlusion scenarios\n",
    "- **Scale Variation Handling**: Multiple wheat head sizes requiring multi-scale processing\n",
    "- **Background Interference Filtering**: Distinguishing wheat heads from complex backgrounds\n",
    "- **Environmental Robustness**: Handling weather, soil, and seasonal variations\n",
    "\n",
    "---\n",
    "\n",
    "## Comprehensive Analysis Results\n",
    "\n",
    "### Wheat Head Distribution Analysis\n",
    "\n",
    "#### Density Characteristics\n",
    "- **Average Wheat Heads per Image**: 15-30+ objects per field image\n",
    "- **Density Categories**: Low, medium, high, and extreme density classifications\n",
    "- **Spatial Distribution**: Non-uniform clustering patterns across field images\n",
    "- **Density Variation**: High coefficient of variation in wheat head counts\n",
    "\n",
    "#### Clustering Patterns\n",
    "- **Clustering Strength**: Quantified spatial clustering using DBSCAN analysis\n",
    "- **Uniformity Measures**: Assessment of wheat head spatial distribution consistency\n",
    "- **Overlap Analysis**: IoU-based overlap detection and severity quantification\n",
    "- **Spatial Entropy**: Measurement of wheat head placement randomness\n",
    "\n",
    "### Field Condition Assessment\n",
    "\n",
    "#### Illumination Profile Analysis\n",
    "- **Brightness Stability**: Variation in lighting conditions across images\n",
    "- **Lighting Categories**: Classification of easy, moderate, and challenging lighting\n",
    "- **Shadow Effects**: Impact of shadows on wheat head visibility and detection\n",
    "- **Contrast Adequacy**: Assessment of image contrast for effective detection\n",
    "\n",
    "#### Environmental Factor Analysis\n",
    "- **Vegetation Density**: Ratio of wheat vegetation to background elements\n",
    "- **Soil Visibility**: Interference from exposed soil and non-wheat vegetation\n",
    "- **Background Complexity**: Quantified complexity of field backgrounds\n",
    "- **Weather Impact**: Effects of environmental conditions on image quality\n",
    "\n",
    "### Detection Challenge Assessment\n",
    "\n",
    "#### Difficulty Classification\n",
    "- **Easy Cases**: Clear, well-separated wheat heads with good contrast\n",
    "- **Moderate Cases**: Some overlapping or challenging lighting conditions\n",
    "- **Hard Cases**: Significant occlusion or poor environmental conditions\n",
    "- **Extreme Cases**: Maximum density with severe overlapping and poor conditions\n",
    "\n",
    "#### Specific Challenge Quantification\n",
    "- **Occlusion Severity**: Average number of overlapping wheat heads per image\n",
    "- **Scale Complexity**: Variation in wheat head sizes requiring multi-scale detection\n",
    "- **Visual Challenges**: Illumination, contrast, and visibility issues\n",
    "- **Boundary Detection**: Edge effects and partial wheat heads at image borders\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Insights and Pattern Discovery\n",
    "\n",
    "### Key Statistical Insights\n",
    "\n",
    "#### Density Statistics\n",
    "- **Mean Density**: Average wheat heads per image across all datasets\n",
    "- **Density Standard Deviation**: Variability in wheat head counts\n",
    "- **Density Range**: Minimum to maximum wheat heads observed\n",
    "- **High-Density Prevalence**: Percentage of images with challenging densities\n",
    "\n",
    "#### Difficulty Distribution Analysis\n",
    "- **Mean Difficulty**: Average challenge level across all images\n",
    "- **Hard Case Prevalence**: Percentage of images classified as difficult\n",
    "- **Occlusion Consistency**: Reliability of overlap patterns across datasets\n",
    "- **Challenge Correlation**: Relationship between different difficulty factors\n",
    "\n",
    "### Pattern Discoveries\n",
    "\n",
    "#### Spatial Organization Patterns\n",
    "- Wheat heads exhibit strong clustering patterns in dense field conditions\n",
    "- Spatial distribution follows agricultural row planting patterns\n",
    "- Cluster sizes correlate with field maturity and growing conditions\n",
    "- Edge effects create detection challenges at image boundaries\n",
    "\n",
    "#### Morphological Patterns\n",
    "- Scale variations correlate with field perspective and camera distance\n",
    "- Growth stage variations create morphological detection challenges\n",
    "- Aspect ratio consistency across different wheat varieties\n",
    "- Shape deformation patterns under wind and environmental stress\n",
    "\n",
    "#### Environmental Impact Patterns\n",
    "- Illumination challenges significantly impact detection difficulty\n",
    "- Background interference increases with soil visibility\n",
    "- Seasonal variations affect wheat head appearance and detectability\n",
    "- Weather conditions create systematic detection challenges\n",
    "\n",
    "### Optimization Opportunities\n",
    "\n",
    "#### Architecture-Specific Optimizations\n",
    "- Multi-scale feature extraction critical for varying wheat head sizes\n",
    "- Attention mechanisms essential for dense clustering scenarios\n",
    "- Spatial transformation networks beneficial for perspective variations\n",
    "- Advanced NMS required for overlapping object management\n",
    "- Data augmentation crucial for illumination robustness\n",
    "\n",
    "#### Training Strategy Optimizations\n",
    "- Progressive training from simple to complex field scenarios\n",
    "- Density-aware loss functions for imbalanced object distributions\n",
    "- Multi-scale training for handling size variations\n",
    "- Extensive augmentation for environmental robustness\n",
    "- Specialized evaluation metrics for dense object scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## CBAM-STN-TPS-YOLO Architecture Alignment\n",
    "\n",
    "### Component Justification and Benefits\n",
    "\n",
    "#### CBAM (Convolutional Block Attention Module)\n",
    "**Rationale**: High background interference and dense clustering require attention mechanisms\n",
    "\n",
    "**Specific Benefits**:\n",
    "- Channel attention for wheat-soil discrimination\n",
    "- Spatial attention for crowded scene focus\n",
    "- Illumination invariance through adaptive attention\n",
    "- Multi-level attention for different growth stages\n",
    "\n",
    "**Configuration Recommendations**:\n",
    "- Reduction ratio 16 for optimal wheat feature extraction\n",
    "- Spatial kernel size 7 for wheat head receptive field\n",
    "- Multi-level attention integration across network layers\n",
    "\n",
    "#### STN (Spatial Transformer Network)\n",
    "**Rationale**: Field perspective variations and camera angle diversity require spatial transformation\n",
    "\n",
    "**Specific Benefits**:\n",
    "- Perspective normalization for consistent detection\n",
    "- Rotation handling for varying field orientations\n",
    "- Scale compensation for distance variations\n",
    "- Viewpoint invariance for different camera positions\n",
    "\n",
    "**Configuration Recommendations**:\n",
    "- Affine transformation for perspective correction\n",
    "- Localization network with field-specific features\n",
    "- Progressive transformation for training stability\n",
    "\n",
    "#### TPS (Thin Plate Spline)\n",
    "**Rationale**: Irregular wheat head shapes and wind deformation require non-rigid transformation\n",
    "\n",
    "**Specific Benefits**:\n",
    "- Non-rigid deformation handling for wind effects\n",
    "- Irregular boundary adaptation for growth variations\n",
    "- Shape normalization for consistent feature extraction\n",
    "- Flexible transformation for natural object variations\n",
    "\n",
    "**Configuration Recommendations**:\n",
    "- 20-24 control points for wheat head complexity\n",
    "- Regularization lambda 0.01-0.1 for shape preservation\n",
    "- Uniform grid initialization for field structure alignment\n",
    "\n",
    "#### YOLO Optimization\n",
    "**Rationale**: Dense object detection with real-time requirements for agricultural applications\n",
    "\n",
    "**Specific Benefits**:\n",
    "- Efficient dense detection for high wheat head counts\n",
    "- Multi-scale processing for size variations\n",
    "- End-to-end optimization for field deployment\n",
    "- Real-time inference for agricultural monitoring\n",
    "\n",
    "**Configuration Recommendations**:\n",
    "- Wheat-specific anchor sizes: [8,8], [16,16], [32,32], [48,48], [64,64]\n",
    "- Dense prediction layers for high-density scenarios\n",
    "- NMS threshold 0.3-0.4 for overlap management\n",
    "- Maximum detections set to 100 for extreme density cases\n",
    "\n",
    "### Synergy Optimization Strategies\n",
    "\n",
    "#### CBAM-STN Integration\n",
    "- CBAM attention guides STN localization network focus\n",
    "- STN normalized features enhance CBAM effectiveness\n",
    "- Joint training for optimal transformation learning\n",
    "- Attention-guided geometric transformation\n",
    "\n",
    "#### STN-TPS Coordination\n",
    "- STN global transformation followed by TPS local refinement\n",
    "- Hierarchical transformation from coarse to fine adjustment\n",
    "- Shared feature extraction for computational efficiency\n",
    "- Progressive deformation handling\n",
    "\n",
    "#### Attention-Guided Detection\n",
    "- CBAM features inform YOLO detection heads\n",
    "- Attention maps guide anchor placement optimization\n",
    "- Multi-level attention for multi-scale detection\n",
    "- Feature enhancement for dense object scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## Training Strategy Recommendations\n",
    "\n",
    "### Data Preparation and Preprocessing\n",
    "\n",
    "#### Preprocessing Pipeline\n",
    "- Normalize to [0,1] range with ImageNet statistics\n",
    "- Resize with aspect ratio preservation for field structure\n",
    "- Multi-scale training with progressive sizing (416‚Üí640‚Üí832)\n",
    "- Quality enhancement for low-contrast field images\n",
    "\n",
    "#### Annotation Optimization\n",
    "- Verify dense annotation consistency across all images\n",
    "- Handle overlapping bounding boxes with IoU analysis\n",
    "- Quality control for small object annotations\n",
    "- Validate wheat head boundary accuracy\n",
    "\n",
    "#### Data Splitting Strategy\n",
    "- Stratified split by density categories for balanced training\n",
    "- Temporal split for growth stage diversity representation\n",
    "- Geographic split for field condition variety\n",
    "- Cross-validation for robust performance assessment\n",
    "\n",
    "### Augmentation Strategy\n",
    "\n",
    "#### Geometric Augmentations\n",
    "- Random rotation (¬±15¬∞) for field orientation diversity\n",
    "- Random perspective transform for camera angle variation\n",
    "- Random scale (0.8-1.2) for distance simulation\n",
    "- Horizontal/vertical flipping for spatial variation\n",
    "\n",
    "#### Photometric Augmentations\n",
    "- Color jittering for illumination robustness\n",
    "- Random brightness/contrast for lighting condition simulation\n",
    "- HSV augmentation for growth stage appearance variation\n",
    "- Gaussian noise addition for sensor variation simulation\n",
    "\n",
    "#### Wheat-Specific Augmentations\n",
    "- Mosaic augmentation for density increase simulation\n",
    "- CutMix for realistic occlusion pattern creation\n",
    "- Random erasing for missing wheat head simulation\n",
    "- Copy-paste augmentation for rare density scenarios\n",
    "\n",
    "#### Advanced Field-Aware Augmentations\n",
    "- Field-structure preserving augmentation maintaining row patterns\n",
    "- Density-preserving augmentation maintaining wheat head counts\n",
    "- Weather simulation through atmospheric effect modeling\n",
    "- Growth stage interpolation for temporal consistency\n",
    "\n",
    "### Training Schedule and Methodology\n",
    "\n",
    "#### Progressive Training Phases\n",
    "1. **Phase 1**: Basic detection on simple, clear images (20 epochs)\n",
    "2. **Phase 2**: Multi-scale training introduction with moderate complexity (30 epochs)\n",
    "3. **Phase 3**: Full augmentation pipeline with challenging scenarios (50 epochs)\n",
    "\n",
    "#### Learning Rate Schedule\n",
    "- **Warm-up**: Linear increase for 5 epochs to stable learning rate\n",
    "- **Main Training**: Cosine annealing with restarts for optimization\n",
    "- **Fine-tuning**: Reduced learning rate for final convergence\n",
    "\n",
    "#### Component Training Strategy\n",
    "- Pre-train backbone on ImageNet for feature extraction foundation\n",
    "- Freeze-unfreeze strategy for CBAM integration stability\n",
    "- Joint fine-tuning for STN-TPS coordination optimization\n",
    "- Progressive component activation for training stability\n",
    "\n",
    "---\n",
    "\n",
    "## Comprehensive Evaluation Framework\n",
    "\n",
    "### Multi-Dimensional Metric Suite\n",
    "\n",
    "#### Core Detection Metrics\n",
    "- **mAP@0.5**: General detection performance assessment\n",
    "- **mAP@0.75**: Precise localization capability evaluation\n",
    "- **mAP@[0.5:0.95]**: Comprehensive performance across IoU thresholds\n",
    "- **Precision/Recall Curves**: Threshold sensitivity analysis\n",
    "\n",
    "#### Density-Specific Evaluation\n",
    "- **Small Object AP**: Performance on area < 32¬≤ wheat heads\n",
    "- **Medium Object AP**: Performance on 32¬≤ ‚â§ area < 96¬≤ wheat heads\n",
    "- **Large Object AP**: Performance on area ‚â• 96¬≤ wheat heads\n",
    "- **Crowded Scene AP**: Performance on images with >20 objects\n",
    "\n",
    "#### Robustness Assessment Metrics\n",
    "- **Illumination Invariance Score**: Performance across lighting conditions\n",
    "- **Scale Robustness Measure**: Consistency across size variations\n",
    "- **Occlusion Handling Capability**: Performance with overlapping objects\n",
    "- **Background Interference Resistance**: Robustness to complex backgrounds\n",
    "\n",
    "#### Efficiency and Deployment Metrics\n",
    "- **Inference Time**: Milliseconds per image processing\n",
    "- **Frames Per Second**: Real-time application suitability\n",
    "- **Memory Usage**: GPU and CPU resource requirements\n",
    "- **Model Complexity**: Parameters and computational requirements\n",
    "\n",
    "### Benchmark Protocol\n",
    "\n",
    "#### Comprehensive Test Scenarios\n",
    "- **Standard Test Set**: Baseline performance evaluation\n",
    "- **Cross-Field Generalization**: Performance on unseen field conditions\n",
    "- **Growth Stage Robustness**: Consistency across wheat maturity levels\n",
    "- **Weather Condition Stress Test**: Performance under challenging conditions\n",
    "\n",
    "#### Detailed Ablation Studies\n",
    "- **Component-wise Ablation**: Individual CBAM, STN, TPS contribution analysis\n",
    "- **Loss Function Ablation**: Impact of different loss formulations\n",
    "- **Augmentation Strategy Ablation**: Effectiveness of augmentation techniques\n",
    "- **Architecture Variant Comparison**: Alternative design choices evaluation\n",
    "\n",
    "#### Competitive Baseline Comparisons\n",
    "- Standard YOLO variants (YOLOv5, YOLOv8)\n",
    "- RetinaNet with Feature Pyramid Networks\n",
    "- Faster R-CNN with ResNet backbone\n",
    "- EfficientDet architecture variants\n",
    "- Specialized agricultural detection systems\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment Considerations\n",
    "\n",
    "### Hardware Requirements and Optimization\n",
    "\n",
    "#### Computational Specifications\n",
    "- **GPU Requirements**: NVIDIA RTX 3060 or equivalent for real-time inference\n",
    "- **CPU Specifications**: 8-core processor for preprocessing pipeline support\n",
    "- **Memory Requirements**: 16GB RAM for batch processing capabilities\n",
    "- **Storage Solutions**: SSD for fast dataset access and model loading\n",
    "\n",
    "#### Mobile and Edge Deployment\n",
    "- **Edge Device Compatibility**: NVIDIA Jetson series optimization\n",
    "- **Model Quantization**: INT8 quantization for mobile inference acceleration\n",
    "- **Optimized Inference Engines**: TensorRT and ONNX runtime integration\n",
    "- **Power Consumption Optimization**: Efficient inference for field deployment\n",
    "\n",
    "#### Field Equipment Integration\n",
    "- **Ruggedized Camera Systems**: Weather-resistant imaging equipment\n",
    "- **Robust Computing Units**: Agricultural environment computing solutions\n",
    "- **Reliable Connectivity**: Field-to-cloud data transmission systems\n",
    "- **Power Management**: Sustainable power solutions for remote deployment\n",
    "\n",
    "### Software Optimization Strategies\n",
    "\n",
    "#### Model Compression Techniques\n",
    "- **Pruning**: Parameter reduction while maintaining performance\n",
    "- **Quantization**: Inference speedup through reduced precision\n",
    "- **Knowledge Distillation**: Efficiency improvement through teacher-student training\n",
    "- **Dynamic Inference**: Adaptive computation based on scene complexity\n",
    "\n",
    "#### Runtime Performance Optimization\n",
    "- **Batch Processing**: Throughput optimization for multiple images\n",
    "- **Pipeline Parallelization**: Concurrent preprocessing and inference\n",
    "- **Memory Management**: Efficient GPU memory utilization\n",
    "- **Kernel Optimization**: Custom CUDA kernels for specific operations\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Roadmap\n",
    "\n",
    "### Development Phases\n",
    "\n",
    "#### Phase 1: Architecture Implementation\n",
    "- Implement CBAM-STN-TPS-YOLO architecture with wheat-specific parameters\n",
    "- Configure component integration and synergy optimization\n",
    "- Establish baseline performance with standard training\n",
    "- Validate architecture stability and convergence\n",
    "\n",
    "#### Phase 2: Advanced Training Pipeline\n",
    "- Design comprehensive augmentation pipeline for field condition robustness\n",
    "- Implement progressive training strategy with density-aware scheduling\n",
    "- Develop wheat-specific loss functions and evaluation metrics\n",
    "- Optimize hyperparameters for agricultural detection scenarios\n",
    "\n",
    "#### Phase 3: Evaluation and Validation\n",
    "- Implement comprehensive evaluation framework with wheat-specific metrics\n",
    "- Conduct extensive ablation studies for component validation\n",
    "- Perform cross-field generalization testing\n",
    "- Evaluate robustness under diverse environmental conditions\n",
    "\n",
    "#### Phase 4: Optimization and Deployment\n",
    "- Optimize model for deployment with hardware-specific considerations\n",
    "- Implement mobile and edge device optimizations\n",
    "- Develop field deployment protocols and monitoring systems\n",
    "- Create agricultural integration interfaces and APIs\n",
    "\n",
    "#### Phase 5: Field Testing and Refinement\n",
    "- Conduct real-world field testing across diverse agricultural environments\n",
    "- Collect performance feedback and identify improvement opportunities\n",
    "- Iterative refinement based on agricultural user requirements\n",
    "- Scale deployment across different wheat growing regions\n",
    "\n",
    "#### Phase 6: Production and Maintenance\n",
    "- Production system deployment with monitoring and maintenance protocols\n",
    "- Continuous learning integration for model improvement\n",
    "- Agricultural stakeholder training and support systems\n",
    "- Long-term performance monitoring and optimization\n",
    "\n",
    "---\n",
    "\n",
    "## Key Findings and Recommendations\n",
    "\n",
    "### Critical Success Factors\n",
    "\n",
    "#### Technical Requirements\n",
    "- Multi-scale feature extraction essential for wheat head size variations\n",
    "- Attention mechanisms critical for dense clustering scenarios\n",
    "- Spatial transformations necessary for field perspective variations\n",
    "- Advanced post-processing required for overlapping object management\n",
    "\n",
    "#### Training Optimization\n",
    "- Progressive training strategy from simple to complex scenarios\n",
    "- Extensive augmentation pipeline for environmental robustness\n",
    "- Density-aware loss functions for imbalanced object distributions\n",
    "- Multi-scale training approach for consistent performance\n",
    "\n",
    "#### Deployment Considerations\n",
    "- Edge device optimization crucial for field applications\n",
    "- Real-time inference requirements for agricultural monitoring\n",
    "- Robust performance under challenging environmental conditions\n",
    "- Integration with existing agricultural workflow systems\n",
    "\n",
    "### Risk Mitigation Strategies\n",
    "\n",
    "#### Technical Risks\n",
    "- **False Positive Control**: Robust background discrimination training\n",
    "- **False Negative Minimization**: Comprehensive occlusion handling strategies\n",
    "- **Scale Sensitivity Management**: Multi-scale training and evaluation\n",
    "- **Environmental Robustness**: Extensive augmentation and field testing\n",
    "\n",
    "#### Deployment Risks\n",
    "- **Hardware Compatibility**: Thorough testing across target devices\n",
    "- **Performance Consistency**: Robust evaluation under diverse conditions\n",
    "- **Integration Challenges**: Careful API design and documentation\n",
    "- **Maintenance Requirements**: Automated monitoring and update systems\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The comprehensive analysis of the Global Wheat Head Detection dataset reveals significant challenges and opportunities for CBAM-STN-TPS-YOLO architecture optimization. Key achievements include:\n",
    "\n",
    "### Analysis Completeness\n",
    "- **Multi-dimensional Assessment**: Distribution, clustering, field conditions, and challenge analysis\n",
    "- **Pattern Discovery**: Identification of wheat-specific detection patterns and challenges\n",
    "- **Architecture Alignment**: Detailed justification for CBAM-STN-TPS-YOLO components\n",
    "- **Implementation Guidance**: Comprehensive training and deployment strategies\n",
    "\n",
    "### Technical Insights\n",
    "- Dense object detection challenges requiring specialized attention mechanisms\n",
    "- Multi-scale variations necessitating adaptive feature extraction\n",
    "- Environmental robustness critical for agricultural deployment success\n",
    "- Real-time performance requirements achievable with proper optimization\n",
    "\n",
    "### Implementation Readiness\n",
    "The analysis provides a solid foundation for advancing to CBAM-STN-TPS-YOLO implementation with comprehensive understanding of dataset characteristics, training strategies, and deployment considerations tailored specifically for wheat head detection challenges.\n",
    "\n",
    "---\n",
    "\n",
    "*Analysis completed with enhanced pipeline providing comprehensive multi-dimensional wheat detection assessment*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
