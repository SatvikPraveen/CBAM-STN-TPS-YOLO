# VALIDATION: Run `python src/utils/config_validator.py` before training
# This validates parameter counts, dependency versions, and configuration consistency
# config/training_configs.yaml
# Enhanced Training Configuration for CBAM-STN-TPS-YOLO
# Authors: Satvik Praveen, Yoonsung Jung
# Institution: Texas A&M University
# Version: 2.0.0

# =============================================================================
# DATASET CONFIGURATION
# =============================================================================
data:
  # Primary dataset path
  data_dir: "data/PGP"

  # Dataset-specific paths
  datasets:
    PGP:
      train_path: "data/PGP/train"
      val_path: "data/PGP/val"
      test_path: "data/PGP/test"
      type: "multi_spectral"

    MelonFlower:
      train_path: "data/MelonFlower/train"
      val_path: "data/MelonFlower/valid"
      type: "rgb"

    GlobalWheat:
      train_path: "data/GlobalWheat/train"
      test_path: "data/GlobalWheat/test"
      type: "rgb"

  # Class configuration
  num_classes: 3
  class_names: ["Cotton", "Rice", "Corn"]
  class_weights: [1.0, 1.2, 1.1] # Adjust for class imbalance

  # Input configuration
  input_size: 640 # Standard YOLO input size
  input_channels: 4 # Multi-spectral: RGB + NIR
  spectral_bands: 4

  # Data loading
  batch_size: 16
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  drop_last: true

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  type: "CBAM-STN-TPS-YOLO"

  # Backbone configuration
  backbone:
    type: "darknet53"
    pretrained: true
    freeze_early_layers: false
    channels: [32, 64, 128, 256, 512]
    strides: [1, 2, 2, 2, 2]

  # CBAM attention configuration
  cbam:
    reduction_ratio: 16
    spatial_kernel_size: 7
    use_channel_attention: true
    use_spatial_attention: true
    activation: "sigmoid"

  # STN-TPS configuration
  stn:
    num_control_points: 20
    reg_lambda: 0.01
    feature_size: 28
    localization_channels: [8, 10]
    fc_hidden_dim: 32
    transformation_type: "tps" # or "affine"
    normalize_control_points: true

  # Detection head configuration
  detection_head:
    num_anchors: 3
    anchor_sizes:
      - [[10, 13], [16, 30], [33, 23]] # Small objects (leaves, flowers)
      - [[30, 61], [62, 45], [59, 119]] # Medium objects (fruits, stems)
      - [[116, 90], [156, 198], [373, 326]] # Large objects (whole plants)
    feature_pyramid_levels: [3, 4, 5]
    use_focal_loss: true

# =============================================================================
# TRAINING PARAMETERS
# =============================================================================
training:
  epochs: 200
  save_interval: 10
  log_interval: 50
  eval_interval: 5

  # Early stopping
  early_stopping:
    patience: 30
    min_delta: 0.001
    monitor: "val_mAP"
    mode: "max"

  # Gradient clipping
  grad_clip_norm: 1.0

  # Mixed precision training
  mixed_precision: true

  # Compilation (PyTorch 2.0)
  compile_model: false

  # Checkpoint configuration
  checkpoint_dir: "checkpoints"
  save_best_only: false
  save_last: true

  # Resume training
  resume_from: null # Path to checkpoint to resume from
  load_weights_only: false

# =============================================================================
# OPTIMIZER CONFIGURATION
# =============================================================================
optimizer:
  type: "AdamW"
  lr: 0.001
  weight_decay: 0.0001
  betas: [0.9, 0.999]
  eps: 1e-8
  amsgrad: false

  # Parameter group specific learning rates
  backbone_lr_mult: 0.1 # Lower LR for pretrained backbone
  attention_lr_mult: 1.0 # Standard LR for attention modules
  detection_lr_mult: 1.0 # Standard LR for detection heads

  # Alternative optimizers
  alternatives:
    SGD:
      lr: 0.01
      momentum: 0.9
      weight_decay: 0.0005
      nesterov: true

    Adam:
      lr: 0.001
      betas: [0.9, 0.999]
      weight_decay: 0.0001

# =============================================================================
# LEARNING RATE SCHEDULER
# =============================================================================
scheduler:
  type: "CosineAnnealingWarmRestarts"
  T_0: 20
  T_mult: 2
  eta_min: 1e-6

  # Alternative schedulers
  alternatives:
    CosineAnnealingLR:
      T_max: 200
      eta_min: 1e-6

    OneCycleLR:
      max_lr: 0.01
      pct_start: 0.3
      anneal_strategy: "cos"

    ReduceLROnPlateau:
      mode: "max"
      factor: 0.5
      patience: 15
      verbose: true

    MultiStepLR:
      milestones: [100, 150, 180]
      gamma: 0.1

# =============================================================================
# LOSS FUNCTION CONFIGURATION
# =============================================================================
loss:
  type: "yolo"

  # YOLO loss weights
  weights:
    lambda_coord: 5.0
    lambda_obj: 1.0
    lambda_noobj: 0.5
    lambda_class: 2.0 # Higher for agricultural class imbalance

  # Component-specific loss weights
  attention_loss_weight: 0.1
  stn_loss_weight: 0.05
  tps_loss_weight: 0.01

  # Loss function parameters
  params:
    iou_loss_type: "ciou" # or "diou", "giou"
    use_focal_loss: true
    focal_alpha: 0.25
    focal_gamma: 2.0
    label_smoothing: 0.0

  # Alternative loss configurations
  alternatives:
    combo:
      losses:
        yolo: 0.7
        focal: 0.2
        tversky: 0.1

    adaptive:
      base_losses: ["yolo", "focal"]
      initial_weights: [0.8, 0.2]
      adaptation_rate: 0.01

# =============================================================================
# DATA AUGMENTATION
# =============================================================================
augmentation:
  # Geometric augmentations
  geometric:
    rotation_deg: 15
    shear_deg: 10
    scale_range: [0.8, 1.2]
    translation_range: 0.1
    flip_horizontal: true
    flip_vertical: false

  # Photometric augmentations
  photometric:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.3
    hue: 0.1
    gamma: [0.8, 1.2]
    noise_std: 0.01

  # Multi-spectral specific
  multi_spectral:
    spectral_shift: 0.05
    nir_augmentation: true
    band_dropout: 0.1

  # Advanced augmentations
  advanced:
    cutout:
      enabled: true
      holes: 3
      size: 16

    mixup:
      enabled: false
      alpha: 0.2

    cutmix:
      enabled: false
      alpha: 1.0

  # Augmentation schedule
  schedule:
    warmup_epochs: 10
    strong_aug_epochs: [50, 150]
    final_epochs_no_aug: 20

# =============================================================================
# MULTI-SPECTRAL PROCESSING
# =============================================================================
multi_spectral:
  enabled: true
  bands:
    red: 660 # nm - Agricultural red band
    green: 560 # nm - Vegetation analysis
    blue: 480 # nm - Standard blue
    nir: 830 # nm - Near-infrared for biomass
  # Note: These bands are PGP dataset specific
  # Standard agricultural bands would be: 680nm (red), 800nm (NIR)

  # Pseudo-RGB conversion for visualization
  pseudo_rgb:
    enabled: true
    mapping: [660, 820, 560] # Red, NIR, Green (false-color composite)
    normalization: "minmax"

  # Band-specific preprocessing
  preprocessing:
    normalize_bands: true
    band_means: [0.485, 0.456, 0.406, 0.5]
    band_stds: [0.229, 0.224, 0.225, 0.2]

# =============================================================================
# EVALUATION METRICS
# =============================================================================
metrics:
  # Detection metrics
  detection:
    iou_thresholds: [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
    conf_threshold: 0.5
    nms_threshold: 0.45

    # COCO-style evaluation
    coco_eval: true

    # Agricultural-specific metrics
    agricultural:
      plant_counting: true
      growth_stage_accuracy: true
      area_based_analysis: true

  # Advanced metrics
  advanced:
    track_per_image: true
    area_thresholds:
      small: [0, 1024] # pixels²
      medium: [1024, 9216] # pixels²
      large: [9216, 999999] # pixels²

# =============================================================================
# LOGGING AND MONITORING
# =============================================================================
logging:
  # Weights & Biases
  wandb:
    enabled: true
    project: "cbam-stn-tps-yolo"
    entity: null
    tags:
      ["agricultural", "object-detection", "attention", "spatial-transformer"]
    notes: "Enhanced CBAM-STN-TPS-YOLO with multi-spectral support"
    group: null
    job_type: "train"

  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "runs"

  # File logging
  file_logging:
    enabled: true
    log_file: "training.log"
    level: "INFO"

  # Visualization
  visualization:
    save_plots: true
    plot_interval: 10
    save_attention_maps: true
    save_tps_transformations: true

# =============================================================================
# HARDWARE CONFIGURATION
# =============================================================================
hardware:
  device: "cuda" # or "cpu", "auto"
  gpu_ids: [0] # List of GPU IDs to use

  # Distributed training
  distributed:
    enabled: false
    backend: "nccl"
    init_method: "env://"
    world_size: 1
    rank: 0

  # Memory optimization
  memory:
    gradient_checkpointing: false
    find_unused_parameters: false

# =============================================================================
# REPRODUCIBILITY
# =============================================================================
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false # Set to true for consistent input sizes

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================
deployment:
  # ONNX export
  onnx:
    enabled: false
    opset_version: 11
    dynamic_axes: true
    simplify: true

  # TensorRT optimization
  tensorrt:
    enabled: false
    precision: "fp16"
    workspace_size: 2147483648 # 2GB

  # Quantization
  quantization:
    enabled: false
    backend: "fbgemm"
    calibration_dataset_size: 100

# =============================================================================
# EXPERIMENT TRACKING
# =============================================================================
experiment:
  name: "cbam_stn_tps_yolo_v2"
  description: "Enhanced CBAM-STN-TPS-YOLO with comprehensive training pipeline"
  version: "2.0.0"

  # Experiment metadata
  metadata:
    paper_title: "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms"
    authors: ["Satvik Praveen", "Yoonsung Jung"]
    institution: "Texas A&M University"
    contact: "satvikpraveen_164@tamu.edu"

  # Results tracking
  results:
    save_dir: "results"
    save_predictions: true
    save_metrics: true
    create_report: true

# =============================================================================
# VALIDATION SCHEMA
# =============================================================================
validation:
  required_fields:
    - "data.num_classes"
    - "data.input_size"
    - "data.batch_size"
    - "training.epochs"
    - "optimizer.lr"
    - "model.type"

  constraints:
    data:
      batch_size:
        min: 1
        max: 256
      input_size:
        allowed: [224, 256, 320, 384, 416, 512, 608, 640]
      num_classes:
        min: 1
        max: 1000

    training:
      epochs:
        min: 1
        max: 1000

    optimizer:
      lr:
        min: 1e-6
        max: 1.0
      weight_decay:
        min: 0.0
        max: 1.0

    model:
      cbam:
        reduction_ratio:
          allowed: [4, 8, 16, 32]
        spatial_kernel_size:
          allowed: [3, 5, 7, 9, 11]

      stn:
        num_control_points:
          min: 4
          max: 50
        reg_lambda:
          min: 0.001
          max: 1.0

  # Warning thresholds
  warnings:
    high_learning_rate: 0.1
    large_batch_size: 64
    many_epochs: 500

# =============================================================================
# MODEL VALIDATION
# =============================================================================
model_validation:
  verify_parameter_count: true
  log_model_summary: true
  check_gradient_flow: true
  validate_forward_pass: true

  # Parameter count validation
  expected_params_million: 31.4
  tolerance_percent: 5.0

# =============================================================================
# DEBUGGING AND PROFILING
# =============================================================================
debug:
  enabled: false

  # Profiling
  profiling:
    enabled: false
    profile_memory: true
    profile_shapes: true

  # Model analysis
  analysis:
    print_model_summary: true
    visualize_architecture: false
    check_gradients: false

  # Data analysis
  data_analysis:
    visualize_batches: false
    check_data_distribution: false
    validate_annotations: false

# =============================================================================
# CONFIGURATION VALIDATION AND WARNINGS
# =============================================================================
validation_warnings:
  spectral_bands:
    message: "Using PGP-specific bands (580,660,730,820nm) not standard agricultural bands (680,800nm)"
    severity: "info"

  parameter_count:
    message: "Verify 31.4M parameter count matches actual model implementation"
    severity: "warning"
    action: "Run model_validation.verify_parameter_count = true"

  dependency_versions:
    message: "Ensure PyTorch versions are compatible across requirements.txt and setup.py"
    severity: "critical"

# Configuration consistency checks
consistency_checks:
  input_channels:
    model_config: 4 # From model_configs.yaml
    training_config: 4 # From training_configs.yaml
    must_match: true

  spectral_bands:
    count: 4
    wavelengths: [580, 660, 730, 820]
    pseudo_rgb_mapping: [660, 820, 560] # Updated mapping
